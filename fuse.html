[{"title":"About","url":"about.html","body":"This site is a mixture of my notes, a portfolio and a record of my hobbies and learning journey. It might be morphing into a Zettelkast I work mostly as a freelance data-scien but I also develop web apps and setup blockchain services for\u00a0client If you have questions, comments or find a mistake, you can find me on twitter. Thanks for\u00a0readin"},{"title":"Recognizing Traffic Lights Using The Azure Custom Vision API","url":"traffic.html","body":"You can upload any image, I suggest googling dashcam traffic lights, or use one of the images below. Images should be smaller than 4mb and should be .jpeg, .bmp or .png. The Shortest Route To A\u00a0Demo This demonstrat was inspired by the job descriptio for a freelance role I recently applied for. The project involved recognisin faults with traffic lights and I wanted to see how quickly I could develop an end-to-end computer vision system that recognises and labels This is a relatively simple solution which prioritize speed, simplicity and low costs. I used the free tier of the Azure Custom Vision service to train and deploy the model. The model is trained to recognise and label Summary The model is hosted on Azure Custom Vision on the free\u00a0tier The model is trained on ~4500\u00a0imag Images are part of the DriveU Traffic Light\u00a0Data To improve the model I\u00a0would: Use many more\u00a0image Experiment with different Tune Method Find a good dataset. It would have taken too long to create my own labelled dataset so I needed to find a freely available set of labelled images. It turns out there are several to choose from. Waymo have a huge dataset that is freely available but I chose to use the DriveU Traffic Light dataset instead. It\u2019s well documented easily accessible and good\u00a0enoug Convert the images - the DTLD images are 16-bit .TIFF images. I needed .JPEG or .PNG images. I first converted the 16-bit .TIFF images to 8-bit, and then converted them to .JPEG. The DTLD dataset contains more image data and metadata than I needed so I simply ignored or stripped out what I didn\u2019t\u00a0nee Parse the label data to extract only the informatio I needed. The DTLD dataset contains labels that specify the location of the traffic lights in the images as well as the type of traffic light and what phase the lights are in. I was only interested in the location of the lights for this demo. I needed to convert the coordinate of the regions from absolute to\u00a0relativ Create a Custom Vision project, create custom tags, and upload pairs of images and labels in the required form. The documentat was good enough and whilst there were a few steps that were\u00a0uncle I was able to quickly figure out what to do, usually by clicking around to try a couple things and check the results. Each cloud platform has its own quirks and design concepts, and once you\u2019ve understood the pattern you can develop a good intuition for how each platform (in this case Azure) \u201cwants\u201d you to do\u00a0somethi Train the model. There aren\u2019t many options to choose from and the dataset wasn\u2019t very\u00a0big. Use the model to Create a simple UI on a static site (this page) using JavaScript and HTML. The JavaScript Fetch API is used to query the Custom Vision API. jQuery and some custom (vanilla) JavaScript is used to parse the results and create the interactiv elements on the\u00a0page. The model\u2019s results are shown by overlaying an HTML canvas element on top of the img element that shows the image that\u2019s been uploaded by the user. The regions and probabilit are drawn using HTML Canvas methods (strokeRec fillText etc). Next\u00a0steps The model is trained on images from German Cities. In order to generalise the model it should be trained using images from a wider distributi This could\u00a0incl Rural and Views from footpaths as well as\u00a0roads. Different Different cities and\u00a0countr It would be nice to let the user adjust the minimum probabilit threshold. Currently only results with a probabilit above 10% are\u00a0shown. Test\u00a0image You could use these images to test the model. You\u2019ll need to store them locally Test Image\u00a01 Test Image\u00a02 Test Image\u00a03 Test Image\u00a04"},{"title":"Experience","url":"experience.html","body":"I\u2019m available for freelance and consulting please send enquiries via email. 2021: Freelance Blockchain integratio for a cryptocurr broker. Deploying and managing nodes for networks including Ethereum, Binance Smart Chain, Polkadot, Cardano, Tron, Binance\u00a0Ch Using Rosetta to create workflows such as generating batches of addresses, notifying services about deposits and 2020: Technical Founder and Web\u00a0Develo PipPip.ema Event-driv and long-term scheduled email delivery. Focus on writing then relax knowing that scheduled delivery is\u00a0guarant MoneyBar.n Personal financial dashboard and data 2019: Freelance Data Scientist -\u00a0Shell \u201cJohn is a creative and conscienti software engineer, who understand business requiremen well and translatin those into applicatio under tight deadlines. Highly recommende - NLP, data driven web-apps, mentoring junior team members. Working as an internal consultant I developed and delivered a range of tools. I worked with a wide variety of stakeholde and prioritize understand business needs and defining scopes, whilst using agile developmen practices. I advised my team on software developmen practices and tooling decisions, and mentored junior members to improve their coding and business skills. Tools included Python, and used Plot.ly Dash, WSL, and Azure 2019: Freelance Data Analyst -\u00a0Uber \u201cJohn played a big role helping my team get our biggest and most challengin analytics tool across the line. He ramped up quickly, communicat well and was always responsive - Sankari Nair, Lead Developed an analytics tool using Python and Plot.ly Dash. The tool has a broad scope covering multiple regions, scales and business lines. I re-designe the app for scalabilit and performanc whilst increasing functional and provided a flexible foundation for new features to be developed after I left the project. Challenges included building custom refactorin legacy code, and processing large data 2018: Data Specialist -\u00a0Blockpor Design, create and maintain internal data tools for a cryptocurr exchange. As the only data specialist I did whatever needed to be done. I delivered tools to provide business insights including management informatio fraud analysis, tax reporting, KPI tracking, regulatory compliance and marketing and growth. I also delivered sentiment analysis of key social I worked with stakeholde across the business including back-end, founders, customer support, finance, DevOps and growth\u00a0tea Tools included: Python, Bokeh, Google Cloud Platform (BigQuery, DataStore, Data Studio), PostgreSQL Blockport has been bought by Bux 2017: Freelance Consultant - Technical design and execution of a novel cryptograp crowd-fund method. Whilst working with a tech startup as a financial accountant I worked with stakeholde and external developers to design, test and execute an initial coin offering (ICO). I also led investor relations throughout the funding round and provided 2014: PwC Assurance - Banking & Data engineerin and analytics Deliver the ETL pipeline, analysis and visualizat of large financial datasets including financial journals and loan books. As a chartered accountant and external financial auditor of clients in banking and capital markets in London (HSBC, Barclays, Lloyds, BNP Paribas) I facilitate the transfer of data from client systems before transformi and loading them into our on-prem SQL environmen We recalculat clients\u2019 financial statements and mined the data for additional I also qualified as a chartered accountant with the ICAEW. 2010: PhD: Geotechnic In 2010 I began my PhD researchin granular materials at the University of Natural Resources and Life Sciences in Informatio about my research of silos and granular flows can be found here. 2009: Masters Degree: Civil and During my final year at Edinburgh University the Great Recession arrived. After I graduated I found a job at Starbucks and became curious about finance. I resolved to one day understand \u201chow banks work\u201d. \u2014 @johnmathe"},{"title":"fuse-search-snippets","url":"fuse-search-snippets.html","body":""},{"title":"fuse","url":"fuse.html","body":""},{"title":"landing-images","url":"landing-images.html","body":""},{"title":"title","url":"index.html","body":"This is the page\u00a0conte"},{"title":"Portfolio","url":"portfolio.html","body":"Projects which aren\u2019t covered by a non-disclo agreements or that don\u2019t require access to Data\u00a0Scien View\u00a0Demo Recognisin and labelling traffic lights from An Web\u00a0Apps MoneyBar - A personal finances dashboard. Extract transactio from PDF bank statements from Bunq, ABN AMRO, ING, and\u00a0Raboba Makes your transactio searchable and\u00a0querya Aggregate transactio by time\u00a0perio Calculates income, expenditur and transfers within a time\u00a0perio Automatica PipPip - Write an email now, send it if you die or disappear. Send messages if a particular event\u00a0occu Or send a message on a specific date, even if its many years in the\u00a0future"},{"title":"Snippets","url":"snippets.html","body":""},{"title":"Where and when will the current Bitcoin market\u00a0peak?","url":"when-bitcoin-top.html","body":"TLDR: mid-Septem $300,000 Checklist Top Cap \u2248 Market Cap chart MVRV > 4 chart S2F deflection > 3, but noisy chart 0.875 \u00d7 Delta Cap \u2248 Realised Cap chart. HODL waves - 45% moved in the last 6 months chart 12 Month RSI > 90 chart 3-month coin days destroyed - check glassnodes most recent email and STH and LTH chart Summary S2F model suggests a peak around the beginning of 2021Q4, in the region of $300,000. The rainbow chart seems to broadly agree with S2F. If the age-adjust 3-month coin days destroyed goes above 550,000 then get ready to sell. Willy Woos \u201cdouble top\u201d chart suggests a peak around $400,000. RationalRo comparison of bull runs suggests a market top around 14 September and a maximum price around $2,000,000 [sic]. RationalRo comparison of 12 months RSI suggests that the market top is reached shortly after a 12 month RSI exceeds 90. This seems less reliable than the above points Jurrien Timmer suggests a peak price around $100,000 if price shoots up unexpected quickly. I expect the market top to be significan higher. $100k will be a massive psychologi level. If the price does increase to this level before approximat July then the optimal sell price would therefore not be exactly $100,000. Market top is expected around September 2021 at the earliest. The behaviour of the market will change as its participan change. There hasn\u2019t been a bull run with significan institutio investors before. Sell using a cost averaging strategy. Awareness Buy when everyone is selling, sell when everyone is buying. Be brave when there is fear, if there is no fear then its about to get very messy. If everyone is super confident that prices are definitely going to go up, something bad is about to happen. A lack of uncertaint is a big warning bell. 14 September, $300,000. Threshold Values MVRV MVRV > 3 \u2192 Local top MVRV > 4 \u2192 Macro top MVRV has historical been one of the best on-chain predictors of market tops and bottoms. The ratio of Market Value to Realised Value is calculated by dividing Bitcoin\u2019s market cap by its realised cap. chart Top Cap Top Cap is 35 x Average Cap Market top when Top Cap is equal to Market Cap. Delta Cap Delta cap is Average Cap subtracted from Realised Cap. When Delta Cap is almost Realised Cap, it\u2019s a market top. When Delta cap touches Average Cap, it\u2019s a market bottom. Market Top when Delta cap is within 20%\u00ad15% of Realised Cap S2F Deflection Get key ratio values If s2F deflection > 3, but its noisy HODL Waves >45% of supply has been moved in the last 180 days (6 months) \u2192 Sell >70% of supply has been held over 180 days \u2192 Buy chart - hover the cursor over todays date and add up all the age brackets from 24hr to 3-6 months 12 month RSI > 90 14 month RSI > 95 \u2192 Sell 12 month RSI > 90 \u2192 Sell Noisy - defer to other metrics. SOPR Use 7 day average. 1.04 \u2192 Sell 0.97 \u2192 Buy Noisy - defer to other metrics. Realised Cap > NVT Cap Realised Cap should be lower than NVT Cap. Sell when Realised Cap almost exceeds NVT cap See chart below, there have been false positives. Realised Cap is lower than NVT cap during a bull market only. Noisy, could be a miss. Charts Stock to Flow Model Rainbow Model Top Cap Realised Cap, NVT Delta Cap 3 month coin days destroyed Double top Similariti to previous bull runs Version 1: Version 2: Halving model: 12 Month RSI comparison Bitcoin price history Lowest price forward model Key metrics and terms Average Cap The \u201cforever\u201d moving average of market cap. It is the cumulative sum total of daily market cap values divided by the age of the market in days. Top Cap Average Cap multipled by 35. NVT Cap A valuation using monetray velocity. Checkout CoinMetric for more info. MVRV The ratio of Market Value to Realised Value is calculated by dividing Bitcoin\u2019s market cap by its realised cap. Realised Cap The sum of the products of each UTXO and the market price of Bitcoin when the UTXO was generated. Market Cap The price of the most recent Bitcoin transactio multipled by the number of Bitcoin UTXO Unspent Transactio Outputs. These are kind of like unspent coins. If you have 1.5 BTC then you might have bought 2 and sold 0.5. The total value of UTXOs in your wallet will be 1.5. SOPR The Spent Output Profit Ratio is a measure of the average profit or loss on a coin. Is a coin is moved when the price is higher than when it was received the SOPR increases, if a coin is moved when the price is lower than when the coin was received then SOPR decreases. It won\u2019t be accurate for individual coins but in aggregate it gives an idea of whether coins are being sold at a loss or for profit. Market participan who have owned BTC for 3 months behave differentl to those that have held BTC for 3 years. A more experience investor will likely make more measured and less rash decisions. By segregatin the UTXOs according to age you can compare old coins and new coins, experience and inexperien investors (in aggregate) Weak hands will sell before stronger hands, and when market price decrease it\u2019s useful to know aggregate age data for the coins being sold. If coins are moving from young wallets then the selling is likely much less significan than if coins are being moved onto exchanges from old wallets. aSOPR The Adjusted Spent Output Profit Ratio is the same as SOPR but it ignores coins less than 1 hour old. If profits are taken by old coins, aSOPR trends higher. It will trend lower when older (and therefore profitable coins remain dormant. The higher aSOPR is, the more profit has been taken off the table. When aSOPR is less than 1, spent coints are moved at an aggregate loss. URPD UTXO Realised Price Distributi - If a lot of coins have moved within a particular price band, it is likely that there is strong price support and resistance at this price. This would be truer and more reliable in a mature market. Because the market for Bitcoin is expanding so rapidly and the price is so volatile, the attitudes and expectatio of market participan is also much more malleable than in traditiona finance. For example, what was considered a very high price 12 months ago would be considered a disaster today. RSI The Relative Strength Index is borrowed directly from traditiona finance. You can calculate it over different time periods. Miner Net Position Miner Net Position shows the degree to which, on aggregate, Bitcoin miners are profiting from the coins they\u2019ve generated from mining. Miners are expected to be among the most bullish of all market participan and therefore it is notable when they start moving coins from their mining wallets into exchange wallets. Stock-to-F A stock to flow model is used to measure the scarcity of a commodity. It\u2019s a calculatio based on the ratio of existing supply and how much is being produced. The higher the ratio, the longer it will take for supply to meet existing demand. Gold has a stock to flow ratio of 66, which means it would take 66 years at the current rate of production to produce the amount of gold currently in circulatio Silver has a S2F ratio of 74. BTC has a S2F of about 50. Background Over the past 8 years, Bitcoin has gone through phases of rapid price increase followed by periods of rapid decrease. The price has been driven by increasing market size and a decreasing rate of issuance, and has been so volatile (compared to traditiona finance) that \u201c1 month in cryptocurr markets is like 1 year in traditiona markets\u201d. However volatility is decreasing and we are seeing lower highs and higher lows during each subsequent market cycle. The single biggest factor driving multi-year market cycles appears to be the decreasing rate of supply increase (the issuance rate). The last 3 halvings1 seem to have provoked the last 3 bull cycles. We are in the third bull cycle now (April 2021) and I fully expect it to be followed by a bear cycle. As an amateur investor, I want to buy low and sell high. I\u2019d like to time the top and bottom of the market with reasonable accuracy, just like everyone else. But I\u2019m aware that my methods are less nuanced than profession traders and analysts - I have access to less data than them and I\u2019m not willing to put in as much effort as they are. I\u2019m happy to do this Pareto style - I\u2019ll give it 20% of my maximum effort and I\u2019ll be happy with 80% of an ideal result2. This is a review of what I consider to be the best sources of metrics and analysis that I\u2019ve come across. All the resources used in this article are attributed to the original author and have been made freely available on Twitter. I hope its OK for me to repost them here, if it\u2019s not then let me know and I\u2019ll edit the post. Hopefully helpful links This article assumes some familiarit with blockchain and financial markets. Some more general articles on this site are: Bitcoin compared to Gold How to buy bitcoin Analysts These insights, metrics and charts are the work of the following people and organisati Willy Woo Timothy Peterson PlanB Glass Node CoinMetric Jurrien Timmer Every 210,000 blocks, the number of bitcoin awarded to the miner for successful adding a block is halved. The last halving occurred in May 2020 and the rate of issuance halved from 12.5 BTC/block to 6.25 BTC/block \u21a9I realise this probably isn\u2019t, strictly, what Mr. Pareto was thinking when he published his research. I hope you get my intention. \u21a9"},{"title":"Reading - April\u00a02021","url":"reading-april-2021.html","body":"Table of Contents The Architectu Behind A One-Person Tech\u00a0Start Boring tech behind a one person\u00a0Saa Djangos place in a web Cache invalidati To fix the social sciences, look to the \u201cdark ages\u201d of medicine Emotional resilience in 3 virtues of a\u00a0programm Is WebAssembl magic performanc pixie\u00a0dust Yamauchi No.10 Family\u00a0Off Improve and Extend Your Text Objects A Vim Guide for Adept\u00a0User Deep dive in CORS The Articles and blog posts I saved to read\u00a0later The Architectu Behind A One-Person Tech\u00a0Start An article by Anthony Najjar Simon about how he runs his one-man SaaS using\u00a0Djan Low stress, one-person company, run from a\u00a0flat. Self-funde taking things slow - he likes taking things slow. Grateful to be standing on the shoulders of the open source giants who went ahead and made the OSS stack we all enjoy and benefit\u00a0fr Your own context matters when choosing a tech stack. There is no \u201choly grail\u201d. I guess this means that you should use what you\u2019re familiar with and what is boring for\u00a0you. Uses K8s on AWS. He says \u201cdont fall into the trap of thinking you need to use it too\u201d - expertise with these tools was learnt over several years fighting fires on the day\u00a0job. Productive because he used the tools he is most familiar with and he can focus on\u00a0shippin Django, Celery, PostgreSQL Python, AWS, Redis. - same as MoneyBar and\u00a0PipPip Boring tech behind a one person\u00a0Saa The tech-stack keeps\u00a0evol \u201cBoring\u201d means \u201cwhat I\u2019m familiar with so that I can focus on the\u00a0busine Most of his time is spent talking to people and thinking. He spends 15% of his time on engineerin Probably it was more when he was setting stuff\u00a0up. The methods described in the post are definitely not the best way. It\u2019s just one way of doing engineerin in a very specific type of business. It\u2019s not the only\u00a0way. Django, Celery, PostgreSQL Python, AWS, Redis. - same as MoneyBar and\u00a0PipPip Uses Ansible for provisioni machines. - YAML files. Should look into\u00a0this. \u201cGood enough for\u00a0now\u201d Doesn\u2019t use CI tools, he uses Ansible with a shell script he\u00a0wrote. Uses Datadog for monitoring and alerting, and\u00a0PagerD Uses Rollbar, which seems similar to\u00a0Sentry. Uses Slack, not to talk with humans, but integratio with Rollbar and Datadog, and also slack incoming webhooks to be notified when someone signs up or does something interestin like deletes a\u00a0thing. Make a \u201cding\u201d sound whenever someone signs\u00a0up. He\u2019s always very careful about operationa stuff to prevent downtime and outages. Servers are to protect against spikes caused by sudden Avoids working from home or cafe cos its more productive in an office. Productivi is important - make every minute count. He doesn\u2019t often work more than 8\u00a0hours/da He optimizes for spending less time and making money instead of more time and saving\u00a0mon Uses Vagrant and virtualbox on an MBP so that dev work is on the same systems as prod\u00a0infra Uses 1 monolithic repo. Likes this\u00a0appro He uses contractor Usually, the biggest obstacle to building and shipping things is overthinki What if this, what if that. You are not important at all and no one cares. No one sees. Even if you screw up the initial product launch, no one will care because they\u2019re not looking. Only when you\u2019ve proved that you deserve their attention will you have any attention Think big, start small, act fast. Use boring technology or whatever tech you want. Just make sure you\u2019re solving an Ignore the cargo cult people and ignore the noise. Keep calm and carry\u00a0on. Djangos place in a web Not everything needs to be an SPA Django is a back-end framework, it\u2019ll work with an front-end framework, because separation of\u00a0concern To use django with an SPA, you could use DRF, or just normal views that return JSON. Reading this makes me want to check out React to see what all the fuss is\u00a0about. Cache invalidati Cool and all, but not as powerful as snippets I\u00a0think. Auto correcting typos: teh ->\u00a0the Expanding phrases: kr -> kind\u00a0regar Multi-line It really seems similar to what I\u2019m using UltiSnips\u00a0 I found this question on SO comparing abbreviati and snippets. TLDR: It\u2019s easier to add and maintain snippets than abbreviati and you have less boilerplat with snippets than abbreviati especially in complex\u00a0ca To fix the social sciences, look to the \u201cdark ages\u201d of medicine Emotional resilience in 3 virtues of a\u00a0programm Laziness - The quality that makes you go to great effort to reduce overall energy expenditur It makes you write labor-savi programs that other people will find useful and document what you wrote so you don\u2019t have to answer so many questions about\u00a0it. Impatience - The anger you feel when the computer is being lazy. This makes you write programs that don\u2019t just react to your needs, but actually anticipate them. Or at least pretend\u00a0to Hubris - The quality that makes you write (and maintain) programs that other people won\u2019t want to say bad things\u00a0abo Also, I read a quote somewhere saying the mark of a great program is having people use it in ways you didn\u2019t expect, or something like\u00a0that. Is WebAssembl magic performanc pixie\u00a0dust Yamauchi No.10 Family\u00a0Off A beautifull Improve and Extend Your Text Objects A Vim Guide for Adept\u00a0User How to manipulate multiple quickfix and What are digraphs and how to use\u00a0them. Useful keystrokes in INSERT\u00a0mod Useful keystrokes in VISUAL\u00a0mod Vim Using shell commands in\u00a0Vim. Deep dive in CORS The ps 1 - see which processes are running or sleeping. WCHAN tells you which kernel event a waiting processing is\u00a0awaitin"},{"title":"Learning - April\u00a02021","url":"learning-april-2021.html","body":"Table of Contents Google Cloud Platform Ansible SSH Vagrant Google Cloud Platform It seems like I\u2019m looking for some general overview of how roles are managed, viewed, compared, and\u00a0inheri How can you tell if a users (or a service accounts) roles are adequate, or too much or too little for a particular task? And what\u2019s the difference between a user having some roles, and a user using a service account that has those\u00a0role It would also be nice to have some kind of adversaria test, that would identify how/if users or service accounts can create identities with more flexible permission that their\u00a0own. These short videos are good, but they\u2019re not a complete solution. I\u2019m not sure where to look\u00a0next. Ansible Based on Jeff Geerlings book. There are 15 episodes. Jeff seems like a great guy. I\u2019m going to try listen to one of these each\u00a0day. SSH This is also a very useful article. I made notes from it in another post. Vagrant Good for local developmen (Especiall when Not as good for cloud providers as\u00a0Terrafo No more"},{"title":"Tweets - April\u00a02021","url":"tweets-april-2021.html","body":"Table of Contents Front-End Mental\u00a0Mod Agency Razors Crypto Front-End 6 website for top landing page inspiratio onepagelov .com by @robhope \u2022 lapa .ninja by \u2022 landingfol .com by @dannypost saasframe .io by .com by @Cruip_com \u2022 saaspages .xyz by @Versoly\u2014 Jim Raptis (@d__rapti April 14, 2021 anyone interested in a fun @microacqu 90 or 180 days from start to finish to build and sell a tiny company in public?wou be really great M&A practice for my Jim Bisenius April 14, 2021 Mental\u00a0Mod Tobi's favorite example of FIRST PRINCIPLES is a Truck driver.His truck was sat still for 8 HOURS on THANKSGIVI waiting for his cargo to be unloaded when he realized\u2026\u201c not take the WHOLE trailer off the back of my ship rather than unloading + reloading each\u00a0item? George Mack May 18, 2020 LUTKE LEARNING 6 - TALENT STACK LED BY CURIOSITY > MBA He didn't have an MBA. He didn't grind 100-hour workweeks. Instead, he played video games (which led to coding) and he snowboarde (which led to an online snowboardi store). This 'Talent Stack' led to\u00a0Shopify George Mack May 18, 2020 A super long thread, worth reading it all: Josh Waitzkin might be the most INTERESTIN person alive.He doesn't have Twitter. And he barely uses the internet.I compiled my favorite 5 MENTAL MODELS of his below.THRE George Mack August 8, 2020 Agency 1/ HIGH AGENCY Once you SEE it - you can never UNSEE it. Arguedbly the most important personalit trait you can foster. I've thought about this concept every week for the last two years since I heard discuss it on @tferriss' podcast. THREAD\u2026\u2014 George Mack November 29, 2018 Razors THREAD: 15 of the most useful razors and rules I've found.Rule of thumb that George Mack January 16, 2021 Crypto Now let\u2019s compare this to the stock-to-f model. Below I added in the S2F model, which is the aforementi inflation rate regressed against price. /10 Jurrien Timmer April 13, 2021 #Bitcoin is looking strong at RSI 92. Still not above RSI 95 like 2017, 2013 and 2011 bull markets. I calculated BTC price needed for RSI 95 at April close: $92K. Let's see what the Coinbase IPO will do today\ud83d\ude80 PlanB April 14, 2021"},{"title":"SSH-Notes","url":"ssh-notes.html","body":"Table of Contents Background Authentica Passwords and\u00a0Keys Handshake Background Secure Shell lets you securely connect to remote servers. You connect using an account that exists on the remote server. Once you\u2019ve connected you\u2019ll be dropped into a shell\u00a0sess The computer you connect from is the \u201clocal\u201d or \u201cclient\u201d computer. The computer you connect to is the \u201chost\u201d or the\u00a0\u201cserve When you\u2019re connected to the server using SSH, any commands you input from the client are sent securely and privately (through a \u201ctunnel\u201d) to the remote computer, where the commands are\u00a0execut SSH is implemente using the client-ser model. The server must be running a small app to listen for SSH connection This kind of app is called a daemon (pronounce day-mon). For SSH to work, the host must be running the SSH daemon. The SSH daemon listens for connection on a specific port (22), authentica connection requests, and (if the connection request is approved) it will spawn the correct environmen The correct environmen is a The client (the computer you connect from) must be running an SSH client, which is a small app that can communicat using the SSH protocol. (A protocol is a set of rules.) It needs to be able to receive informatio about which host to connect to, which user to connect as and which credential to use when trying to\u00a0connect Authentica Passwords and\u00a0Keys Clients authentica using passwords or keys. Passwords are less secure than keys. Keys are very secure. You can also connect using no authentica by specifying a particular IP address I\u00a0think. Passwords are encrypted, and are conceptual familiar, but can (and will) be brute forced. There are tools that will block repeated attempts or block/allo authentica attempts from particular IP\u00a0address SSH keys using public and private (or secret) key pairs and are very secure. Keys are generated in pairs. The public key can be shared freely without concern. The secret key must be kept as secure and secret as a\u00a0password To authentica using a key pair, the client must have both keys (a key pair) on their computer. The server must have the public key in a file called If the server has the private key then something has gone wrong and you should be\u00a0alarmed The server stores the public keys of users who can connect as a particular user in the file The file contains a list of public keys, one public key on each\u00a0row. The default location to store the public key on the remote server is in the users home folder in a subdirecto called The full path would be Handshake When a client wants to connect to a server using SSH, it tells the server which public key to use. The server then checks in the file for the public\u00a0key A unique session ID is generated and shared between the client and\u00a0server If the server has the same public key that the client sent when it began the connection attempt, the server generates a random string and encrypts it using the public key. This random string can only be decrypted using the private (secret) key associated with the public\u00a0key The server sends the encrypted string to the client. The client decrypts it using the secret key and combines the original random string with the session ID. The client then hashes the combined string and sends the hash back to the\u00a0server The server already has the hash of the string combining the original random string and the session ID. If the hash from the client matches the hash on the server, the server can be sure that the client has the private\u00a0ke Source"},{"title":"ChezMoi","url":"chezmoi.html","body":"Table of Contents Background Razor One Question Command Reference Links Background Chezmoi seems to be dotfiles management for power users. Until a few days ago, when I realised I\u2019d massively broken a lot of things, I\u2019d been putting my dotfiles in a version controlled directory and using a shell script to generate symlinks in my home directory. This had worked really well for several years. It\u2019s a great system for maintainin dotfiles on a single machine. But it\u2019s not robust or flexible enough for managing multiple machines or multiple operating systems. Also, it\u2019s not just files that sit in $home anymore, I need to track configurat files that live in other places too1. I needed something more robust and flexible than generating symlinks from a bash script. Razor I think I read somewhere that when purchasing a new appliance you should buy the cheapest you think you can get away with, and if that doesn\u2019t work then get the best you can afford. Moving from a custom bash script to Chezmoi is an example of this. My custom bash script was the most basic approach, and it broke badly. Chezmoi seems like the most heavy duty tool for dotfiles management I could find. For example, I think I\u2019m able to encrypt my files using 1Password or similar. I can use Jinja templates to create scripts for different scenarios. I don\u2019t even know what that means at the moment. I\u2019ve only scratched the surface but I\u2019m happily managing dotfiles across different machines and them safely. Chezmoi (aliased to cm) is free and OSS so I\u2019m confident it\u2019ll be \u201cmy\u201d tool for the next few decades. I\u2019m happy to invest a couple of hours to learn a few new habits and iron out a few wrinkles2. One Question I don\u2019t understand why chezmoi cd creates a new shell in order to jump into the Chezmoi directory. Why not simply cd into the Chezmoi directory? What\u2019s the of a new shell? You have to remember to exit after you\u2019ve done whatever you went there to do, but my habit is to cd or z ... I guess it\u2019s nice to exit and then immediatel go back to where ever you were before, but there are other ways of doing that - you could write the current directory to an environmen variable. It seems unnecessar complex. Command Reference Show which files have changed \u2192 cm status List of managed files \u2192 cm managed List of unmanaged files \u2192 cm unmanaged Start tracking a file \u2192 cm add Update a file, add the file again \u2192 cm add .. Edit tracked version of file \u2192 cm edit - don\u2019t think I\u2019m going to use this, I\u2019d rather edit the source file, test it, and then update using cm add .. Difference between local version tracked versions \u2192 cm diff - this tells me which files I need to cm add again. This feels clunky and I suspect there is a better workflow. Parsing git style diff files is horrible. Clobber local version with the tracked version \u2192 cm apply Dry run and see diff between local version and tracked version \u2192 cm -nv apply Pull the latest changes from your repo and apply them \u2192 cm update Remove a file \u2192 Create .chezmoire in the source directory chezmoi apply \u2013remove \u2013dry-run \u2013verbose Type cm instead of chezmoi \u2192 alias cm=\u201dchezmo Links Github Project Site For example, every file in should be version controlled \u21a9Happily, it seems like the amount of time required to learn or become familiar with a new tool is decreasing I guess this is to be expected as experience increases but nonetheles it\u2019s gratifying to management was a fairly new and interestin concept when I first began symlinking into my home directory. It still feels amazing to bootstrap a fresh machine and have it feel like home in just a few minutes, but the technicali of it are now familiar. \u21a9"},{"title":"I Leaked Credentials Onto A Public GitHub\u00a0Repo","url":"i-leaked-credentials-onto-a-public-github-repo.html","body":"Table of Contents Don\u2019t post secrets to public Background The\u00a0hack Remediatio Questions Study Comments Don\u2019t post secrets to public I made this mistake a while ago, and in the interests of openness and learning from others, I\u2019d like to describe what happened. Maybe it\u2019ll help others avoid the mistake, and maybe I\u2019ll learn something from any conversati this Background Using Google Cloud Platform (GCP), I\u2019ve been doing some work across multiple compute instances. Thankfully the work wasn\u2019t business critical or on production systems. My account was isolated away from the rest of the\u00a0busine As the number of servers I was working with increased, I realised I needed to begin using some tools to automate server setup. This lead me to begin using Ansible, and once I\u2019d cobbled together a working playbook I pushed my Ansible project to my GitHub account\u2026 And accidental leaked the key for an account I\u2019d been\u00a0using The\u00a0hack Within a couple of minutes of pushing the repository to GitHub\u00a0I: Made the Stopped tracking the keys in git and removed them from the cache git rm -r --cached <dir>. Received an email from Google saying they\u2019d found OK, close call. The secret was leaked for less than 5 minutes. On my obscure I thought there was nothing to worry about.. But then I noticed some activity in the\u00a0consol Compute instances were being created, I could see the list growing rapidly. Over the next few minutes 195 compute instances and disks were being created, each with a unique name in zones across the world. The format of the name was Where type was either applicatio backup, jenkins, gke, prod, staging, worker, www, build, redis, or runner. Maybe some others too. The number seemed to be 5 random\u00a0dig Some of the instances were ephemeral. They all had delete protection enabled. I checked the details of a few of them and noticed some scripts that included references to\u00a0Monero. So I guess a Monero mining bot was being set\u00a0up. The logs showed that GKE and networking resources had also been requested, but the account which the stolen credential belonged to didn\u2019t have the necessary permission Our project also maxed out its quota of compute instances in multiple regions and\u00a0zones. Remediatio I deleted the account that had been leaked, and began quantifyin the damage. I wanted to know exactly what permission the key had, which resources could be created, and could the leaked account be used to create other accounts? No, it\u00a0can\u2019t. After looking around and becoming confident that it was only 195 compute instances with disks and delete protection that had been created, in regions and zones across the globe, I began to remove them. No other resources had It took me 10 minutes and some googling to create the Get all the compute instances and dump them into a file. I expected to run a script that iterated through the file line by line, setting variables based on the content of the current line: gcloud compute instances list --format zone)' > names.txt In Vim, find the rows that contain the instances that I don\u2019t want to delete, and remove these from the file. There are a handful of compute instances I want to keep, and 195 that I want to remove. :v/node- shows any rows that don\u2019t Loop through the file and for each row, which contains the instance name and its\u00a0zone, Remove Delete the\u00a0instan while IFS=, read -r name zone do gcloud compute instances update $name --zone $zone \\ && gcloud compute instances delete $name --zone $zone --quiet done < names.txt The --quiet flag is necessary because otherwise gcloud will ask me to confirm that I want to delete the Questions I\u2019m surprised by the speed with which the attacker found the leaked credential The repo did not belong to the clients account but my own, and I assume that my account is obscure enough to not be on any interestin lists. If my account is being scanned every few minutes, presumably all accounts are being How many resources are required to do that? I guess if one of these attacks works you can use the stolen compute to scan more repositori for more leaked credential It\u2019s easy to imagine scenarios where large corporatio that are already running complicate cloud infrastruc deployment wouldn\u2019t notice a few (200?) unauthoriz compute Study Service accounts on Google SSH crash\u00a0cour Vagrant crash\u00a0cour IFS= Comments There was some useful discussion about this article on\u00a0Lobste."},{"title":"Ansible","url":"ansible.html","body":"Background I\u2019ve been spending a lot of time lately working on nodes for various blockchain projects (Polkadot, Cardano, Tron, Binance Chain, Ethereum, \u2026). The rosetta api spec is super interestin but like most things in crypto the documentat is sometimes wrong or incomplete and there are bugs and Each of the nodes runs on a separate server, and we typically have one node for mainnet and another for testnet. I\u2019m working across mutiple servers, doing difficult stuff, and I want it to be as easy as\u00a0possibl I need to reduce friction and Accessing the servers is easy - I use Tmux with the continuum and resurrect plugins and maintain different sessions for each type of server. This makes accessing multiple servers during the same work day really simple and effortless But working on the servers is still\u00a0awkw On my dev machine I have zsh with syntax highlighti command completion and various tools, like z to make navigation supper easy. I also have a lot of aliases defined. E.g. .. \u2192 cd ... Working on a remote server should be as convenient and familiar as working on my local machine, so I want to find a way to configure a server the same way as my laptop, and I want to do it automatica so that it can be done many times, with no Ansible Ansible seems to be It\u2019s\u00a0free It\u2019s got all the features and capabiliti you\u2019re going to\u00a0need It\u2019s agentless - you don\u2019t need to install anything on the machine you want to control - you can use Ansible with anything that you can ssh\u00a0into. I used the following resources to get\u00a0starte This useful video gave me some orientatio and helped me figure out what I was aiming for and how to get started. Before watching it, I didn\u2019t know \u201cwhich way was\u00a0up\u201d. This blog post showed me how to create an inventory using the gcp_comput plugin. I spent a lot of time being unnecessar confused about service accounts. I guess until you have 1 success at understand something you don\u2019t know if you\u2019ve misunderst by a little or a\u00a0lot. Once you have an inventory of servers that you want to connect to, you still need to specify (and prepare for) how you will connect to them. I\u2019d hoped that the gcp_comput plugin would do some heavy lifting for me in this step, but it seems not. It can do lots of useful stuff like creating instances and specifying disk space and networks, but it won\u2019t really help you ssh into an instance. No matter\u00a0tho This blog post turned out to be just what I needed. I found it at the beginning of my search when I was trying to create an inventory, and discarded it as almost useful. Turns out that OS Login is the best way to ssh into a GCE instance and once you\u2019ve got your inventory taken care of, this blog post really\u00a0hel When I was installing python modules, I had some errors about pyenv shims being incorrect. The scripts were looking for versions that weren\u2019t present. Running pyenv reshash fixed it. Kind of magically, but\u00a0annoyi Setting up a service account and giving it the correct permission took more time and was more confusing than anything to do with\u00a0Ansib I found this blog post about setting up vim for yml files. The preferred way to install ansible on Mac is using pip. When you use OS Login the username you have when you ssh into the compute instance will change. This SO question explains\u00a0w Commands gcloud auth list ansible-co view|list| -i --graph ansible -i all -m ping"},{"title":"Over-Engineering this\u00a0blog","url":"over-engineering-this-blog.html","body":"Over the last few weeks I\u2019ve spent an unreasonab amount of time and energy making unnecessar improvemen to this\u00a0blog. Some of these Adding keyboard shortcuts (type ? to find out\u00a0which) Implementi then and then optimizing client side fuzzy\u00a0sear Using src-set to serve responsive images Lazy loading images to make this site load\u00a0faste Compressin page files using brotli and also gzip (Precompre Trying (and ultimately failing) to avoid a \u201cwhite flash\u201d when dark mode is chosen and a new page loads (Github discussion I\u2019m not really sure why I did it. It makes almost no difference to anyone but me. It felt a I like tinkering, and it\u2019s nice to build something that will continue to work with no maintenanc I tell myself that over the next few years I will gain the benefits of these features even when I\u2019ve forgotten I It\u2019s taught me a lot of JavaScript which is a great language to be familiar with - it\u2019s everywhere I would warmly encourage someone younger than myself to pursue interests for the sake of curiosity and fun. And there is a very high chance that even if no-one uses the shortcuts except me, my new javascript skills will come in useful But even if they do I\u2019m not sure its a good enough reason - things should be built when they solve a present problem, not for what-ifs and maybes. YAGNI. I wouldn\u2019t let myself do this in a profession capacity. There is a tension between being curious and I\u2019m not really sure that I need to justify myself. Its a hobby, I wanted to do it, I enjoy tinkering with web technologi and learning new\u00a0things But also, I lost sleep over this - I stayed up too late, and let it put pressure on other\u00a0thin I know that being curious, and making room to play with interestin things, has been one of the most useful approaches to personal developmen and up-skillin myself. But there must be a\u00a0limit.. There is a tension between wasting my time and taking a risk, and it will take a few years before I know for sure if these efforts were worthwhile or\u00a0not. If it\u2019s not fun, don\u2019t do\u00a0it. Successful business owners seem to be very good at leaving things alone once they\u2019re \u201cgood enough\u201d, and not being In fact, I think that being a perfection is antithetic to being an entreprene I am not a perfection I\u2019m just really curious and have a big appetite for\u00a0learni But this \u201cappetite for learning\u201d stops me from focussing. I let myself become distracted by adding new features to this blog, when instead I should zoom out a bit and think about working towards a more substantia and meaningful goal, to the exclusion of more minor\u00a0goal I think that good entreprene are very focussed, to a fault. I am not that focussed. I am too distracted by\u00a0life. It\u2019s a balancing act, there is a tension between being emotionall and physically present with my family and friends, and ignoring as many things as possible so that I can focus on doing something meaningful that is"},{"title":"Fuse\u00a0Search","url":"fuse-search.html","body":"Adding search made the site feel faster and more accessible I\u2019ve reimplemen search on this site using fuse.js instead of tinySearch You can read about how I implemente tinysearch here. When I first implemente search I was surprised how much faster and more accessible the site began to feel. I could quickly access any content by typing a few words, I didn\u2019t need to scroll or follow a link.1 This means I can find content without having to think about how to get there - I don\u2019t need to break my flow or concentrat It might sound like a trivially small considerat but lowering friction or cognitive load in small ways can make the difference between using or not using something when you\u2019re already working hard or concentrat on something else. For example, if I want to look up my notes about using the nohup command, I can quickly go to the site, type / (the keyboard shortcut for search), type \u201cnohup\u201d and hit enter. This is all muscle-mem level impulses. I don\u2019t need to think about the content, think about its category or when I posted it, then scroll down and scan a list, or use a mouse to click on intermedia links. Win. Working at the speed of thought rather than the speed of input is a big deal. Why I switched from tinySearch to Fuse.js Before implementi fuse.js, this site had a search feature powered by TinySearch I wouldn\u2019t have had enough knowledge to implement fuse.js if I hadn\u2019t already learnt some JavaScript whilst setting-up tinySearch TinySearch had an example for Pelican Blogs, and a simple and clear readme. By using tinySearch first I saw an example of how to build the JSON array that becomes the search index, and how to implement the javascript that\u2019s required for client side search. Also, in the course of developing and this blog I\u2019ve become much more proficient and comfortabl with JavaScript (and jQuery) in general. Fuse.js is really quite simple to set up once you\u2019re familiar with JavaScript It\u2019s much more flexible than tinySearch you can choose search weights for different fields, accuracy thresholds and some parameters for the fuzzy search algorithm. The general approach is to instantiat an instance of Fuse by calling Fuse with a JSON array for it to parse, along with some options. You then give the instance a string and get back an array of results which you can do whatever you want with. The accuracy of the search results is higher with fuse.js and the speed is still acceptable I did have to do some optimizati of the search index that Fuse generates, though. Optimizing the search index The \u201cnormal\u201d search index that Fuse uses to return results is a JSON array of all the content of all the articles that you want to be able to search. You can generate it using a jinja template or any other way you want. (There simply needs to be a JSON array that the browser downloads and does a fuzzy search on). This gave me a file that was about 4MB. Once I asked Fuse to search the complete text of each article (not just the default first 600 chars, iirc) then speed really suffered. I optimized the index file in the following three ways: Removed any non-words. Some of my articles are jupyter notebooks that have been converted to articles (the plugin to do this is one of the reasons why I began using Pelican). When the index is built, lots of code and html gets included, which isn\u2019t helpful. Any \u201cwords\u201d that are more than 20 chars I just delete. Removed the 150 most common words. Any word that is in many articles is not useful for distinguis between different articles, so they can be deleted from the index. They don\u2019t add any meaning. I wrote a short pipeline of shell commands using tr, sort, uiq to generate a file with a list of the most common words. I then wrote a python script to update the original search index by removing all the common words. Shortened any long words by only keeping the first 12 characters If a word was 15 characters long, I simply removed that last 3 chars. I figured this would work fine because matching the first 12 characters would already be quite unique and give a good result. Doing these 3 optimizati reduced the file size by about 90%. Compressin the JSON using gzip or brotli makes the files even smaller, and now the amount of data transferre to the client seems reasonably small. (This is a static site, and therefore search has to happen client side.) The browser would still begin to lag as the search string length increased. It takes more time to search for a 10 character string than for a 5 character string, and initially fuse was doing a search every time a character was typed. I wanted the site to feel as fast as possible and thought that if search was paused whilst typing and occurred a short time after the last key was pressed this would be an improvemen I added a short delay of 200ms to the function call, and typing during the delay time resets the time. This reduced the lag and made the search tool feel responsive I learnt that this is called \u201cdebouncin There was some further complexity when I wanted to debounce characters used for searching, but not the navigation or keyboard shortcuts. Getting the debounce function to only run on some key presses was surprising complex. It taught me a lot of JavaScript though, and it\u2019s satisfying to have made a useful user interface. also immediatel gave me the idea to add keyboard shortcuts. Type ? to see what happened \u21a9"},{"title":"Python\u00a0Notes","url":"python-notes-2.html","body":"__call__() In Python, every time you call a function or method such as my_functio or the interprete will replace the ( with .__call__( >>> def >>> return x+1 >>> 3 class Prefixer: def __init__(s prefix): self.prefi = prefix def __call__(s message): return self.prefi + message Then use prefixer like\u00a0this: >>> simonsays = says: \") >>> up high!\") 'Simon says: jump up high!' Every time you call a function or method, you\u2019re really just calling a built in __call__ method. There should be one, and preferably only one, obvious way to do\u00a0somethi It\u2019s in the \u2018zen of Python\u2019, which is a set of guidelines that help make design decisions. It\u2019s a choice that Python made, and other languages do There are different levels to languages and this applies more to the idiom level than the design pattern level. It applies even less at the architectu level where there can be several equally good ways of organizing business logic and Perl has the \u201cTMTOWTDI\u201d (tim towtdi) principle - \u201cThere\u2019s More Than One Way To Do It\u201d. Perl\u2019s philosophy is to give users more than one way to do\u00a0somethi"},{"title":"Adding\u00a0Search","url":"Adding-search.html","body":"I\u2019ve added search to this blog. Results are generated as you type. Try it by typing / or cmd-k. If you look on the Pelican plugins index you\u2019ll see that Tipue search is the only search tool with a ready-made Pelican plugin, but unfortunat the project seems to have died and the projects website is now But searching a static site must be quite a common need and googling for alternativ gave me a few choices. Lunr.js seems to be the most popular, but it also seemed fairly complicate and like it was probably more than I needed. I went with Tiny Search because it seemed to do what I needed and was easy to setup. There\u2019s even an example for Pelican\u00a0bl One hurdle to success was minimising the false positives. The default settings seem to prioritise keeping the size of the index small (tiny) over giving a good user experience Maybe its because the amount of text on my site is significan less, or more, than the typical use case. Either way, after checking the project\u2019s issues on Github I found an issue that matched my problem perfectly. The solution is to increase the tiny_magic variable at build\u00a0time According to the Readme, this requires using a container and building the index using docker run.... Unfortunat the Dockerfile wouldn\u2019t complete without errors. Checking the issues again and adding to the discussion resulted in an alternativ Dockerfile being suggested, which works. Woohoo! I could then build the search index with a massive tiny_magic value\u00a0(204 Then something weird happened. I write in Vim and I use fzf to find and open files. I realised that fzf had stopped working. After some investigat I realised it was only not working in the blog project, and that fzf.vim calls the fzf CLI tool, which in turn calls the ripgrep tool. The underlying issue was that ripgrep wasn\u2019t working, and after a few hours (sob) of debugging, I found out that one of the things that makes rg special is that it ignores stuff in your .gitignore file. Sneakily, and without me noticing, the Docker image for constructi the tinysearch files had created a .gitignore file with a single entry. The entry was *, which selects everything So rg was ignoring everything and giving no results. Which meant I couldn\u2019t find and open\u00a0files I still don\u2019t know how (or which part of) the Dockerfile does this, so I\u2019ve created a file which contains the correct content, and after I generate a new search index I replace the new traitorous .gitignore with the contents of I\u2019ll come back to it later when/if I have a better understand of Dockerfile syntax, or\u00a0Rust. Adding search to the site made the content feel a lot closer and more accessible Once it was working I immediatel wanted to use some keyboard shortcuts to open the search box and select results. Kind of like does it. It feels really fast and\u00a0precis Googling for some jquery packages, and also some vanilla javaScript showed me enough to get things working. You can hit / or ctrl-k or cmd-k and bring up a search box that populates results as you\u00a0type! Only whole words are matched unfortunat but its still a super useful feature. The search index includes article content as well as article titles and categories I\u2019d like to tweak a few of the keyboard shortcut behaviours and add the contents of various pages (which aren\u2019t articles) to the search\u00a0ind Update I\u2019ve reimplemen search using fuse.js. You can read about it here"},{"title":"Cardano: Generating\u00a0Addresses","url":"cardano-generating-addresses.html","body":"If many different customers are to deposit or send ADA (The unit of currency on the Cardano blockchain to a Cardano node, it will be necessary to determine which customer is responsibl for each transactio so that the correct customer account can be\u00a0updated As with many things involving blockchain this initially seemed like a simple requiremen but involved several hours of\u00a0work. Cardano wallets are generated using a parameter called The default value is 20, and is the number of unused addresses that the node will generate and return to a client using the REST API. If one of the addresses is used, the node will automatica generate another so that there are always 20 This is probably very convenient for personal use. If I want someone to send me some funds, I can make a simple api call using cURL and get a fresh address. But if you are running a service, weather its e-commerce or a financial service, its not really good enough. Some advice on the forums says to generate a wallet with a very large value such as 10,000 and just generate a new wallet when you run out of fresh addresses, but it still feels like a\u00a0compromi But lets explain our situation in more detail first. If a customer wants to send us some ADA, we want to give them a fresh address that\u2019s never been used before and that only they have. Then we know that any funds that arrive to that address are from a particular customer. However we don\u2019t know if the customer will actually use the address and transfer any funds. The address might remain unused or it might not. Neverthele that address is now reserved for them, and no one else can use\u00a0it. In this way, we might need to generate and maintain a list of thousands of addresses that are never used. Using for this seems like a bad\u00a0soluti Fortunatel has the answer, albeit in a fairly convoluted and obscured form. If you have the mnemonic that was used to generate a wallet originally you can generate 2^31 unique addresses like\u00a0so: Clone the repo and build the docker\u00a0ima git clone docker build -t . Get the mnemonic and generate a file containing a list of space separated words on one\u00a0row. Run the\u00a0follow export increment= && cat mnumonic.t | docker run --rm -i key Shelley | docker run --rm -i key child | docker run --rm -i key public | docker run --rm -i address payment --network- testnet > payment.ad && cat payment.ad ;echo"},{"title":"More VIM\u00a0Notes","url":"more-vim-notes.html","body":"q: - opens the command line window. Good for yanking and viewing :UltiSnips - opens the ultiSnips file for the current buffers filetype. See which snippets are\u00a0define The Valuable Dev has the following gems that I\u2019d like to start\u00a0usin gf - edit the file at the file path under the\u00a0cursor gx - open the file at the file path under the\u00a0cursor [m, ]m - move to the start or end of a\u00a0method @: - repeat the last\u00a0comma :<C-f> - open command history\u00a0li >> will indent a line. . will repeat the operation, so >>.. would indent a line 3\u00a0times. You can use this along with a count, which will do the indention for n number of lines (with the current line being the top line). 3>>.. will indent 3 lines 3 blocks to the\u00a0right. <C-y> - moves screen up one line, and moves the cursor if it would go off the\u00a0screen <C-e> - moves the screen down one lines, and moves the cursor if the would go off\u00a0screen <C-f> - move screen down one page, with cursor at top of\u00a0screen <C-b> - move screen up one page, with cursor at bottom of\u00a0screen Vim for Python has some great notes on linting and code completion plugins that I\u2019ve either copied or was more or less doing\u00a0alre"},{"title":"Two Years Of\u00a0Vim","url":"two-years-of-vim.html","body":"I\u2019ve been feeling very comfortabl with my Vim + Tmux setup recently. Navigating around shells and files isn\u2019t taking much mental effort It\u2019s taken about 2 years of working full time with vim to get to the stage where the commands are so I pepper text files outside of vim (email, notes, etc) with vim keys accidental - j k x etc I can\u2019t remember what the command is to do something if I\u2019m not actually doing it. When I need to do an action, I do it from muscle memory and I only pay attention to the underlying key press if something goes\u00a0wrong This is noticeable when trying to find an unbound key combinatio for some new action, or when reading an article about vim and thinking \u201cthat\u2019s new\u201d when actually I\u2019ve been doing it A pleasant surprise has been that it doesn\u2019t take much effort to rebind a single command and retrain myself to use it. This is presumably because the mental effort for all the other commands has become negligible In the early days, retraining a key combinatio took a lot more effort because I was already making an effort to get used to doing things in\u00a0Vim. I can work even when my vision is blurry (and my speech slurred and my head heavy) because I can use text objects and navigation commands to get to where I know text is. I\u2019m not saying I should work when I\u2019m that tired, but I can, if I\u2019m already familiar with"},{"title":"Binance-Chain: Running a\u00a0node","url":"binance-node-api.html","body":"This week I\u2019ve been setting up a binance-ch node. Unlike Polkadot or Cardano, I\u2019m not going to run it from a container until it\u2019s working reliably. The Binance docs show a couple of ways to install a node. I used the install.sh script and went with default values as much as possible. Installati My first attempts to sync a full node used the install.sh script, but the node wouldn\u2019t sync completely it would get stuck. I setup a new VM and did a manual install (\u201cOption Two\u201d) and so far the node has been syncing without any issues. You need to download the genesis file separately in this case. Also, be sure to download the node-binar repo using git lfs and not just git. It will look like it worked but bnbchaind wont have completely downloaded unless you use lfs It took me awhile to realise that the documentat assumes that you have an environmen variable called BNCHOME. You can either create it using export (like you would for any environmen variable) or replace the environmen variable in the start node command with the file path: nohup bnbchaind start --home BNCHOME & Note: I\u2019m not sure if the bnbchaind needs the environmen variable to be set or not. It doesn\u2019t give errors if it isn\u2019t set, but I seem to be having more success when BNCHOME is defined. Syncing the node There are three ways to sync a node. Fast-sync isn\u2019t the fastest way to sync your node, hot-sync is. Using install.sh should put the correct default values in the file, but I needed to adjust ping_inter and pong_timeo to the recommende values. Surprises The documentat assumes you have familiarit with running tasks in the background of a terminal session, and that you\u2019re familiar with nohup. I wasn\u2019t - I\u2019d even forgotten what the & symbol does1 so I did some research and wrote some notesIt starts a process in the background You can move it to the foreground with fg or see a list of running jobs using jobs. You can move a running job to the background (like a vim session) using ctrl-z \u21a9"},{"title":"nohup and Background\u00a0Processes","url":"nohup-and-background-processes.html","body":"Stop stuff from\u00a0stopp If you run a command in a terminal session and the terminal session is disconnect the processes running in it will also be\u00a0termina I discovered this when I was trying to download a ~500gb database overnight. I logged in the next morning expecting to see a completed download, but found I only had half the\u00a0file. Use nohup to ignore HUP signals One solution to this seems to be to use nohup, a command that ignores the HUP signal. It stops your programme from stopping if the terminal session its running in is\u00a0stopped By convention the HUP signal is the method used by a terminal to warn dependent processes that it is about to\u00a0logout. You probably want to run nohup in the background You might want to prevent it from creating nohup.out. Close or redirect fd0 -\u00a0fd2 On Linux, nohup automatica closes stdin. If you\u2019re using MacOS or BSD this doesn\u2019t automatica happen, so you might want to redirect it yourself. This is because if a background process tries to read anything from stdin then it will pause itself whilst it waits for you to bring it to the foreground and type some input. This is probably a waste of\u00a0time. If nohup detects that you have redirected stdout and stderr then it won\u2019t create nohup.out. As with all commands, if you put & at the end of the command, it will run in the background You can bring it to the foreground by running fg, or see a list of jobs by running jobs. If you redirect input to /dev/null (</dev/nul you will stop the program from receiving keyboard (stdin) input, but you won\u2019t prevent it from accessing the terminal directly. Also you haven\u2019t removed the program from the shell\u2019s process\u00a0gr Stopping signals using disown If you want to remove a program from the shell\u2019s process group, then immediatel after you\u2019ve run the command to start your programme, run disown with no arguments. This will make the background process no longer associated with the shell job and it wont have any signals forwarded to it by the\u00a0shell. A disowned process gets nothing (no signals) sent to it by the shell. But without a nohup it will still be sent a HUP signal sent via other means, such as a manual kill command. A nohuped process will ignore any and all HUP signal, no matter how they are\u00a0sent. Source Use w to see who is logged in and what they are doing. It\u2019s summary of every user logged into a computer, what each user is currently doing, and the load all the It\u2019s a combinatio of who, uptime, and ps -a. Process\u00a0Gr A collection of one or more processes. It\u2019s used to control the distributi of a signal. When a signal is directed to a process group, the signal is delivered to each process that is a member of the\u00a0group. Sessions A collection of one or more process groups. A process may not create a process group that belongs to another session. A process is not permitted to join a process group that is a member of another session. A process is not permitted to migrate from one session to\u00a0another"},{"title":"File Descriptors and\u00a0/dev/null","url":"file-descriptors.html","body":"/dev/null In Linux everything is a file, including virtual devices like keyboards. Processes (programme can request access to or from these\u00a0devi The only difference between these virtual device \u201cfiles\u201d and real files, is that for a virtual device the OS generates the data that goes into the file, instead of reading the data from\u00a0stora /dev/null is a virtual device that looks like a file and is used to write output into a black hole that is discarded, lost forever and never seen. It isn\u2019t written to the\u00a0termin File descriptor are integer values assigned to a\u00a0file. stdin has a file descriptor of\u00a00 stdout has a file descriptor of\u00a01 stderr has a file descriptor of\u00a02 Two outputs are generated whenever a CLI is run stdout and stderr. By default, both these data streams are associated with the terminal. You can use file descriptor to redirect\u00a0t If a command exits successful the exit status is\u00a00. If it exits the exit status will be If you don\u2019t specify which file descriptor you want to use, bash will use stdout by\u00a0default The following redirects stdout away from the terminal and into /dev/null. $ echo \u201cHello World\u201d > log.txt This will redirect stderr into a\u00a0file: $ asdfadsa 2> error.txt If you run a command that generates lots of error messages along with \u201cgood\u201d messages, you can redirect all the error messages (stderr) into /dev/null so that you can only see the useful stdout $ grep -r hello /sys/ 2> /dev/null If you want to run a command and only see the errors, (stderr) then you can filter out all the stdout by redirectin the stdout messages to /dev/null: $ ping google.com 1> /dev/null Redirect all output into /dev/null if you want a command to run\u00a0quietl Redirect all the output. The command below redirects stdout to /dev/null (the default file descriptor is 1 if it isn\u2019t specified) and then redirects file descriptor 2 into file descriptor $ grep -r hello /sys/ > /dev/null 2>&1 Read input from a file instead of the\u00a0termin 0<infile Direct stderr to append to a 2>>logfile Combining 2>&1 means send stderr wherever stdout is going. This means that you\u2019ve combined stdout and stderr into one data stream and you can\u2019t separate them anymore. It also means you can pipe stderr the same as you can stdout. Input You can redirect stdin similarly. If you run </dev/null then if the program attempt to read from stdin then it will The merge (or redirect) syntax (for example <&2) won\u2019t work, because you can only redirect in the"},{"title":"Cardano: Running a full\u00a0node","url":"cardano-node-api.html","body":"I recently deployed a Cardano node on Google Cloud Platform and used its API to create and watch addresses, and make transactio Helpfully, Cardano make it quite simple to get up and running if you are familiar with and know where to look, and what questions to ask. Table of Contents Docker Compose Cardano wallet Cardano-CL REST API Surprising things Docker Compose The Cardano Wallet repo1 contains almost all you need to get started. The command to run is: docker-com up -d This does a couple of things for you: Creates a Cardano node and begins syncing with the network Creates a Cardano Wallet instance Creates all the required data volumes Maps the ports required to make API calls. Running docker ps should show that two containers are running, cardano-no and Cardano wallet In order to run cardano-wa commands (not using the API, but directly on the node) you\u2019ll need to docker exec into the container like this: sudo docker exec -it sh Then you can run commands like: cardano-wa network informatio Cardano-CL Similarly, if you want to use the cardano-cl programme, exec into the cardano-wa container: sudo docker exec -it sh cardano-cl \u2014version REST API Perhaps you wont need to do this though because once the containers are up and running and online, you can use the REST API to monitor the node, make transactio and watch addresses. For example, a good test to see if the node is ok is to run curl Addresses on Cardano need to be BIP39 compliant, and before you can use the REST API to create the address you will need to have already generated the keys and the mnemonic. This can be done using various other tools (web page, python) and the results put into a JSON file according to the API spec. Surprising things Cardano requires that addresses are created sequential and instead of allowing the user to generate them ad-hoc, the node by default will manage the creation of addresses of each wallet.3 The value of sets the number of unused addresses in each wallet. By default this is 20. When an address is used, the node will automatica generate a new unused address for the wallet, so that there is always a pool of 20 unused \u21a9"},{"title":"Polkadot: Running and interacting with a full\u00a0node","url":"running-a-polkadot-node.html","body":"I recently set up a Polkadot node on Google Cloud Platform that could create addresses and Instead of building from source I used Docker. After some I found the command to run\u00a0is: docker run -it -p 30333:3033 -p 9944:9944 -p 80:9933 -v --rpc-exte --rpc-cors --chain westend --ws-exter This differs from the (current) documentat in two\u00a0ways: The data volume needs to point to The symlink that is supposed to exist for /data appears to be broken in the current image. See this GitHub issue for\u00a0detail Port 9944 needs to be\u00a0mapped. One of the first API calls you are likely to make to check that things are working as expected, particular for WebSocket connection is to open to a WS connection to be notified when your node syncs a new block. The node only does this once it has caught up with its piers. Whilst it is still syncing it will only return the current highest block when you make the initial API call. I ended up chatting to one of the Parity devs about this issue on discord and then on Stack Overflow. Other than that, everything went as described in"},{"title":"Microservices, Docker,\u00a0Azure","url":"microservices_events_docker.html","body":"This is a great presentati about microservi and event And this is a comprehens overview of\u00a0Azure"},{"title":"Regrets Of The\u00a0Dying","url":"regrets-of-the-dying.html","body":"I wish that: I\u2019d had the courage to live a life true to myself, not the life others expected of me. I hadn\u2019t worked so hard. I\u2019d had the courage to express my feelings. I had stayed in touch with my friends. I\u2019d let myself be happier. \u2026the five most common regrets of terminally ill patients, as described by a palliative care nurse1:Sou \u21a9"},{"title":"A List Of Unconnected Thoughts And\u00a0Aphorisms","url":"a-list-of-unconnected-thoughts-and-aphorisms.html","body":"Do the people you care about love you back? It does not matter how slowly you go as long as you do not stop. Assume that nobody is going to help you, and nobody is going to stop you. Money is (just) fuel. Lets talk about the end. My current self, existing now, is to be seen in the context of my end, and my beginning. When you want money, ask for advice. When you want advice, ask for money. You follow the rules of war for you, not your enemy. You fight by rules to keep your humanity.1 If you want to be a top performer in any field you must become abnormal. Normal is average, which by definition is far below the top! \u201cWe can always be average and do what\u2019s normal. I\u2019m not in this to do what\u2019s normal.\u201d \u2013Kobe Bryant Becoming Abnormally Good2 at something: No reasonable excuses - don\u2019t make (reasonabl excuses that let you off the hook. You need the negative emotions that leads to inspired decision making. Not Worrying About What You Can\u2019t Control We know it\u2019s best to be genuinely friendly no matter who you\u2019re trying to get informatio out of, thanks in part to the work of Hanns Scharff and a slew of studies on interrogat techniques source \u21a9source \u21a9source \u21a9"},{"title":"Notes From An Interview With Geoffrey\u00a0Hinton","url":"notes-from-an-interview-with-geoffrey-hinton.html","body":"Read the literature but don\u2019t read too much of\u00a0it. Trust your intuitions because if you don\u2019t trust them then there\u2019s no\u00a0point \u2018This person is either drunk or stupid\u2018 - feedback on one of his papers that went on to For creative researcher read a bit of the literature and notice something that you think everybody is doing wrong. (I guess GH is contrarian in this\u00a0sense Look for a problem that doesn\u2019t feel right. Then figure out how to do it\u00a0right. When people tell you that your (contraria approach is just no good, just keep at\u00a0it. Either your intuitions are good or they\u2019re not. If they are good then you should follow them and you will eventually be successful If they are bad then it doesn\u2019t matter what you\u00a0do. You might as well trust When you try to replicate a published paper you discover all the little tricks needed to make it\u00a0work. Never If you give a student something to do and they are a bad student then they\u2019ll come back and say it didn\u2019t work. They\u2019ll say this because of some little decision they made which they didn\u2019t realise was crucial. But if you give it to a good student, you can give them anything and they will come back and say it\u00a0works Read enough so that you start developing intuitions and then trust your intuitions and go for it! - Don\u2019t be too worried if everybody else says its\u00a0nonsen If you think it\u2019s a really good idea, and others tell you its complete nonsense, then you know you are really One example of this is when Ramford and I first came up with variationa methods, I sent a mail explaining it to a former student, who showed it to his colleagues He told me that they said \u2018Either this guy is drunk or he is just stupid\u2019. They really really thought is was\u00a0nonsen Maybe that is partly because of how i explained it - I explained it in intuitive terms, but when you have what you think is a good idea and other people think it is complete rubbish, that is the sign of a really good\u00a0idea. See if you can find an advisor who has beliefs similar to your own, because if you work on stuff that your adviser feels deeply about, then you\u2019ll get a lot of good advice and time from\u00a0them. Read enough so that you can trust"},{"title":"Questions For Interesting\u00a0Conversations","url":"questions.html","body":"What\u2019s the best piece of advice you ever\u00a0recei What would you say to your 18-year-ol self if you had a chat with them\u00a0today Have you ever lost or rejected a friend? Tell me about a time when you changed your mind. What have you been thinking about lately? What\u2019s on your\u00a0mind? Do you work in a language that\u2019s not your mother tongue? What\u2019s it like to live in translatio If there is a choice between rememberin and forgetting do you lean towards the side of\u00a0forgett Were you raised for autonomy or loyalty? Was there ever a moment where you thought, \u2018I\u2019m giving up\u2019 \u2014 what did you\u00a0do? Was your identity given to you or chosen by you?"},{"title":"Portfolio: Image\u00a0Recognition","url":"portfolio-image-recognition.html","body":"I\u2019ve created a computer vision model that recognizes traffic lights. I\u2019ve also created a page where the model can be queried and the results shown. This needed to be a custom page and not a typical blog article, so its hosted in the portfolio section here. This page will redirect automatica if you view it in a browser, but for those of you reading this using RSS, head on over and see the\u00a0result"},{"title":"Photographs","url":"photographs.html","body":""},{"title":"Notes From \u201cMastering Vim\u00a0Quickly\u201d","url":"notes-from-mastering-vim-quickly.html","body":"Table of Contents Verbs Registers Insert\u00a0Mod Normal\u00a0Mod Command\u00a0Mo Visual Block Mode\u00a0<C-V> Ranges Searching Undo Splits Macros Other Verbs s - delete char under cursor and enter Insert\u00a0Mod r - replace char under\u00a0curs c/hello - change until next occurrence of\u00a0\u2018hello\u2019 Registers \"ayy yank the entire row into register a. \"Ay yank to register A and append the new text to the existing contents of the\u00a0regist :registers - preview the contents of Insert\u00a0Mod <C-W> - delete back one\u00a0word. <C-U> - delete back to the start of the line or start of cgn - if you are searching for a word (either by using / or * or #) and you want to change each instance of the search result, use <verb>gn to change or delete and then go to the next result. This will let you use the .dot operator to repeat both the steps (moving and\u00a0changi <C-R> 0 - paste. <C-R><C-P> if there are new-line chars Normal\u00a0Mod <C-A> or <C-X increase or decrease a\u00a0number. Command\u00a0Mo set ft? - find out which filetype is\u00a0loaded. Visual Block Mode <C-V> Select a column of numbers you want to increment, then g<C-A> will turn them into an Ranges :put =range(1,1 - insert a list of :for i in range(1,10 | put | endfor - use a loop to generate a long\u00a0list. Searching g# or g* for partial matches, like # or * for exact\u00a0matc Search for the word under the cursor, or similar: Press /. <C-R> <C-W> - this will copy and paste the word under the cursor into the search box. Edit it as\u00a0necessa After you\u2019ve done your search, <C-o> to jump back to where your cursor was\u00a0before Find and replace whole words only: Find and replace either old-word1 or old_word2: g <C-G> - show some stats about current bugger - word count, line count, char\u00a0count Undo g- and g+ - undo\u00a0branc Under changes within a period of time: :earlier 2d - undo changes in the last 2\u00a0days :later 5m - redo all changes in the last 5\u00a0minutes :earlier 3f - undo all changes in the last three buffer\u00a0wri s seconds, m minutes, h hours, d days, f saves @a - Use the global command to execute macro a on all lines of the current buffer containing - For every line containing \u201cgood\u201d substitute all \u201cbad\u201d with\u00a0\u201cugly Splits <C-W> r - rotate the splits from left to right but only if they are split vertically <C-W> R - rotate the splits from right to left. <C-W> H - move the current split to the far left and make it full height. <C-W> J - move the current split to the bottom of the screen and use the full\u00a0width :only - close all splits except the current\u00a0sp Macros @o - do the macro stored in buffer O on all lines that match the Other <C-O> in Insert Mode will jump you into Command Mode for one command only and then put you back into Insert The .dot command only repeats commands that changes the buffer content. It wont repeat"},{"title":"Notes From \u201cPowerful\u00a0Python\u201d","url":"python-notes.html","body":"The parts of Aaron Maxwell\u2019s Power Python newsletter that I don\u2019t want to forget: Table of Contents Emergent Abstractio Practioner Engineer, Scientist Sentinel Values Levels of Python Code Read PEPs Emergent Abstractio Get used to expecting and letting abstractio emerge from projects. If you find yourself repeatedly solving similar problems in similar ways, what can you do that will simplify the code and the Is it a couple of convenienc methods on some helper class? The below code snippet gives you three ways of instantiat the twitter API client: A generic \u201cnormal\u201d way A specialize way that looks for certain environmen variables A specialize way that looks for a configurat file import os # so you can get the environmen variables import twitter # class ApiClient: def __init__(s consumer_k self.api = twitter.Ap consumer_k @classmeth def return cls( @classmeth def path): with open(path) as config_fil # ... return cls(...) # ... Practioner Engineer, Scientist Practioner - You can use a thing (a framework, a tool) Engineer - You can use a thing and if you needed to, you could recreate it Scientist - You can create frameworks and paradigms that have never existed before Aim for the engineer level. Sentinel Values Instead of setting your sentinel value to something that is not quite impossible like None or \"None\" set it to object() This is better because it creates a unique instance of the object class and there can be no ambiguity about where it came from. A sentinel value is a value you can set a variable to. It\u2019s special because it differs from all other legal or possible values that the variable could have. It\u2019s used as a signal or as a canary that something has happened. Levels of Python Code Syntax - understand what indentatio is important, sometimes you need parenthesi etc Idiom - the building blocks of a program. \u201cParagraph of code that follow common patterns, like for loops, or context managers. Design Pattern - Less well defined that Idioms, but more useful. - Creational Patterns, like factories - Structural Patterns, like Adapters or Proxies - Behavioura Patterns, like Visitor or Strategy These tend to be the same across different languages. Architectu - the large-scal structure of your software system. The language itself doesn\u2019t make a lot of difference an applicatio would have the same architectu whether it is written in Python or Java. The interface between different components would be different, but the \u201corgans\u201d of the body would essentiall be the same. Read PEPs A Python Enhancemen Proposal is a document that\u2019s written to propose a new feature of Python. It fully details the proposed feature, the arguments for and against it, and lots of sample code. If the PEP is accepted into a future version of Python, the PEP becomes the authoritat document for that feature and how to use it. PEPs tend to be written by the best programmer in the world, so hang out with them. Abstractio as a principal of OOP \u21a9"},{"title":"Mental Models I Used To\u00a0Use","url":"mental-models-i-used-to-use.html","body":"The rules1 and mental models that helped me succeed in one season or phase of life may not be the best for the next phase. Here is a list of a few mental models I remember being concious of in previous years. Probably I\u2019ve already forgotten some. Always ask \u201cwhy\u2026\u201d. Be obsessive about this. It\u2019s going to make things harder for a while before things get easier. You\u2019ll find difficult answers that you otherwise wouldn\u2019t. If you\u2019re only concerned with the present then its true that ignorance is bliss, but otherwise it\u2019s a liability. \u201cWhat if\u2026\u201d is another good question to ask a lot. Adapt to the situation, don\u2019t make it adapt to you if you have any choice. Be kind of like water, going around things and through the gaps. Look for the edges and the gaps, the parts that aren\u2019t well known. Let people talk as much as they want to. Shut up and listen. If they mean you harm or don\u2019t respect you then it\u2019ll become more obvious the more they keep talking. If they mean you well or they\u2019re saying something useful, you will benefit more from letting them talk more. Inversion - it can be hard to know if you should do something, but how would you feel if you didn\u2019t do it, or if it didn\u2019t happen? Regrets are inevitable everyone has them. Same as making mistakes. Let your regrets be for things that you did do, and not what you didn\u2019t do. If you are willing to try something, fail at it, and still be glad that you tried, then you should almost definitely do it. Commit to it and enjoy the experience Don\u2019t be scared, or at least, be scared and optimistic and happy2. There is beauty and luxury in being in such a bad spot that you are backed into a corner with seemingly no way out. Things become black and white, instead of shades of gray, and that will make priorities and options much clearer. You are likely to work very efficientl and effectivel in this scenario, and you will learn important things about yourself. Now that I\u2019m older and I have I can\u2019t ever let things become so bad that a situation becomes black and white. I have to navigate a world of grays. If they do become black and white, I\u2019ll already have a long list of failings. When I was younger, things were more fragile. My resources were smaller and things could quickly flip from good to bad. Enjoy the few benefits that a situation like that gives, because (hopefully once its gone its gone for good. The best way to solve a problem is to prevent it from occurring in the first place. Succeeding at this will bring its own challenges Take responsibi for things you are not responsibl for, kind of. Do it deliberate and for your own benefit, but don\u2019t forget that you are only pretending that it\u2019s your If you do this, you will force yourself to understand a situation more deeply and from other peoples perspectiv This will let you learn faster and help you in future. Keep this at arms length though - it\u2019s make-belie and you need to be able to switch it on and off. It\u2019s a toy for you to play with. This seems to be what \u201cextreme is. I think its important to have mental models that you\u2019re comfortabl with, because it lets you make decisions quickly and consistent But understand that the map is not the territory, and these are just tools in a toolbox.or policies \u21a9Courage isn\u2019t the absence of fear. It\u2019s being scared and doing the right thing anyway. \u21a9"},{"title":"Vim: GoTo Tag\u00a0Definition","url":"vim-notes-goto-tag-definition.html","body":"Update (2021-03-3 Just use neovim.coc instead of YouComplet or Syntastic. It\u2019s faster, easier to setup, and works intuitivel ALE is still wonderful and useful, though there\u2019s a lot of overlap - coc can lint as well. Jump Lists and Change Lists If you\u2019re going to be jumping around to where things are defined, you will need to know how to jump back again. It seems there are two lists you need to be aware of, the jump list1 and the change list2. Jump List A list of locations that the cursor has jumped to. Relevant to jumping to a definition <C-O> move up the jump list <C-I> mode down the jump list Change List A list of locations where a change was made. A change is something that can be undone using u. '. will move you to the . mark. . is a special mark that is automatica set at the location of your last edit. '' will bring you back to where you were before your last jump g; and g, will also move you up and down the change list Original Post: There are multiple ways of doing anything with vim, including going to where a function or object is defined, and I usually need to do something at least 3 times before I can do it without breaking my focus or train of thought. My memory is hazy but I remember spending a 1/2 day looking into this and considerin which solution I wanted to commit to.3 My options seemed to be between YouComplet and ALE. [Update!4] I can\u2019t remember everything I read and tried, but I trust my conclusion Looking in my .vimrc I see that I have <leader>x mapped to :YcmComple GoTo and it works just fine, even when a module is imported from somewhere outside the current project. The tool was working and ready to use, I just hadn\u2019t internaliz it yet. Commands to remember: <Leader>x - GoTo definition - YCMs best guess at an \u2018intellige goto command, whether its a declaratio or a definition <F2> - Toggle tagbar :help jumplist \u21a9:help changelist \u21a9The more powerful the tool, the more worthwhile it is to take a closer look at what it can and can\u2019t do. \u21a9YCM and ALE work fine for goto definition and linting, but they don\u2019t give me satisfacto looks like it might offer some improvemen \u21a9"},{"title":"What\u2019s So Different About\u00a0Now","url":"whats-so-different-about-now.html","body":"I think we are less aware of our ignorance than previous generation It is easy to implicitly assume that all useful informatio is available to us, and that we are therefore more informed than we really\u00a0are I think this is because the internet has made informatio more accessible and global air travel have made the world feel\u00a0small Whilst an individual would hopefully never pretend to know everything I think its easy to assume that the right informatio exists and is being used by the people to whom it is\u00a0relevan But the accessibil of all informatio has put us in a situation similar to informatio scarcity. We still need to actively search for the informatio we want, because the informatio that comes to us easily or for free is not equal to what we find when we apply\u00a0effo I can easily have so many short pieces of news or informatio that I am always slightly overwhelme The pace of modern communicat encourages me to never slow down enough to form my own questions or frame my own arguments. I can always find an answer to my questions, but when was the last time I checked that whoever gave it to me wasn\u2019t going to profit from\u00a0it?"},{"title":"Predicting the Future using Human Nature and\u00a0Technology","url":"what-happens-next.html","body":"Predicting the future sounds like a tough problem, but we try to do it all the time without realising\u00a0 We predict the future when we think about how risky or scary something is, or when we think about what\u2019s really going to change because of an announceme or press release. We try to predict the future when we\u2019re at the supermarke checkouts and we try to pick the queue that will move the fastest. I always seem to pick the wrong\u00a0one. There must be a million ways of trying to predicting the future but all the good ones are models which reduce complexity and emphasise key One of them could be comparing the influence of human nature and technology on the outcome, and then comparing the event to what\u2019s Human nature doesn\u2019t change, so if something is driven by fear or greed then it probably doesn\u2019t matter what century it occurs in. Technology is change, and if something is enabled or prevented due to technologi progress then the date is\u00a0importa What is driving the scenario? Is it human nature or technology Supermarke checkouts are mostly manual and require a couple of adults to work together, so human nature has a much bigger role on efficiency than tech. Young men will stack and pack quickly, old women will be the opposite. What types of shopping bags they have, or how they pay, or even how many items they\u2019re buying, are probably not going to lead you to the The same probably works for getting through"},{"title":"Financial Doom And\u00a0Gloom","url":"financial-doom-and-gloom.html","body":"Financial crises seem to happen fairly regularly so they shouldn\u2019t be unexpected But no-one seems particular concerned about our current financial system, at the moment our attention is controlled by other threats. I\u2019m concerned that a lot of money has been injected into the money supply but we haven\u2019t seen any inflation. And I am concerned that the price of stocks is no longer related to the value created by the company but instead by macro economics. It\u2019s a terrible time to be a value investor. This should be an alarming statement. Value investing should always be a decent way to make money unless markets are broken. If the price of something doesn\u2019t represent its value then a correction is inevitable Interest rates are really low at the moment so if you have spare money and you want to make it work for you then where do you put it? Not into a bank account, because interest rates are low1, and not into government debt, because the yield is so low. It has to be stocks if you want the value of your investment to increase meaningful But everyone is doing this which drives the price up, and because their price is increasing they increase even further. I think that the main reason for concern is super low interest rates and massive increases in the money supply, but there are a couple of other factors that are also contributi It\u2019s easier than ever for retail investors to participat in the stock market, and this seems like a good idea. However if retail investors have influence to effect prices, and they themselves can be manipulate or influenced regarding what or when they buy or sell, then that is likely a new kind of threat to financial stability. We\u2019ve never seen social media combined with quick, cheap investment services for amateurs before. Index funds are also more popular than ever2 - the efficacy of index investing relative to traditiona funds that use stock pickers is very high over medium or long time horizons because index funds are much cheaper. But if index funds become too large then they end up influencin the market in predictabl and rigid ways. Index funds cannot choose what they buy or how much they buy - they just track the index. If a company\u2019s stock crosses certain thresholds their stock has to be bought or sold. It seems like its possible to create feedback loops where funds have to buy more of a rising stock, which increases its scarcity and price, which then requires index funds to purchase more of the same stock. The amount of euros in existence in 2019 was 90% more than in 2010.3 But inflation between 2010 and 2020 is 13%.4 Why is that? If the price of something doesn\u2019t represent its value, then a correction is inevitable are interest rates low? Because confidence in the economy is low, so central bankers have to lower interest rates to make it 1. Cheaper for a business to borrow money to invest in their business and therefore easier for a business investment to be profitable and 2. So that its more attractive for investors to use their capital to invest in a business (which grows the economy) relative to depositing spare cash in a bank account (which is safer but a less efficient way to deploy capital). Interest rates affect the relative risk-rewar ratios of different investment strategies \u21a9Index Funds Are the New Kings of Wall Street \u21a9statista \u21a9"},{"title":"Debugging the more_categories plugin for\u00a0Pelican","url":"debugging-more-categories-pelican-plugin.html","body":"I\u2019ve realised that one of the plugins I use to make this blog is not working correctly. I use the plugin to: add subcategor assign multiple categories to articles. Subcategor aren\u2019t working and Pelican thinks each article just has categories than contain forward slashes. In his \u201cPowerful Python\u201d emails, Aaron Maxwell recommends looking at the source code for popular python libraries to see how really good Python is written, and how talented developers write code and solve problems. This is a good opportunit to look at the code that powers the plugin and see if if I can: Understand the source code Locate the source of the problem Fix the problem I don\u2019t know if Pelican is amazingly good quality or not, I get the feeling it could do with more developer resources, but I\u2019ve got a real reason and motivation to look at the underlying code so I\u2019m going to give it a shot. The documentat is sparse which doesn\u2019t help, I get the impression that whoever wrote it feels like Pelican is simple and it\u2019s obvious what\u2019s going on 1. It\u2019s not obvious to me. Pelican Plugins Every plugin has to have a register() function, here it is for the plugin: def register() I understand the idea of signals from Django, and generators are discussed a bit in the documentat So what else is happening\u2026 As I write down my understand of the plugin, I\u2019m aware that my understand is definitely incomplete and probably wrong. I hope that as I progress I will see the mistakes in what I\u2019ve already written. is called first, and it takes two arguments, generator and metadata. The entire function is 3 lines so here it is: def metadata): categories = = for name in categories = It looks like it gets the category from the metadata for each article. Presumably by the time this function is called the articles have already been parsed and a metadata object has already been created and populated with metadata about the articles, including categories The first row of splits up the categories if multiple categories are listed. metadata must be a dictionary and there must be a metadata dict for each article, otherwise you couldn\u2019t just get get the value assoiciate with the dictionary key and then split the string on commas. This means that this function is called once for each article. I don\u2019t know what text_type does yet. Maybe it ensures that the output is always a string. It\u2019s imported from six which I remember seeing being a dependecy of some other packages. .. Having checked the documentat for six it looks like I was right - it represents unicode textual data in both python2 and python3. Pelican was originally written in Python2 I guess. Next step is to write a new key-value pair to the metadata dictionary for each article. This plugin adds functional to python by enabling categories and not just a category for each article. It seems clear that adding a categories key to the metadata dict is an obvious way to do this. The value for the categories key is a list where each item is an instance of the Category class. This class is instantiat using two arguments, name which is the string from the previous row, and which is currently not understood .. printing the contents of shows that its a dictionary of all the settings. Easily assumed and good to confirm. I\u2019ll dig into the Category class in a moment, but first lets quickly cover the last row of the function. The category attribute of the articles metadata is simply updated with the first item in the categories list (categorie must be a list because it can be indexed.) class Category() This class is the only class defined by the plugin (which is only 96 lines of code). It has 6 methods, 5 of them are decorated, and it has no constants. The decorators are property [3], _name.sett [1] and [1]. URLWrapper is imported from and I don\u2019t know what that does beyond \u201cwrapping URLs\u201d. @property Decorators are functions that takes methods or functions as inputs. Using property along with setter decorators lets a class have a property assigned to it whilst ensuring that arbitrary conditions or logic is upheld. If the @property decorator is over a method called foo, then there would need to be a decorator called foo.setter on a method somewhere in the class. That doesn\u2019t seem entirely right though, because in our Category class, we have a @property decorator over a _name method, and also a @_name.set decorator over another method called _name. But the other methods with @property decorators (slug and ancestors) do not have any associated setter decorators or methods. The setter for _name seems to create parent categories if the string contains slashes: @_name.set def _name(self val): if '/' in val: parentname val = 1) self.paren = self.setti else: self.paren = None self.short = val.strip( Here, self.paren becomes an instance of the category class, that is instantiat using parentname and self.setti This is recursive to however many levels of subcategor are specified. The ancestors and as_dict methods seem more confusing. ancestors isn\u2019t called or mentioned within the class definition but is called from the function which is called after the get_catego function returns. I don\u2019t understand why it needs an @property decorator though. The class inherits from URLWrapper so that is probably the next best place to look\u2026 Indeed, looking at the definition of URLWrapper shows that the as_dict method is overriding the definition in the base class.I guess it\u2019s the \u201ccurse of knowledge\u201d \u21a9"},{"title":"Different Views For Different\u00a0Users","url":"different-views-for-different-users.html","body":"This blog serves a variety of purposes. It\u2019s partly a journal of how I\u2019m teaching myself to be a developer and a data scientist, and it\u2019s also a personal blog, with articles about my interests and experience It\u2019s unlikely that anyone is interested in every type of article, and I\u2019d like to make it easy for people to only read the content they\u2019re interested in. Therefore I thought I would separate the articles into two broad groups, technical and non-techni If you visit this blog for the first time by clicking a link to a technical article, the site will then only show you the technical articles on the blog. It\u2019s the same for non-techni articles. If you want, you can change these settings by clicking the paw icon1 in the navbar on the blog index page. I did this mainly because I could. I like playing around with the blog. The JAM stack feels accessible and its fun working with tailwind and with jQuery. I think that playing (being curious, lightheart and unhurried and not being concerned with failure) is really important. Especially for adults who don\u2019t usually do it much. Most of my successes or big opportunit have been the result of a process that started with playing around. Here is the list of requiremen I used when adding the feature: Requiremen If user lands on a page and DOESNT have local setting - create local setting based on type of article being read If user lands lands on a page and DOES have local setting which is contradict - reset local setting to \u201call\u201d If user lands on index and DOES have local settings, only show articles that match the setting Steps: Index page: check if local storage option exists, print to console the result Index page: make button group Index page: make correct button active on page load by using localstora Index page: update active button on page click Index page: articles when button clicked Index page: if local storage does exist, respect it Index page: add 3 stage switch to hamburger menu Index page: make hamburger menu behave intuitivel on small screens Index page: if local storage does not exist, pop up a modal asking for a choice Article page: check if local storage option exists, print to console the result Article page: if local storage doesn\u2019t exist, create it according to article type Article page: if local storage does exist and is contradict update article type to all It\u2019s a paw because cat\u2019s have paws and cat is like category. I might change this to something more intuitive in future, like making the icon an N if the user is only seeing non-techni posts, T for technical, and A for all posts. \u21a9"},{"title":"3 Different Types Of Programming\u00a0Problems","url":"different-types-of-problem.html","body":"Three categories of problem Last year when I was creating moneybar and pippip there were a few problems that took much more effort to solve than all the others. I think I could group problems into 3 buckets, based on how much time they take to solve. Type 1 takes less than 15 minutes to solve, type 2 takes between 15 and 45 minutes to solve, and type 3 takes more than 45 minutes (usualy much more). Type 3: When I start learning a hard thing (like web developmen almost everything is in the third bucket and it\u2019s exhausting You need to set aside big chunks of time, you need to be focussed and undistract calm and wide awake, and you need to be prepared for a long arduous journey. Probably your criteria for success should be \u201cam I dead?\u201d because then if you\u2019re asking the question you\u2019re guaranteed to be successful and keeping morale high is necessary for success. Type 2: Hopefully you can make good progress understand the basics and internaliz the relevant abstractio and your problems quickly1 become type 2 problems. They each take from 15 to 45 minutes to solve. Maybe this is because you know enough to break some big general problem into smaller problems (you are developing domain expertise) and your intuitions for how to solve the problem are becoming better so your first or second attempts are likely to be correct, rather than your fifth or sixth. Knowing how to google a problem so that you get the answer you need is also a really important skill, which requires intuiting how an English speaking expert would ask the question. This isn\u2019t trivial but I don\u2019t hear people discussing this often. When most of my coding problems are type 2, it feels like I\u2019m learning most efficientl and when I\u2019m most productive Type 1: After a while, the problems that need to be solved become type 1 problems. They take less than 15 minutes to solve, because: All the big problems have been solved and now you\u2019ve only got smaller problems left, and Your intuitions are good and your expertise has increased and you know where to look for answers.3 Exceptiona problems: But there seems to be a consistent exception to this model.4 Let\u2019s be silly and call them type W problems. These are the problems that eat up far too many hours, and are tiring to solve, even when you are (in most other respects) an expert. For me, these tend to relate to blob storage solutions for web apps deployed into production I can think of several factors why this is so, and I\u2019ll describe the specifics before generalisi When a web app runs in production the data is not stored on the web server because the things that make a web-server cheap and efficient are not the things that make a database or a file storage bucket cheap and efficient. Therefore they are stored somewhere else and you need some plumbing to join everything together. There are some abstractio involved to make this work easily and securely. However when developing locally, you are doing everything on your laptop. You have a web-server relational database and file system all in the same place. This is a big, fundamenta architectu difference between your developmen environmen and your production environmen As a general rule, these are supposed to be as similar as possible. These difference make it much easier to make something that works locally but doesn\u2019t work in production and it\u2019s very hard to test if a thing will work in production without deploying it to your staging environmen which you are likely less familiar with than your local developmen setup. Deploying to staging and debugging on staging is slower and harder than doing the same thing locally. Logging (and filtering) will likely be more important. Solving exceptiona problems So how do you solve these problems quickly and efficientl What is it about this problem that makes it so hard? Let\u2019s examine what makes the problem difficult to solve: Iteration cycles are slow - I can\u2019t test locally, I have to deploy to staging and this takes time. The problem occurs in a \u2018high friction\u2019 environmen - its difficult to dig around and figure out what\u2019s really going on when its hidden below 3 different layers of abstractio on a remote machine that I have limited access to via a web browser. I want to be able to dig and investigat quickly and easily using the same tools I use for writing and testing code locally. I\u2019ve taken great efforts to set up my local developmen environmen so that I can do this, and its stressful to switch to a different and more limited set of tools. The problem is the result of several things interactin at once, and I can\u2019t just test things one at a time. These things are probably very similar to the abstractio Thinking clearly, learning, buidling, solving problems, all rely on being able to separate or untangle a seemingly complex situation into its component parts so that you can figure out what causes what. If you can\u2019t isolate individual concerns or components you have a black box that is keeping you ignorant. In web developmen customized logging is usually a good way to being isolating and exploring particular components Having said all that, I think the best way to solve a problem is to prevent it from occurring in the first place, but I\u2019m not good enough to figure out how to do that, yet.on which timescale? Life is long, does it really matter if it takes 1 week or 1 month to learn something meaningful Momentum, and having fun, is important though. \u21a9from a personal growth point of view. I suppose from an employers point of view they want all problems solved fast, type 1 problems. \u21a9Open the right file, google the right query (and follow the link to stack overflow), make some changes, run your static type checker and linter, run your tests, and push. Done and on to the next item. \u21a9which is totally fine. It\u2019s just a mental model, and the map is not the territory \u21a9"},{"title":"Why I Want To Write\u00a0Regularly","url":"why-i-want-to-write-regularly.html","body":"I\u2019ve started writing more frequently I want to do this because I often have thoughts which I\u2019d like to explore and develop further but rarely do. Writing forces me to organise my thoughts and look at how substantia they really are, or aren\u2019t. There is truth in the saying that \u201cto know a thing you need to be able to teach it\u201d 1 , and writing well has several similariti to teaching. Can I really copy a collection of thoughts from my head to yours? Powerful ideas are resilient and have many consequenc The older I get the more I believe that ideas matter2. They have so many subtle consequenc They are the first dominos. I don\u2019t expect writing regularly to become a permanent habit - it doesn\u2019t need to be. But I do want to focus on it for awhile so that I become significan better. It\u2019s a skill that has too many benefits to be ignored. The blogs I remember most are focussed and unapologet about their priorities Most of them have a lot of text and do not focus on design. They make it easy to read content and don\u2019t spend time or attention on header images or styling. Before I redesigned this blog I had default settings that asked me to supply an image for each post, and for a summary, and a suggested tweet. None of it was necessary and whilst they all tried to make the blog better they ended up making it harder to write. These peripheral features added complexity and distracted from the main thing. They\u2019re are still there if I want to use them but they are not set up to be used by default anymore. They\u2019ve been moved to the background and if I forget they exist then that\u2019s OK - it just shows they weren\u2019t as important in practice as I thought they would be. I was probably just having fun adding new features and working out how to build them. You Ain\u2019t Gonna Need It, mate.Wikip article, and some external validation \u21a9A compliment notion is that asking the right question is more important than finding the right answer. I guess asking the right question is always necessary, but finding the right answer is only sometimes sufficient Sometimes you can get the answer a bit wrong if you asked the right question, and still get enough benefits to avoid the problem. \u21a9"},{"title":"Python: Becoming A Better Python\u00a0Developer","url":"becoming-a-better-python-developer.html","body":"I\u2019ve been subscribed to Aaron Maxwell\u2019s \u201cPowerful Python\u201d newsletter for over a year and I really like it. His emails are opinionate and candid, and singularly focussed. He seems passionate about what he does and I like\u00a0that. Ultimately the emails are designed to drive sign-ups for his courses which I suspect would be very good, but there is a lot of value in the free emails. Thanks Aaron. I realised that the emails are sequential and each subscriber gets the same sequence of messages regardless of when they signed up. There is the \u2018first\u2019 message, and then the \u2018second\u2019, and they kind of progress and\u00a0flow. This means that there are more benefits to paying attention than for usual email subscripti Even though the emails arrive when I\u2019m at a supermarke or making dinner for my kids, it\u2019s good to try and read it After being subscribed for several months, I unsubscrib and resubscrib Now that I know how reliable and high quality this advice is I\u2019m going to prioritise working through the examples and doing some of what I missed the first time. I\u2019ve gone back to the beginning to reinforce the parts I know and to try again with what eluded me the first\u00a0time Three kinds of practice projects to become a A web app - use Django if you don\u2019t know which framework to user. Done\u00a0this. A command line tool - use the argparse module, because it\u2019s in the standard library. Haven\u2019t done this yet, I guess now is a good time to start. It seems like the simplest and quickest of the three kinds of project, and I can see how useful it could be - it lets you use the app in many different contexts, outside the python eco-system and anywhere command line tools can be A machine learning model - I\u2019ve already studied this, from theory (numpy) to frameworks (tensorflo I\u2019m happy to see it\u2019s\u00a0inclu"},{"title":"Using Vim with large\u00a0codebases","url":"vim-for-large-projects.html","body":"I use Vim as my text editor and IDE. I like that its free, open source and customizab Below are some of the most useful plugins and features I\u2019ve started using this year when I was building Moneybar and learning how to use\u00a0Django There\u2019s a copy of my .vimrc at the\u00a0end. I\u2019m happy to invest time and effort learning how to make the most of Vim and its plugins. I\u2019m confident that I\u2019ll still be using it twenty years from\u00a0now. Filetype plugins - if you want some settings to be active only for particular filetypes, like .py (python) or .txt (text) then create a file in Vim will look in this file when it opens a buffer of the correspond file type. Good for formatting options like line length, tab spaces, vim commands that are You can\u2019t activate plugins in these files though. All the plugins have to be activated in your .vimrc in the usual\u00a0way. - this plugin lets you runs tests without leaving vim. You can run the test that\u2019s nearest the cursor, or all the tests in the current buffer. It\u2019s very customizab I wish it could be a bit faster, but I could probably improve that myself by changing some\u00a0setti - The incredible Asyncronou Linting Engine (A.L.E) applies fixers and linters to various filetypes, when you want and how you want. Super useful for writing tidy code and catching mistakes before the code is\u00a0run. junegunn/f and - It took a little getting used to at first, but now I can\u2019t imagine not using a tool like this (this could be said about so many vim-relate things). Use fzf to switch between open buffers, open a new file, search for files using the filename, or search within all the files in the project for specific\u00a0t - This plugin opens a sidebar which contains a list of of functions and classes and methods (tags). You can use it to see which methods a class contains, and jump to the part of the buffer where a tag is\u00a0defined This is my .vimrc during January\u00a020 \" ========== Global ========== set nocompatib \" always put it at the top of .vimrc. effects mappings, undo, etc. set encoding=u \" utf-8 encoding set termguicol set t_Co=256 \" number of colors set noerrorbel vb t_vb= \" no error bells, yes screnflash set linespace= set scrolloff= \" minimum number of screen lines above and below the cursor set shortmess- \" show how many times a search result occurs in current buffer, and index of current match set hidden set number relativenu \" Line numbers set splitbelow set splitright \" set tabstop=8 softtabsto expandtab shiftwidth smarttab set undofile \" Maintain undo history between sessions set \" put all the undo files in this dir filetype on \" enables filetype detection filetype plugin indent on \" detection on, plugin on, indent on. To see the current status, type: :filetype syntax on \" syntax highlighti - try 'syntax on/enable' set noesckeys \" might break stuff, should make <ESC> delay smaller set timeoutlen \" timeoutlen is used for mapping delays set ttimeoutle \" ttimeoutle is used for key code delays set incsearch ignorecase smartcase hlsearch highlight Search guibg=purp guifg='NON highlight Search cterm=none ctermbg=gr ctermfg=bl highlight CursorColu guibg=blue guifg=red highlight CursorColu ctermbg=re ctermfg=bl nnoremap // nnoremap # #`` nnoremap * *`` \" close buffers properly go to previous buffer, then delete the buffer you were just in. nnoremap <Leader>bd :bp\\|bd #<CR> inoremap <Leader>bd :bp\\|bd #<CR> \" Spell check set spelllang= nnoremap <leader>ss :setlocal spell!<CR> nnoremap <leader>sf z=1<CR><CR \" ========== Set leader and local leader let let \" insert 1 space to the right, without leaving normal mode nnoremap \" Flash the cursor row (and column) colors are set after color scheme ======== \" nnoremap <leader>f :call Flash()<CR \" function! Flash() \" \" set cursorline cursorcolu \" set cursorline \" redraw \" sleep 110m \" set nocursorli \" endfunctio \" Edit/Reloa the .vimrc file nnoremap <silent> <leader>ve :e $MYVIMRC<C nnoremap <silent> <leader>vr :so $MYVIMRC<C augroup VimReload autocmd! autocmd BufWritePo $MYVIMRC source $MYVIMRC augroup END \" Yank to clipboard vnoremap <C-c> \"+y if set \" copy to the system clipboard if \" X11 support set endif endif \" Go into NORMAL mode inoremap jk <ESC> \" view working directory nnoremap <leader>pw :cd %:p:h<CR> \" toggle line wrap nnoremap <leader>lw :set nowrap!<CR \" toggle line numbers nnoremap <leader>ln :set \" Insert current datetime nnoremap <leader>dt A ()<ESC>hh \" map w to ` nnoremap ` w \" Swap : and ; nnoremap ; : nnoremap : ; vnoremap ; : vnoremap : ; \" Navigation & movemement \" save buffer if it has been changed nnoremap ww :update<CR \" close Vim, but not if there are unsaved changes nnoremap qa :qa<CR> \" save all changes nnoremap wa :wa<CR> \" close buffer nnoremap qq :bp\\|bd #<CR> nnoremap wq #<CR> \" switch buffers nnoremap <silent> + :bn<CR> nnoremap <silent> _ :bp<CR> \" Split navigation nmap <Leader>h <C-W><C-H> nmap <Leader>j <C-W><C-J> nmap <Leader>k <C-W><C-K> nmap <Leader>l <C-W><C-L> nmap <Leader>ww <C-W><C-W> nmap <Leader>wq <C-W><C-Q> \" split (pane) resize nnoremap <C-k> :resize +2<CR> nnoremap <C-j> :resize -2<CR> nnoremap <C-h> :vertical resize +2<CR> nnoremap <C-l> :vertical resize -2<CR> \" open help in vertical split by default cabbrev vhelp vert help \" Natural cursor movement over wrapped lines nnoremap j gj nnoremap k gk \" Insert blank lines in normal mode nnoremap <leader>o o<ESC>k nnoremap <leader>O O<ESC>j \"========= PLUGINS ========== call \" numbers as text objects Plug \"run shell commands async in vim8\" Plug let = 10 \" When using :python or :!python, access the packages in venv \" \" <tab> Plug \" force quickfix to be full widtth au FileType qf wincmd J \" testing - many languages and test runners Plug let test#strat = let = 'pytest' let = '-x' let = \"belowrigh nnoremap <silent> t<LEADER>n nnoremap <silent> t<LEADER>f :TestFile< nnoremap <silent> t<LEADER>s :TestSuite nnoremap <silent> t<LEADER>l :TestLast< nnoremap <silent> t<LEADER>g :TestVisit \" toggle the quickfix window function! if copen 15 setlocal else cclose endif endfunctio nnoremap <silent> cc :call \" generates an index (or tag) file of language objects found in source files \" <C-]> jump to definition \" <C-O> jump back \" g] see a list of multiple matches \" <C-t> Plug \" (re)genera tags file in the bg Plug let = ['.json', \" sidebar that displays the tags of the current file, ordered by their scope Plug nnoremap <F2> nnoremap <F2> \" add python library code to tags file, goto def with <C-]> let pyEnvLib = $VIRTUAL_E let pyEnvLib .= \" Async linting engine Plug let = 0 let = 0 \" ALE completion let = 0 set let = 1 nnoremap <leader>at :ALEToggle nnoremap <leader>af :ALEFix<CR nnoremap <silent> <leader>aj :ALENext<c nnoremap <silent> <leader>ak \" iSort Plug \" track the snippets engine Plug \" Snippets are separated from the engine. Add this if you want them: Plug \" Trigger configurat Do not use <tab> if you use let let let \" If you want :UltiSnips to split your window. \" let Plug Plug Plug nnoremap <leader>x :YcmComple GoTo<CR> \" the subcommand add entries to Vim's 'jumplist' so you can use \" 'CTRL-O' to jump back to where you were before invoking the command (and \" 'CTRL-I' to jump forward; see ':h jumplist' for details) let = 0 let let = 1 let = 1 let = 1 let = 1 let = 1 let = 1 \" autoclose parens, brackets etc \" Plug \" vim-tmux focus events Plug \" Code folding \" Plug \" match m of n \" Plug \" adds vertical lines to easily show indent levels Plug \" Fugitive Plug \" Marks Plug \" Latex Vimtex Plug let g:tex_flav = 'latex' autocmd Filetype tex set updatetime let = 'open -a Preview' let = \\'specifie changed to'.\"\\n\". \\'You have \\'Missing number, treated as zero.'.\"\\n \\'There were undefined \\'Citation %.%# \\'Double space found.'.\"\\ let = 8 \" Rainbow parenthesi let blacklist = ['html', 'md', 'wiki'] autocmd BufWritePr * if &ft) < 0 | Plug let = 1 let g:rainbow_ = { \\'guifgs': ['green', 'magenta1' 'gold', 'red', \\'guis': \\} \" Set color scheme. set Plug \" colorschem colorschem badwolf let = 1 let = 1 let = 1 \" colorschem modificati highlight Comment ctermfg=cy guifg=cyan highlight pythonComm ctermfg=cy guifg=cyan highlight LineNr ctermfg=cy guifg=cyan hi nontext term=bold ctermfg=Cy guifg=#80a gui=bold hi vimLineCom term=bold ctermfg=Cy guifg=#80a gui=bold \" SpecialKey - use :set list to toggle visibility of EOL, CR, etc hi specialKey term=bold ctermfg=Cy guifg=#80a gui=bold \" colors for flashing cursorline and cursorcolu hi CursorLine cterm=NONE ctermbg=gr ctermfg=bl guibg=gree guifg=blac hi CursorColu cterm=NONE ctermbg=gr ctermfg=bl guibg=gree guifg=blac \" query what kind of syntax is this color? - wc nnoremap wc :echo \"hi<\" . . '> trans<' . .\"> lo<\" . . \">\"<CR> \" fuzzy file, buffer, tag finder set \" ensure you have the latest version Plug { 'do': { -> fzf#instal } } Plug nnoremap <silent> <Leader>e :Files<CR> nnoremap <silent> <Leader>r :Buffers<C nnoremap <silent> <Leader>t :Tags<CR> nnoremap <silent> <Leader>ff :Rg<CR> \" nnoremap <silent> <Leader>ff :Ag<CR> nnoremap <silent> <Leader>la :BLines<CR nnoremap <silent> <Leader>ll :Lines<CR> nnoremap <silent> <Leader>' :Marks<CR> nnoremap <silent> <Leader>fh :Helptags< nnoremap <silent> <Leader>fs :Snippets< nnoremap <silent> <Leader>fc :Commits<C nnoremap <silent> <Leader>fb :BCommits< nnoremap <silent> <Leader>hh :History<C nnoremap <silent> <Leader>h: :History:< nnoremap <silent> <Leader>h/ :History/< \" let = --info=inl \" let --files --hidden\" let = 0 let g:fzf_layo = { 'down': '~50%' } \" let = '' let = 'right:0%' function! let joined_lin = join(a:lin \"\\n\") if len(a:line > 1 let joined_lin .= \"\\n\" endif let @+ = joined_lin endfunctio let g:fzf_acti = { \\ 'ctrl-t': 'tab split', \\ 'ctrl-x': 'split', \\ 'ctrl-v': 'vsplit', \\ 'ctrl-o': \\ } let g:fzf_colo = \\ { 'fg': ['fg', 'Normal'], \\ 'bg': ['bg', 'Normal'], \\ 'hl': ['fg', 'Comment'] \\ 'fg+': ['fg', 'CursorLin 'Normal'], \\ 'bg+': ['bg', 'CursorLin \\ 'hl+': ['fg', 'Statement \\ 'info': ['fg', 'PreProc'] \\ 'prompt': ['fg', \\ 'pointer': ['fg', 'Exception \\ 'marker': ['fg', 'Keyword'] \\ 'spinner': ['fg', 'Label'], \\ 'header': ['fg', 'Comment'] } \" grep in vim - shows results in a split window Plug \" session tracking Plug \" pairs of handy bracket mapping Plug \" Plug \" repeat commands from plugin mappings Plug \" vinegar Plug let = 3 \" CSV Plug \" nerdtree Plug nnoremap <Leader>n let let = 1 \" Automatica delete the buffer of the file you just deleted let \" 2 - open nerdtree only if directory was given as startup argument let \" always focus file window after startup let \" Status bars Plug Plug let = 1 let = 0 let = 0 let let \" remove encoding status let = 1 let let = 1 let = 1 let = 1 let = 1 let = 1 let = 0 let = 0 let = 0 let = 0 let = \" comments Plug let = 1 let = 1 let = 'left' let = 0 let = 1 \" markdown. tabular is required Plug Plug let = ['python=p let = 0 let = 0 let = 1 let = 0 let g:tex_conc = \"\" let = 1 let = 4 let = 1 \" writing prose Plug Plug augroup pencil autocmd! autocmd FileType wiki,md,tx call pencil#ini autocmd FileType wiki,md,tx :PencilSof augroup END let = 'soft' autocmd! User GoyoEnter autocmd! User GoyoLeave \" Ensure :q to quit even when Goyo is active function! s:goyo_ent let b:quitting = 0 let = 0 autocmd QuitPre <buffer> let b:quitting = 1 cabbrev <buffer> q! let = 1 <bar> q! setlocal wrap endfunctio \" Quit Vim if this is the only remaining buffer function! s:goyo_lea if b:quitting && bufnr('$') == 1 if qa! else qa endif endif endfunctio autocmd! User GoyoEnter call autocmd! User GoyoLeave call nnoremap <Leader>g :Goyo<CR> \" python linting \" F7 checks flake8 Plug Plug \"Flagging Unnecessar Whitespace highlight BadWhitesp ctermbg=re guibg=dark Plug let = ['latex', 'html'] let = 1 let = [] \" javaScript Plug let = 1 let = 1 \" format .JSON files by using the jq cli tool com! JQ %!jq \" HTML/JINJA Plug Plug \" Plug let = \"*.html, *.xhtml, *.phtml\" call plug#end()"},{"title":"Using RSS","url":"using-rss.html","body":"Updated: 10 Feb 2021 I found a blog post which is surprising similar to my thoughts on RSS feeds, but better presented and thought through. The post mentions the idea that \u201cRSS is about capturing the long tail of blogs that don\u2019t post frequently 1. This idea crystalise why I was so glad I\u2019d started using RSS feeds again. If readers use RSS, then authors don\u2019t need to concern themselves with attention. This removes pressure on the author to post frequently and lets them focus on quality over quantity. News feeds and ad supported platforms have fundamenta different mechanics and incentives With RSS I can let good quality content come to me, on its own schedule. I don\u2019t need to remember to look for it, and the authors don\u2019t need to remind me that they exist. Google Reader RSS is a very effective way of having good quality informatio come to you. Back in 2008, I used to use Google Reader to subscribe to RSS feeds. I was an aspiring photograph back then and I remember being subscribed to around 80 blogs. Each day I\u2019d read articles from whoever had posted something new, without needing to visit their websites or remember who they are or that I\u2019d subscribed to their blog. The authors didn\u2019t need to optimize their output according to an opaque and changing algorithm either - they didn\u2019t need to optimize article length, tags, post frequency, image inclusion or linked content. They could write how they wanted to, which I suspect leads to higher quality content. Social Media A few years later Google Reader was closed down, presumably because using RSS didn\u2019t fit with Googles advertisin model. I was unaware of it at the time but I imagine it sent shockwaves through blogging communitie and probably upended many businesses I mostly stopped reading blogs. Facebook was growing fast, Instagram felt new and exciting, and content was moving onto \u2018platforms or into walled gardens. And as they kept on growing the average quality of the content decreased. Twitter is like this now I think. There are some real diamonds to be found from time to time, but there\u2019s a lot of mud too. Mostly its just mud, but the occasional diamond can have outsized benefits. RSS isn\u2019t like this. I choose the contents of my \u2018news feed\u2019, and each article can be much longer than a Tweet, or a caption to a photo, or a status update. It\u2019s hard to write well and to create an interestin or useful blog post, and that makes it harder to dilute quality with entertaini distractio I have complete control about what content I see, and I can change it whenever I want. The process is designed around me. Reeder5 I used netNewsWir for a few weeks, but it couldn\u2019t sync between my laptop and phone, so I bought Reeder 5. It\u2019s got a few unusual design patterns, but it works well and has all the features I want. I\u2019ve been unsubscrib from email newsletter and subscribin to the RSS feed instead. It keeps my inbox quieter, and it feels good to have a \u2018separatio of concerns\u2019. It makes it easier to read interestin content without being \u21a9"},{"title":"Notes on learning\u00a0Django","url":"learning-to-django.html","body":"Table of Contents In the\u00a0beginn A personal The best moments Adding a unique identifier to an existing Testing\u00a0co In the\u00a0beginn I came to web developmen via business analytics. I was working as an accountant and Excel wasn\u2019t good enough anymore, so I looked around for a way to get started and came across Jupyter Notebooks. Notebooks are said to be a kind of \u201cgateway drug\u201d to programmin and I think that\u2019s true. They\u2019re the easiest and fastest way to start programmin that I\u2019ve come\u00a0acros When you\u2019re working in a notebook, its easy to get data, wrangle it, and show some results. But as soon as you can create a chart or some summary table you inevitably wonder how you can show this to people more easily, and publishing the results to a website feels like the best most general and Unfortunat it\u2019s also the hardest, and so begins a long series of compromise and incrementa progress. Learn to use a dashboardi API, and learn to create static sites. But the end-goal, the ultimate solution, is a data driven web app, with saved user preference scalable performanc and automatica updated data\u00a0sourc A personal When I moved to the Netherland I wanted to use a personal finances dashboard to check weekly expenses. There wasn\u2019t a web-app that would do this (though there are a couple of apps that are trying) so I built my own dashboard. Then a few friends asked if they could use it too. They couldn\u2019t because it was just a dashboard and not a web app, but I thought this was a good reason to jump into It was a much bigger task than I anticipate (And that\u2019s OK.) It took several attempts and was super frustratin I would dabble for a few weeks, do a few tutorials, and then get completely lost when I tried to do something by myself. I\u2019d get disorienta working across many different files and trying to visualise which part of the model, or the cycle I was currently working\u00a0on I came to realisea that the mental load seems so large at the beginning because is really a whole stack of technologi and abstractio combined (or stacked) together. Many of these have to be used together at the same time before you can see any evidence of success at all. I think the hardest things about Django are not actually Django. You\u2019ll need to comfortabl with classes and inheritanc You\u2019ll also need to be comfortabl with working across multiple files, and have some tools for searching across all you open buffers, or all the files in the project, at the same time. You\u2019ll also need to be comfortabl with version control (Git) and using the command line. Get familiar with stack traces\u00a0too If you\u2019re familiar enough with all these things, so that using them doesn\u2019t feel new, but ideally feels familiar and comfortabl then I think you\u2019ll make quite quick progress with\u00a0Djang Django uses the model. Models are how django maps Python objects to items in your database (oh yeah, you need to be familiar with SQL too\u2026), Views are where requests are processed (also Middleware and turned into Responses, which are then combined with templates (unless your building an API). You might notice I haven\u2019t mentioned what a Controller is - get used to informatio feeling incomplete whilst you\u2019re learning the ropes. It\u2019ll become clear soon\u00a0enoug The best\u00a0momen The \u2018curse of knowledge\u2019 states that once you\u2019ve learnt something you can\u2019t imagine or remember what it\u2019s like to not know it. Before that happens completely I want to record some of the \u2018ahah!\u2019 moments of For context, I stopped working as a freelance data scientist in April and after a few weeks wondering if django and PostgreSQL and python was the way to go (yes it is. use boring technology I began working full-time on what would become MoneyBar.n I called it \u2018myeuros\u2019 in the\u00a0beginn The learning curve felt steep. I wanted to do things \u201cright\u201d the first time because I wasn\u2019t building a toy, and although I felt that hindsight would show this to be a mistake in terms of efficiency I did it anyway because I have a hunch that following my compulsion sometimes makes life harder in the short term and better in the long\u00a0term. The best moments are usually preceded by the Adding a unique identifier to an existing I used pydanny\u2019s template. Honestly, by the time I\u2019d gone through the quickstart process and googled the nouns in all the questions (what is Sentry, what is Celery and what is a task que, what is whitenoise etc.) I was already tired. Play with it a few times and come back to\u00a0it. Anyway, I wanted to start with because the project template has that part kind of up and rnuning for you out of the box. uses the Django Allauth package, which is awesome, and reliable, and fully featured\u2026 and extremely abstracted Good luck looking at the module code and understand it if youre not an\u00a0expert. I wanted to give each user a unique ID - a UUID when they signed up. This would be used in query strings instead of usernames or incrementa keys. This was so hard the first time! And it turns out its not a trivial task, not if you already have a few users in your (test) database. Sure you can reset the database and start again, but experiment like this is fairly complex. Understand how the python model classes (the ORM) maps to the relations in the PostgreSQL databse was complex, and if I got confused, should I try to fix it by changing python Models, or editing migrations or working on the database directly? Getting started is one of the After I\u2019d figured out I started creating models for other simpler data (transacti and bank accounts I expect). This was much simpler and faster. I remember driving home one evening thinking that if I could get this far then success Testing\u00a0co Before long, testing each part of the app by hand when I added or changed a feature was no longer trivial. I needed to find some way of automatica creating users and checking that they could log in and access\u00a0vie I began working with pytest, and really found it hard to wrap my head around the idea accessing different parts of the app not by requests and responses but by accessing class I think its normal and good to code at the limit of your knowledge, where you know just enough to make a thing \u201cwork\u201d. But this approach falters when you want to then test what you wrote. Or at least, the measure of \u201cjust enough\u201d really changes when you require tests to be written. You don\u2019t just need to make it work, you need to understand why it works, so that you can write tests to assert that certain conditions pass and others\u00a0fai This feels really satisfying when it works, because you have proof that you really have grasped a bigger picture. There are far fewer (relevant) black boxes when you write tests. But it also makes learning slower, at least in the short term. It means you might have two get comfortabl with a handful of abstractio when you\u2019ve already solved the problem you started with. This is frustratin and it takes discipline to slow down, take a deeper look at the solution, and not just race on to the next\u00a0featu"},{"title":"Data Science vs Web Development: Larger Code\u00a0Bases","url":"larger-code-bases.html","body":"Code\u00a0Struc One of the most immediate and basic difference between working as a data scientist or as a web developer is the number of files the codebase is spread across and the amount of code within each\u00a0file. Web applicatio tend to be very modular - there are a lot of different things going on in a modern web app and generally they all need to be able to be modified or updated independen of each other. This requiremen encourages modular code base architectu with the code broken down into When working on a data science project you often have a well defined and quite narrow pipe line. Each stage of a pipeline has well defined inputs and\u00a0output This seems to have the consequenc of making data science projects tend towards a handful of files each with a substantia amount of unique (not boilerplat code. In web developmen there seems to be more boilerplat many more files spread across a tree of directorie and the average number of lines of code per file is IDE\u00a0featur These difference mean that code organizati tools and IDE features play very different roles within each industry. In web developmen you really need to be able to jump between different files (or buffers) quickly, and search for text across multiple files. Writing idiomatica becomes more important, and writing code within discreet testable units becomes essential so that things don\u2019t break without being\u00a0noti In data science, linting feels more optional, and searching for text within methods or functions outside the current module is\u00a0rarer. I didn\u2019t appreciate this until I paused my work as a Data Scientist and began building non-trivia web\u00a0apps."},{"title":"TDD: Test Driven\u00a0Development","url":"test-driven-development.html","body":"Test Driven Developmen was mind-bendi when I first grappled with it. Last summer I was building a web app using Django and began to break things when adding new features. This soon led to lots of clicking around different pages to test if stuff was still working each time I made an update. This soon led to me thinking there must be a better way. Which brought me to Test Driven Developmen (TDD). It should have just led me to writing tests, which it did. But googling whatever I googled got me down the TDD rabbit hole rather than just the \u201cwrite some tests\u201d rabbit hole. Write tests for your code before you write the code. Write tests for bugs you\u2019ve fixed to check they stay fixed. Write tests as a kind of documentat to show what stuff is supposed to be doing. Errr\u2026 Django was a big enough pile of abstractio as it was. Views, ORMs, mixins, serializer Trying to add factories and fixtures into that took some getting used to. But eventually I made some progress, and now I quite enjoy running coverage reports to keep coverage close to 100%1. Some of the main things I\u2019ve learnt about writing tests: Use PyTest as much as possible rather than other testing libraries - its assert statements are more intuitive than Django\u2019s own testing framework, and you can use it in any Python codebase, not just Django. It has lots of extensions and seems good at getting the job done fairly easily. Write tests as you go. I haven\u2019t (yet) reached the elevated level of writing tests before I write the code to be tested, though I see why that would sometimes be useful. I do think writing tests sooner rather than later is best though, ideally as soon as you\u2019ve got a basic version of your feature working. Use Coverage to show you which code is covered by your tests, and which branches or edge cases are not. But be warned, it doesn\u2019t tell you if the test is useful or not, only that it passes and which methods or functions it uses. Fixtures are great for keeping tests fairly DRY. Freezegun is great for testing anything to do with dates and times. Static type checkers, like Mypy, get more attractive in proportion to codebase complexity and size. Which is fun and all, but testing for the sake of it doesn\u2019t necessaril stop bad things happening. Its very possible to write a test that covers the code you\u2019ve just written without ensuring that only the intended behaviours happen. \u21a9"},{"title":"Why Talk About\u00a0Jesus?","url":"faith-in-jesus.html","body":"Start with happiness It seems to me that I am much happier as a Christian than I would be if I weren\u2019t. By \u201cbeing a Christian\u201d I mean following Jesus - trying my best to act, think and speak like he want\u2019s me to because I\u2019m grateful that he did what I believe he did. I believe he died for my sins and is now alive, having been miraculous resurrecte by his father1 (who is now my father too). It\u2019s not just an intellectu exercise. I believe that Jesus is alive because I seem to have experience his companions and interventi in my day-to-day life. This is unusual, mind boggling, and makes things significan more complicate than if I thought he were dead. Nonetheles it seems to be true. It\u2019s not like I see supernatur interventi every day or anything, but there have been various times - too many to discard - when my prayers have been answered in practical ways (I\u2019ll leave out the intangible for now) that have surprised me and given me a lot of respect for the risen Jesus doing what its said he\u2019d do in the bible. Obvious answers There was this time I was in Yemen trying to get back to my room across town, at night and during a storm. I was lost and couldn\u2019t read Arabic to understand which bus to get. So I prayed, and two buses later I got off at the right stop. Another example, I was getting a haircut in Liverpool Street Station in London, and as I was sitting in the chair someone took my work bag. When I came to pay, I realised my laptop and wallet had gone. Trying to find a stolen bag in central London feels ridiculous Even so, I spent a few hours wondering around the alleys and parks looking for it, and I resigned myself to some awkward conversati and using most of my next salary to replace the stolen computer. To my surprise, my parents-in prayed about it and were really confident it would come back to me, which seemed super unlikely. A few days later I received a call from an office worker in the Gherkin building - my bag was under his desk and he wanted to know if I\u2019d like to collect it. I guess I\u2019ll try not to make the same mistake twice. The other example I tend to remember happened not long after I\u2019d first moved to Vienna. I was feeling lonely and isolated and I was wondering how on earth I was going to find some sort of normal that was healthy and sustainabl Try as I might, I wasn\u2019t enjoying things at all and was feeling stressed and overwhelme I remember walking up this steep hill in the 18th district towards my office, and repeatedly praying this really simple prayer \u2018God, please help, please help, please help..\u2019. It was that simple because I couldn\u2019t think of anything more useful to say. Nothing dramatic happened that day, or even that week as far as I can remember. But when I look back, it was a turning point when things started getting better instead of worse. I suppose you can call this last example an intangible answer to prayer. Maybe it is, but I think anyone whose grappled with overwhelmi loneliness or panic would say that the emotions become all too tangible at times. Being in a different emotional state seemed to make a tangible difference to just about every area of life. Eating, productivi at work, relationsh with friends and colleagues etc. This is only the beginning of why I think Jesus is alive and why I think Christiani is a real and living faith. It\u2019s not primarily a tradition, a worldview or a set of rules and ideals. Christiani is a relationsh with Jesus. He did lots of amazing things that have let me have a very practical relationsh with him. It\u2019s an almost unbelievab premise from which to live a life. It has so many implicatio And it holds up to scrutiny and my experience bear it out. Why write this? Despite Jesus\u2019 incredible works and their implicatio modern Christiani seems to be in a really confused and ineffectiv state. Ideas and thoughts about have become mixed up with cultural christiani or christian politics and traditions These are each different things, and unless we distinguis between them with the words we use, we are going to find it hard to think and communicat clearly. I suspect that we are in a negative cycle of imprecise thinking leading to imprecise articulati which leads to further imprecise thinking. Unless we can talk and think about one thing at a time, atomically if necessary, we take on the additional risks of reaching the wrong conclusion personally or arguing with others due to rather than actual disagreeme We should try to create a more precise vocabulary to navigate our Christian lives, so that we can think clearly about the experience and questions we have. Imprecise thinking is frustratin and conversati are less effective when there\u2019s an increased risk of disagreeme or what someone else means. This makes it harder to talk about our faith, which makes talking less common, and this creates room for or apathy, or missed opportunit or sadness.An many other things also. \u21a9from first principals \u21a9"},{"title":"API Design\u00a0Principles","url":"api-design.html","body":"Some super brief notes I made about API\u00a0design Background It\u2019s more of an art than a\u00a0science RESTful State Transfer) API design is an Alternativ API architectu SOAP (Simple Object Access Protocol) is a heavier\u00a0st GraphQL - doesnt overfetch. Graph query language made by\u00a0Faceboo APIs are everywhere (not just web APIs). They\u2019re an abstractio that hides an Django model managers are an API (and also part of Django\u2019s ORM), JavaScript is an API,\u00a0etc. RESTful\u00a0AP Web APIs (all REST APIs?) expose a databases to\u00a0clients A rest api is a URL route (endpoint) that returns JSON or XML. POST, GET, PUT, PATCH, DELETE, correspond to Create, Read, Update/Mod Delete (HTTP methods correspond to CRUD\u00a0metho HTTP METHODS: PUT (create or update) is idempotent POST is not idempotent (keep on PATCH - partial\u00a0up GET, HEAD, OPTIONS and TRACE methods are idempotent cos they are only designed for DELETE HEAD - almost identical to GET, but without any body. Good for checking what a request would return, i.e. Before downloadin a large amount of\u00a0data, OPTIONS - returns data describing what other methods and operations the server supports at the given URL. More loosely defined than other\u00a0verb Use HTTP verbs to make requests Use sensible resource names. Naming things is hard, so think about this a bit before starting. Use identifier in your URLs, not the query string. Good: /users/123 Poor: Use the hierarchic structure of the URL to imply the structure of the API. Design (names and structure of things) for the user/clien not for the database. Resource names should be nouns not\u00a0verbs Use plurals consistent not collection verbiage. Good: customers/ Use camel case or snake Short is better than long, but be\u00a0clear Spend time on design before writing\u00a0co Use HTTP response codes to Prefer JSON over XML. (Hotline does HTML..) XML requires schemas for validation and namespaces Don\u2019t support this complexity at the beginning (or ever) unless required. If it is required, make the XML as similar to JSON as\u00a0possibl Put links in the HTTP link header, or use a JSON representa of\u00a0this. Use the HTTP location header to contain a link on resource creation, or for GET with pagination use first, last, next,\u00a0prev Connectedn - return links in the response which link to useful resources. At minimum, a link to show how the data was received, or\u00a0posted. Idempotenc - clients making the same repeated requests create the same result on the server side. I.e. making repeated requests has the same result as making a similar request, server side. On the client side, a response code may change, of\u00a0course."},{"title":"Principles Of Object Orientated\u00a0Programming","url":"principles-of-oop.html","body":"I recently interviewe for a lead developer role at Lab Digital1 and thought it would be sensible to review some of the fundamenta aspects of Object Orientated Programmin (OOP). You might think that\u2019s a unusual way to prepare for an interview, and you\u2019d be right. Nothing close to these notes arose during the interview, but I find this stuff interestin If I\u2019m motivated enough to study it, then I think that\u2019s a good enough reason by itself, without a specific reason. These are some brief notes. Object Orientated Programmin has four key aspects: Encapsulat (Hiding informatio Abstractio (Hiding the Inheritanc Polymorphi 1. Encapsulat Each object keeps its state private, inside a class. Instance are kept private and accessor methods are made public. Other objects don\u2019t have direct access to this state. They can only call a list of public functions (methods). The object manages its own state via methods, no other class can touch it unless explicitly (not default) allowed. Private variables. Public methods. You can define classes within classes, and functions within functions. 2. Abstractio A natural extension of encapsulat A concept or idea that is not associated with any particular instance. Expresses the intent of the class, rather than a specific Programs are often extremely large and separate objects communicat with each other a lot. This makes maintainin large programs difficult, and abstractio tries to solve this. Applying abstractio means that each object should only expose a high-level mechanism for using it. This mechanism should hide internal implementa details. It should only reveal operations relevant for the other objects. This mechanism should be easy to use and should rarely change over time. Implementa changes \u2014 for example, a software update \u2014 rarely affect the abstractio you use. e.g. a coffee machine. It does a lot of stuff and makes quirky noises under the hood. But all you have to do is put in coffee and press a button. 3. Inheritanc In OOP, objects are often similar, sharing similar logic. But they are not 100% the same. Create a (child) class by deriving from another (parent) class. This way, we form a hierarchy. child class reuses all fields and methods of the parent class (common part) and can implement its own unique part using method or attribute overloadin 4. Polymorphi Gives a way to use a class exactly like its parent so there\u2019s no confusion with mixing types. But each child class keeps its own methods as they are. This typically happens by defining a (parent) interface to be reused. It outlines a bunch of common methods. Then, each child class implements its own version of these methods. Any time a collection (such as a list) or a method expects an instance of the parent (where common methods are outlined), the language takes care of evaluating the right implementa of the common method \u2014 regardless of which child is passed. I\u2019d like to be so familiar with the following features that I can use them without referring to notes: Getters and setters. Instance methods compared to class methods. Inheritanc mixins, and decorators The \u201cmagic\u201d within the Django source code that requires mypy to use extensions in order to do its static type checking correctly. Unfortunat I didn\u2019t get the job. They wanted a senior Python developer with experience with Infrastruc As Code, and also working at an agency. Can\u2019t win them all. \u21a9"},{"title":"Optimizing The Performance Of This\u00a0Blog","url":"site-performance.html","body":"I\u2019m coming to the end of redesignin this site. Now that the main changes have been made its fun (and good practice) to optimize the site so that it loads quickly and is optimized for SEO Lighthouse is a utility built into Chrome that runs a technical audit on a webpage and assesses a wide range of features. It also provides details about how to improve the\u00a0page. My site is hosted on Github Pages and is accessed via Cloudflare which gives me a lot of performanc gains including minified HTML and CSS, caching, and super fast server I\u2019m using Github Pages and Cloudflare for free and I think its amazing that I can get the benefits of these services without needed to pay anything. If someone knows where to look and can teach themselves using free resources, they could be read by anyone anywhere in the world. It\u2019s\u00a0amazi Below are the lighthouse results for the blog\u2019s index page and for a recent\u00a0pos"},{"title":"Unix: Utilities To Analyse And Update Multiple Text\u00a0Files","url":"using-unix-utilities-to-analyse-and-update-multiple-files.html","body":"As part of the redesign of this blog I wanted to make an article\u2019s category more meaningful Previously I simply picked a handful of categories and then assigned a single category to each post. This method becomes limiting when an article is relevant to Also, using nested categories seems like a good way of grouping similar content and allowing more nuanced filtering of\u00a0interes As I considered how to update the categories of existing articles, I realised this would be a good opportunit to practice analyzing and updating text files using Here is how I reviewed and updated the categories of my I use Pelican to generate the static files for this site. It converts markdown into HTML. Metadata for each article is set at the beginning of a file, the title is set by typing Title: ... and similarly the category is set by typing Category: ... on its own\u00a0line. To locate, analyse and update my existing categories I would therefore need to find all the markdown files which have a row that begins with Category: grep -h \u2018Category: **/*.md - prints each search\u00a0res grep -h \u2018Category: **/*.md | sort - prints and sorts each search\u00a0res grep -h \u2018Category: **/*.md | sort | uniq -c prints and sorts each search result, then counts how many occurrence of each unique result there\u00a0are. I had some repeat results though because some rows had white space at the end, so in order to make these the same, I needed to remove grep -h 'Category: **/*.md | sed | sort | uniq -c This gave me the 6 Category: 2 Category:D 16 2 2 15 15 8 Category:T Category is repeated and isn\u2019t\u00a0need grep -h 'Category: **/*.md | sed | sort | uniq -c | sort | sed This gives me the following output, which is\u00a0accepta 2 Data 2 Engineerin 2 Front-end 6 8 Tools 15 General 15 Startups 16 New\u00a0Catego The next stage was to begin updating these categories with the new, nested categories I\u2019ve decided to try splitting the categories into technical and I can imagine splitting Technical > Data even more in future, perhaps having Data Analytics, Data Science, and Data Engineerin as Technical Data Web Cryptocurr Not technical Family Self Career I cd into the directory containing the markdown files, and then to change all the articles with Category: Tools to Category: Tools I\u00a0did: grep -l 'Category: Tools' *.md | xargs sed -i 's/Categor Tools/g' If I want to see a list of files containing Category: General: grep -H 'Category: General' *.md If I want to see just the file names,\u00a0the grep -l 'Category: General' *.md Update Since writing this post I\u2019ve modified the categories a few times. The commands I run to switch out categories are as\u00a0follows export export newName=Li grep -l \"Category: | xargs sed -i '' Notes: Double quotes are not the same as single quotes. You need to use them if you want to access variables or commands inside a\u00a0string. .* is a wildcard operator allowing any number of characters It\u2019s required when an article belongs to"},{"title":"A New Blog\u00a0Design","url":"a-new-blog-design.html","body":"The blog has a new design! Out with the old, and in with well-writt HTML, an improved CSS framework, maintainab code, dark mode, and articles with This website was my first ever project using HTML and CSS, and the codebase for the original blog was terrible. It was poorly written and hard to maintain. I remember when I was first building it and trying to figure out what a <div> or a <span> really\u00a0was At times I felt like little more than a monkey randomly bashing keys, hitting save and refreshing the browser tab. I felt guilty for spending any non-essent time away from my wife and daughter. I wondered if any benefits would actually materializ that would outweigh the costs of not rushing home to take care of a new-born and relieve a tired and stressed\u00a0m It took awhile, but eventually this blog became the most effective force multiplier I\u2019ve ever\u00a0used. As I\u2019ve learnt more about web developmen the JAM stack has become increasing intuitive and familiar. A side effect was that as I became comfortabl with \u201cgood\u201d dev work, working with this blog\u2019s old code base became increasing uncomforta I wanted to update the blog so that it would be easy and fun to use again. I want to be able to play with it quickly I wasn\u2019t aiming for a radical re-design, I like that the focus is on text and I\u2019m not exploring any on-trend design choices. I think my original design choices have held up well. I want a design that will work for many years, with templates and code that is easy and intuitive to read, and design elements that are easier to work\u00a0with. I hope that I\u2019ll be writing here more regularly over the next few months. It\u2019s been a busy year and there is lots to write\u00a0abou"},{"title":"Product-Led\u00a0Growth","url":"product-led-growth.html","body":"Part 1: Design your\u00a0strat Chapter 1: Why is product-le growth of Product-Le Growth is a go-to-mark (GTM) strategy that relies on using your product as the main vehicle to acquire, activate and Product-Le Growth (PLG) means that every team influences the product: Marketing - how can our products generate a Sales - how can we use the product to qualify our prospects for\u00a0us? Customer Success - how can we create a product that helps customers become successful beyond our\u00a0dreams By having every team focussed on the product, you create a culture that is built around enduring customer\u00a0v By leading with the product throughout the org, PL companies get shorter sales\u00a0cycl lower Customer Acquisitio Costs\u00a0(CAC higher Revenue Per Employee (RPE) A GTM strategy is an action plan that specifies how a company will reach target customers and achieve a In order to select a GTM, you first need to understand ideal\u00a0cust Knowing these elements will help you choose the correct GTM that will acquire, retain and grow you customer base in the most Sales-Led Profit Centers: Sales, Marketing, Cost Centers: Advantages Annual Contract Value (ACV) can be very\u00a0high Enterprise first solutions that are very complex and therefore need a high-touch sales\u00a0mode If the Total Addressabl Market (TAM) is very small, because your market is super niche, you can quite easily talk to almost all market participan (PLG is built for large\u00a0TAMs It\u2019s great for new categories of product where education is required, because you need to change how people approach a problem. This takes time and This is turn requires that you understand your customers pain points, objections core problems. If you jump to quickly a PLG GTM strategy then you risk a high church rate because you haven\u2019t understood or educated your customers well\u00a0enoug Disadvanta Sales cycles are very\u00a0long The Life Time Value (LTV) must be high enough to recoup the investment in CAC. This often requires charging the customer a premium. This premium price isn\u2019t because the product is amazing, or more valuable to the customer, but because the customer acquisitio model is If you use a Sales-Led GTM, you need to watch out for competitor who can sell more efficientl or have a more efficient Customer Acquisitio Model. They can steal your market share by offering the same product with at a lower\u00a0pric Customer Acquisitio methods are super leaky. Most leads (MQLs) never result in a closed deal, this is partly because: it encourages markets to gate content in order to hit their MQL\u00a0goals it focusses on content consumptio as a leading indicator of intent. (but reading a white-pape or brochure doesn\u2019t mean I\u2019m going to buy the\u00a0thing) the entire process rewards creating friction in the buying\u00a0pro Consequent there is often a disconnect between marketing and\u00a0sales Product-Le Switching from Sales-Led GTM to Product-Le GTM creates a defensive\u00a0 A product led marketing team asks \u201cHow can we use the product to qualify our prospects for\u00a0us?\u201d A product led customer success team asks \u201cHow can we create a product that helps customers be successful without our\u00a0help?\u201d A product-le engineerin team asks \u201cHow can we create a product with a Growth is much faster, because: you need fewer resources to\u00a0scale, the top of the customer funnel is much\u00a0wider CAC is much\u00a0lower Higher RPE (revenue per\u00a0employ User experience is\u00a0better Chapter 2 - Free, Freemium or\u00a0Demo Use the MOAT framework to pick the right GTM\u00a0strate Market Strategy - Dominant? Ocean Conditions - Blue or\u00a0Red? Audience - Do you have a top-down or bottom-up Time to Value - How much time do you need for a customer to experience the\u00a0value? Dominant - Do it better than the competitio and charge a lower\u00a0pric Is your TAM big\u00a0enough Does your product solve a specific job significan better and at lower cost than anyone else on the\u00a0market Can the user realize significan ongoing value quickly with little or no\u00a0help? Do you want to be the undisputed market\u00a0lea Differenti - Pick and win a fight against an industry giant Main line of defense against the giant Focus on an Do a specific job better than the competitio but charge This is not a Free trials and demos work well\u00a0here. Because its specialize combining freemium with quick time-to-va is\u00a0difficu Main competitiv advantage is how you solve your Does your market have Is the TAM big\u00a0enough is the ACV high\u00a0enoug Could prospects experience a Magic Moment during a free\u00a0trial Disruptive - Charge less for an inferior product (e.g. Canva, Google docs) Build a simpler product that solves a specific pain point, and because its simpler, its faster, you can charge less, and its more appropriat to some over-serve or It\u2019s a Costs must be\u00a0low Product must be easy to\u00a0use Is TAM large\u00a0enou Can on-boardin be Freemium model\u00a0thri Chapter 3 - Red-Ocean or Red-ocean companies try to outperform their rivals in order to grab a larger share of the existing market. As the market gets crowded, opportunit for profit and growth reduces. Products become commoditie Cut-throat competitio turns the waters\u00a0red Blue-ocean companies access untapped market space and create demand. They have opportunit for highly profitable growth. Competitio is irrelevant Create and capture new\u00a0demand Some markets will be red-ocean, but a particular niche within it will be blue\u00a0ocean Blue Oceans require educating the customer to create the demand. This is high-touch and often a PL GTM strategy isn\u2019t going to work. It needs to be sales-led in order to educate the customer enough. But if Time-To-Va (TTV) is short then PLG could be\u00a0great. Red Oceans need big wide funnels in order to compete, and PL GTM strategies work great. They\u2019re defensible keep costs low and make sales cycles\u00a0sho Chapter 4 - Top-Down or Bottom-Up PLG works for\u00a0bottom High touch sales-led strategies work for Top-Down enterprise sales strategies where the product is super complex and the sales cyccle is very\u00a0long. Top-Down (Sales team is the High ACV Lower customer\u00a0c Poor High CAC Long sales\u00a0cycl Free Trial Bottom-Up (Product is the Low CAC Predictabl sales\u00a0figu Scalable\u00a0f Small contracts\u00a0 upfront investment as non-paying customers Freemium Free-Trial Chapter 5 How much time until you deliver on your promise to\u00a0prospec How much time until the product sells\u00a0itse PL GTM strategies require Rank your uses across 2 dimensions and group into 4 quadrants. The dimensions are Ability (low - high) and Motivation (low - high) Low Motivation - Low Ability = High Motivation - Low Ability =\u00a0Rookie Low Motivation - High Ability =\u00a0Veteran High Motivation - High Ability =\u00a0Spoiled Figure out your top two quadrants. Unless all your users are Spoiled, you need to Questions: How motivated are your\u00a0users Is your product easy for your target audience to\u00a0use? Can users experience the core value (magic moment?) Part 2 - Build Chapter 7 - Build a A positive feedback\u00a0l Understand your\u00a0value Communicat Deliver on your\u00a0promi Repeat Chapter 8 - Understand your\u00a0value If you\u2019re selling live-chat software, you\u2019re not really selling live-chat software, you are selling a new and better way to You are selling the outcome, the result, the\u00a0why. Pain makes us want to change so that we can avoid or prevent the Pain is There are three reasons why people buy a\u00a0product: Functional Outcome - the core task that needs to get\u00a0done. Emotional Outcome - how a customer wants to feel or avoid feeling as a result of the Social Outcome - how a customer wants to be perceived by others by using your\u00a0produ In every software, there are usage patterns that point towards to core outcomes that are most important to\u00a0custome One of the biggest difference between sales-led and product-le companies is that SL companies monitor usage patters to see if users are accomplish meaningful outcomes. These outcomes are referred to as value-metr A value-metr is the way you measure value exchange in your product. They are the linchpin to successful execution of a product-le GTM strategy because you are aligning your revenue model directly with your customer acquisitio model. Because your value metrics play a vital role in how you price your product, set up your monitoring and build your\u00a0team. Value metrics could\u00a0be: for Vimeo, number of videos uploaded by the\u00a0user. for Slack, number of messages sent by the\u00a0user for PayPal, amount of revenue generated by the There are functional and outcome based value metrics. Functional value metrics are \u201cper user\u201d or \u201cper 100 videos\u201d. Pricing scales around functions of usage. Outcome based value metrics charge based on the outcome, e.g. how many views a video received, or how much money a customer made when using your payment Many SaaS companies rely on feature as a way to justify higher prices, but this produces higher\u00a0chu Value metrics outperform feature with upto 75% less\u00a0churn Outcome based value metrics reduce church by an additional A good value metric is easy for a customer to understand They need to immediatel understand what they\u2019re paying for and where they fit in your structure. If you\u2019re in an establishe market, it makes senses to look at what the A good value metric is aligned with the value that the customer receives through the product. Consider the low-level components of your high-level outcome. E.g. what low level actions are necessary to get the end result? Sending lots of messages? Meeting lots of people? Finding lots of\u00a0things? A good value metric grows with your customers usage of the valuable outcome. If customers get incredible value from the product, charge them more because the product is worth it. Also, if they aren\u2019t getting much value from the product, charge them\u00a0less. Don\u2019t do user based pricing if you want to get lots of users - its a conflict of\u00a0interes If you\u2019re small, you can try different pricing strategies and iterate to\u00a0success Ask\u00a0yourse What do my best customers\u00a0 What do my best customers not\u00a0do? What features did my best customers try first when they What are the similariti along my best customers that led to\u00a0success For churned customers, what were the main difference between them and the best customers? Were they in the ideal audience? Why did they church? What did they do or not do that good"},{"title":"The Mom\u00a0Test","url":"mom-test.html","body":"Summary Ask Avoid bad\u00a0data. Keep it\u00a0casual. Push for commitment Frame the meeting\u00a0we Focus on the right, tight, Prep well, take good notes, review your\u00a0notes Table of Contents Summary Chapter 1 - Opinions are\u00a0worthl Chapter 2 - Avoid bad\u00a0data Chapter 3 - Ask the Chapter 4 - Keep it\u00a0casual Chapter 5 - The currencies Chapter 6 - Finding Chapter 7 - Choosing you\u00a0custom Chapter 8 - Getting the most from the conversati by prepping and\u00a0review Chapter 1 - Opinions are\u00a0worthl Anything involving the future is an overly optimistic lie. You want objective facts about what happened in the\u00a0past. Find out if people care about your idea by never mentioning Forcing yourself not to mention your idea will force you to ask Make sure your questions pass the mom test: Talk about their lives, not your\u00a0idea. Ask about specific objective events in the past, not Spend most of your time listening, not\u00a0talkin Chapter 2 - Avoid bad\u00a0data You aren\u2019t allowed to tell them what their problem\u00a0is They aren\u2019t allowed to tell you what to\u00a0build. Bad data\u00a0inclu Compliment Fluff Hypothetic Ideas Chapter 3 - Ask the You should be terrified of at least one of the questions you ask in Search out the scary questions you\u2019ve been avoiding. What\u2019s the worst thing a prospect could say? What\u2019s the scariest question you could\u00a0ask? A good way to find scary questions is to imagine that your company has failed and then to ask\u00a0why. If you get an unexpected answer to one of your questions and it doesn\u2019t have any effect on what you\u2019re going to do, then was it really worth asking\u00a0it? General advice for hard things includes asking hard questions. Imagine you were delegating the task, what would you tell the person to do? That\u2019s\u00a0you You can ask about\u00a0mone Love bad news - if you find out that your idea is fundamenta flawed you\u2019ve just saved a tonne of time and energy and money. Move on. It\u2019s getting you closer to the\u00a0truth. Bad news isn\u2019t the result of an opinion. No one knows if your idea will work, only the market\u00a0kno Opinions don\u2019t\u00a0coun A lukewarm response to your idea can be a great conversati because you realise that your idea isn\u2019t a great idea. Lukewarm means they don\u2019t care enough to buy the first (worst) version before it\u2019s\u00a0ready Look before you zoom. Don\u2019t focus on details too soon, you need to understand the big picture\u00a0fi You have product risk and customer risk: Product risk - can I build it and grow\u00a0it? Customer risk - is there a big enough group of people who are going to buy\u00a0it? Pre-plan a list of the 3 most important questions (including a scary one) before every meeting or conversati Be ready with these. They should change from week to week. As you get good quality answers to existing questions you can bring in new\u00a0questi Chapter 4 - Keep it\u00a0casual Problem -> Solution ->\u00a0Sales Normally you would have 3 meetings with a client when you make a big sale. This lets you do each stage really well without blurring the data: Identify your Explain your\u00a0solut Sell the\u00a0soluti Identifyin a problem doesn\u2019t have to be a meeting, keep it casual and you will get more honest feedback much faster. It works better as a chat when people are relaxed and saying what they really\u00a0thi It take 5 minutes (max) to identify if a problem exists and is\u00a0importa Give as little info as possible about your idea whilst still nudging the Chapter 5 - The currencies In early stage sales, the main goal is learning. Money is a\u00a0side-eff Will they spend money, time, or reputation on your\u00a0solut If someone is willing to risk reputation or spend time or money on your idea, then you can believe that Hearing a compliment means they\u2019re trying to get rid of\u00a0you. If it\u2019s a bad meeting or you\u2019re not sure what they really think, push for a commitment of some kind. Ask them to spend time, reputation or money and you\u2019ll see what they really\u00a0thi If they aren\u2019t excited (not just interested but in pain and excited for your solution) then you need to find that out ASAP. It\u2019s still a good meeting if you can discover\u00a0t Are you offering pain A lead isn\u2019t a real lead until you\u2019ve given them a concrete chance to reject\u00a0you Ask learning questions by using the Mom Test, then confirm by selling\u00a0it You need crazy customers: They have a painful and They know they have a painful and They have the money to pay you to solve\u00a0it They already have their own bad solution to this terrible problem, and yours is clearly\u00a0be A crazy customer doesn\u2019t say \u201cyeah that\u2019s great, I\u2019m really interested let me know when its ready\u201d. They say \u201cAHHH THIS IS THE WORST PART OF MY LIFE AND I WILL PAY YOU RIGHT NOW TO FIX IT!\u201d A crazy customer will front you the money when all you have is a barely functional prototype made of\u00a0duct-ta A crazy customer is the person reading you blog, searching for workaround when you haven\u2019t spent loads of marketing and Keep a crazy customer close - they\u2019ll stick with you when times are\u00a0tough. Chapter 6 - Finding Vision -> Framing -> Weakness -> Pedestal ->\u00a0Ask Keep having conversati until you stop hearing new\u00a0stuff. If it\u2019s a topic you both care about, find an excuse. You\u2019ll both enjoy the chat. You don\u2019t need to mention your idea if it breaks the\u00a0premis Warm introducti are the ideal way to start a new conversati 6 degrees of freedom in the world, so find someone who knows someone who knows\u00a0them Cold calls - Serendipit - be prepared, be\u00a0bold. Have a good excuse -\u00a0hustle. Landing pages - so that googling the problems brings them to\u00a0you. Organise an event - bring the businesses together for an event. You\u2019ll be considered an expert because you\u2019re Become a subject matter\u00a0exp Speaking and teaching engagement - you get to have strong opinions, and you\u2019ll be\u00a0respect Chapter 7 - Choosing you\u00a0custom Startups don\u2019t starve, they drown. In options, choices, ideas. You Choose a good customer segment, focus on it, and don\u2019t In the beginning: Google - PhD\u00a0studen PayPal -\u00a0eBay. Evernote - Moms with\u00a0recip It will look obvious with hindsight, probably not so obvious before you\u2019ve figured it\u00a0out. If you can\u2019t get a consistent answer to a question, maybe you\u2019re speaking to If you aren\u2019t finding consistent problems and goals then you don\u2019t have a specific enough customer segment. Within the group, which type of person would want it the\u00a0most? Would everyone within the group want to buy/use it? Or only\u00a0some? Why does the subset want\u00a0it? Does everyone in the group have the What other motivation are\u00a0there? Who else (outside the group) has Go for Who-Where pairs of segments. \u201cFinance profession who You want customers who are reachable, profitable and Good segments are usually \u201cwho-where pairs. If you don\u2019t know where to find your customers, keep slicing until you do. Make sure the segment is reachable, profitable and Chapter 8 - Getting the most from the conversati by prepping and\u00a0review Prepare\u00a0we Have your three big questions ready, including the scary\u00a0ones Know who you are speaking\u00a0t Know what commitment and next-steps you want to push\u00a0for. Spend up to an hour writing\u00a0do Your best guesses about what you think they\u2019ll\u00a0sa What they care\u00a0about What they\u00a0want. If you have a focussed segment, you\u2019ll only need to do If you come across a question which can be answered using the internet, use the\u00a0intern Take good\u00a0notes Ask if you can record the\u00a0audio. Record emotions as well as words. Verbatim alone isn\u2019t always accurate 6 months\u00a0lat Use shorthand for follow\u00a0up. Observe and record emotions - happy, angry, meh,\u00a0etc. Pains and obstacles are a lot more important if someone is embarrasse or angry when they\u2019re talking about\u00a0them Dig into the big emotions, find out what\u2019s causing them, or why its a big\u00a0deal. Review your notes Meta - which questions went well or\u00a0not? What were the answers to the How can you do better next\u00a0time? What were the clear signals? What signals did you\u00a0miss?"},{"title":"Obviously\u00a0Awesome","url":"obviously-awesome.html","body":"Intro In order to do better marketing, understand your value and Understand what makes a Positionin is a fundamenta input to every business tactic you\u00a0use If you fail at positionin you fail at marketing and\u00a0sales Positionin is \u201cconext setting\u201d for\u00a0produc Customers need to be able to easily understand What your product\u00a0do Why it is\u00a0special Why it matters to\u00a0them If your prospects can\u2019t figure out what your product does quickly, they will invent a position for you. It might hide your key strengths or misunderst your\u00a0value Find people to demo your product to, then ask them to describe it back to you. Do the same with existing customers. If they\u2019re not saying the same thing then you have a Products with strong positionin Find the best kind of\u00a0custome Make the value\u00a0obvi Sell\u00a0quick Part\u00a01 Positionin as\u00a0context Without positionin to guide us, we don\u2019t know how to understnad a\u00a0product Positionin lets us make assumption about who a product is\u00a0for what its main features\u00a0a how much it\u00a0costs Without it we would be paralyzed by choice, and we wouldnt be able to make sense of all the products around\u00a0us. The context and purpose of a product from the perspectiv are often different to those of the\u00a0prospe Products can be positioned in multiple ways, and often the best positionin is not the Bad positionin makes it harder for prospects to figure out if your product is worth Positionin requires considerin Customers point of view - what problem/pa does it\u00a0solve The ways your product is The of a The best market context to make your value Five (plus one) components of Competitiv Alternativ - if you didn\u2019t exist, what would your customers use instead? Its what your prospects would use to do the task if your product didn\u2019t exist. It could be excel, or pen and paper. It could be nothing, (in which case maybe you product doesn\u2019t solve a real pain\u00a0point You probably know a lot more about your market, and problem, and alternativ solutions, than your prospects do. (\u201cDoing nothing\u201d could be an opportunit not a red\u00a0flag.) Its important to understand what customers will compare your product with, because this is how they will define \u201cbetter\u201d. Is it \u201ceasier\u201d than a pen and\u00a0paper? Customers might never have purchased a solution like yours\u00a0befo Unique Attributes - What do you have that alternativ do not? Could be: Delivery Model (online vs offline, installed on-site vs\u00a0not) Business model (rental vs\u00a0purchas Specific expertise (data scientist with financial and back-end web dev\u00a0expert Value (and quantifiab objective proof) - What value do the attributes enable for customers? If unique attributes are your secret sauce, the value is why someone might care about your secret\u00a0sau Fact based, provable, demonstrab quantifiab objective. Third party opinions are\u00a0releva Target market - Who cares a lot about that value? Focus on customers who are most likely to purchase quickly, wont ask for discounts, will tell their friends about\u00a0you. You should clearly identify who these are\u00a0and Identity what sets them apart from other groups of customers. Why are they uniquely likely to buy from you when others wouldnt or would take longer to consider and make a\u00a0purchase Market Category - What context makes your value obvious to the ideal customer? Declaring that your product exists in a certain market triggers a powerful set of\u00a0assumpt make these assumption help you, not hinder\u00a0you When presented with your product, customers will try to use what they already know to figure out what your product is all about and why its special. You want them to be able to do this really quickly and\u00a0easily Make these assumption work for you, not against you. You won\u2019t have to list every feature, because most of them will be assumed by the context of the Get it right, and sales efforts (copy) wont be wasted battling those assumption but can instead build off of them to show secret sauce and value (Bonus) Relevant Trends - what trends make your product relevant right now? Used carefully, trends can show prospoects why they should pay attention to your product right now. It can increase urgency, excitement but the trend must be directly relevant, practicall and Blockchain AI, ML are trends - they are relevant to Each of these components is relevant to the\u00a0others"},{"title":"How I learnt to\u00a0code","url":"How-I-learnt-to-code.html","body":"4 years ago I started learning how to code, and it was really difficult! It\u2019s still difficult, but I now have a collection of tools and perspectiv that make it less daunting (more on that below). Leveling up requires one more abstractio to wrap my head around, or one more API to understand I can do\u00a0it. But I don\u2019t think it needed to be so difficult, so now I\u2019m building Code School Meta to make it easier to learn how to\u00a0code. Learning to code has been fun, ultimately successful and life changing (hello job security!) but at the beginning it was sooo slow, and super\u00a0tric I remember feeling like an imposter, and thinking that I was definitely not a real developer - I hadn\u2019t taken any classes in computer science. I felt like I knew almost nothing The process began with Pandas, a (the) Python analytics library. Spreadshee were slowing me down at work, and I was bored. I found this great tutorial by Brandon Rhodes. From there I found out about Jupyter Notebooks, and then I found about Github pages and how to make a blog using pelican. That led to HTML and CSS (and also JavaScript which I tried very hard to avoid for as long as possible). I felt like a monkey bashing a keyboard as I tried to make HTML elements do what I\u00a0wanted. Git was next, but I found this amazing tutorial to help me learn. It made Git seem OK, and also helped HTML and CSS make more sense too.\u00a0Bonus I suddenly realized that great learning materials are crucial if I was going to keep momentum and keep on enjoying the thrill of seeing the computer do something I hadn\u2019t made it do\u00a0before. So now I\u2019m now working on Code School: Meta. It\u2019s an online community to make it easier to teach yourself how to code - less confusion about how to get started or what to learn next, more encouragem and lots of high If you\u2019d like to know more, please check it out and sign up for\u00a0update"},{"title":"The 1-Page Marketing\u00a0Plan","url":"1-page-marketing-plan.html","body":"Introducti Marketing has three\u00a0phas Before (prospect) - get people to know you and During (lead) - get the to like you and buy from you for the first\u00a0time After (customer) - get them to trust you, buy from you regularly, and\u00a0refer new\u00a0custom Marketing is strategy, all the other things are tactics Jargon free definition of\u00a0marketi Advertisin - The circus is coming to town, you paint a sign saying \u201cCircus coming to town on\u00a0Saturda Promotion - Put the sign on the back of an elephant and walk it into\u00a0town Publicity - Walk the elephant through the mayors flower beds and let a newspaper write about\u00a0it Public relations - Get the mayor to laugh about\u00a0it Sales - the town people go to the circus, you explain how much fun the entertainm is, they buy some tickets, you answer their questions, they spend lots of money on food, games,\u00a0sho Marketing is the plan that made the whole thing\u00a0happ"},{"title":"Learning to\u00a0market","url":"marketing-101.html","body":"Since April I\u2019ve been able to work full time as a solo founder. I\u2019ve challenged myself to build something useful enough that customers would want to pay for it and in the process of seeking that goal I\u2019ve become a much better and more I\u2019ve been working solo for about 7 months now, and in that time I\u2019ve begun using test driven developmen I\u2019ve built non-trivia data driven web apps using Django, and I\u2019ve learnt how to deploy and monitor those apps in production and make I\u2019ve made two main apps; moneybar.n and pippip.ema and I\u2019ve learnt so\u00a0much. I\u2019m realising though, that I still have much to learn in other spheres. Being a great developer is deeply meaningful to me. It\u2019s literally a bucket list item for me and I intend to be writing code as long as I live. But there is no point creating products if no one knows they\u00a0exist This is where marketing and positionin comes in. Right now it feels like I know nothing about how to get users, or validate an idea, or position a product. These are all super necessary and super\u00a0unkn On a more meta level, I\u2019m confronted with the lost benefits of working with co-founder or of having friends doing similar things. I want to work faster and make progress more efficientl I need to be part of a\u00a0communit Working in isolation does have its advantages though. I\u2019m self-taugh and self-direc figuring out the contours of uncharted territory and creating my own personal map. In my mind, I have a deep and almost personal relationsh with the coding abstractio and tools I\u2019ve learnt to work with. Classes and functions, strings and floats, literally have (to me) their own textures, colors and weights when I think or dream about\u00a0them It feels like I can pick up these abstractio as if they were physical objects and turn them around to examine them. Place them next to each other and compare the difference Run thought experiemen In my experience this kind of relationsh and affection simply doesn\u2019t happen when taking a class or following someone elses schedule. It\u2019s satisying to feel ownership of a skill like this, and it\u2019s one of the primary reasons I consider coding to be similar to a\u00a0craft. Having said all that, now that I realise I need to validate my product, position it, and figure out marketing, I\u2019ve stopped writing code, put down my tools, and I\u2019m going to learn marketing. I\u2019ve bought some books. Maybe I\u2019ll post some reviews here\u00a0later In no particular order here is what I plan to\u00a0read: The 1-Page marketing plan, by Allan Dib Obviously Awesome, by April\u00a0Dunf Lean Analytics, Hooked, How to build habit-form products, by Nir Eyal\u00a0The Mom Test, by This is marketing, by Seth\u00a0Godin Product-le growth, by Wes\u00a0Bush I don\u2019t know if I\u2019ve covered 90% of the distance required or 50%. It\u2019s exhausting but I\u2019m here for the journey not"},{"title":"Pippip.Email","url":"pippip.email.html","body":"PipPip 6 weeks ago I had an idea for a product whilst reading a news\u00a0artic It would be great if the writing and sending of important messages could be separated, so that I could write something long before I needed to send it, and know that then sending would happen at the right time without having to think about it. This could be useful for sending my daughter a message on her 15th birthday, or to my wife on our 10th anniversar I also designed a check-in mechanism, so that messages could be sent if I disappeare or pass\u00a0away. PipPip is the result - event-driv and scheduled email delivery, from days to\u00a0decades We\u2019re working on validating the idea and finding the right"},{"title":"Between\u00a0Clients","url":"between-clients.html","body":"At the end of the summer I had time between engagement to work on some side projects and grapple with some new (to me) libraries During August and September 2019,\u00a0I: Practiced creating websites using the Investigat and demo\u2019ed - a library that allows exposing Plotly Dash apps to Built a personal finance dashboard using Plotly Dash and began turning it into a web app using Django and Interviewe for a role at CoinMetric and created this investigat as a Created a company \u201cAtlas Consulting Internaton to facilitate life as a freelance data scientist Spent a lot of time working at the cafe in IKEA because my co-working space wouldn\u2019t let me stay late or work on Introduced myself to this new coworking space and suggested we could work together to create the best tech-focus startup hub and coworking space in Bought our first car. It required a lot more time and research than I\u00a0expected Created Texni Data Consultanc with Dan Caputo to provide data strategy Built the website for Texni Data using django and deployed it to Heroku with a custom domain\u00a0nam Deployed a django app to Google Cloud Platform using App Engine. Created a business website to represent my work as a freelance data scientist and made my blog a subdomain of this\u00a0site. Moved my blog off of github pages and onto firebase. Thanks Github for several years of simple trouble free\u00a0hosti Experiment with using storage buckets on Google Cloud Platform to host static sites and serve them over SSL. My conclusion is that serving static sites using storage buckets is great - it\u2019s simple, quick and cheap (free). But adding SSL proved too difficult. I spent too many hours trying to create a loadbalanc that would work for both the root domain johnmathew and also a subdomain In the end I found out about firebase. Firebase is also quick and cheap (free) and simple enough to Created a photo book using PhotoBox that turned out great. It covers the last 3.5 years and is mostly full of snapshots of our kids and selfies with\u00a0Ritsy Chose a primary school for my daughter to go to next\u00a0year."},{"title":"Analysis of the mean and median value of transactions on 5\u00a0Blockchains","url":"btc-fork-analysis.html","body":"This analysis was prepared for Coin Metrics as part of their recruitmen process. It is a short demonstrat of my thought process. The additional steps required to develop this into a useful analysis are CoinMetric Case - to evaluate skills and abilities in multiple\u00a0w Importing\u00a0 Wrangling\u00a0 Exploring\u00a0 Analysis Modeling Provide: A written explanatio of how to approach the\u00a0proble Present the beginning phases of implementa using coin metrics\u00a0da Of the four options made available in the case study, option 3 was\u00a0chosen Advocating for CoinMetric data\u00b6Produ quality research that is of value to potential clients (doesn\u2019t have to be complete) with a particular focus on network\u00a0da Initial ideas\u00b6My first rough ideas\u00a0were Compare different Bitcoin based chains, (BTC, BCH, LTC, BSV) to test the influence of whales and compare this to their respective (evolving) claims to be a store of value (SoV) and/or alternativ to\u00a0cash. Develop and expand some of the research by Willy Woo. I find his research to be outstandin In particular I think the following metrics merit days\u00a0destr hodl\u00a0waves thermo\u00a0cap average\u00a0ca Tracking the number of twitter followers of various crypto-twi thought leaders and celebritie to test the hypothesis that \u201can increase in follower numbers shows that new retail investors are entering and an increase in price is expected\u00a0s Thought leaders / crypto celebritie could be further grouped by what types of coins they speak about most - smart contracts, DeFi, privacy coins,\u00a0etc Weibo could be analysed as well as Twitter to understand Chinese markets, Korean twitter could be analysed for the Korean I have an existing side project which has the goal of using a recurrent neural net using an LSTM architectu to predict BTC price movements. The app (model, stored data, data pipeline, visualizat of results) will run autonomous on Google Cloud Platform. Candle data is consumed from CoinAPI.io and stored in\u00a0BigQuer Technical indicators will be calculated and used as additional factors to the model. Sentiment analysis from news outlets (Bloomberg FT) would be added\u00a0late The model would be written using TensorFlow and the BigQuery tables names would use BQ\u2019s date format capabilite This would make the project faster and\u00a0cheape Idea 1 seemed like a sensible option. Ideas 3 and 4 are interestin and worth investigat but not possible within the scope of this\u00a0exerc Testing the influence of whales\u00b6and \u201cnormal users\u201d on BTC and 4 BTC forks, and discussing results in the context of each chain\u2019s claimed technical advantages and use cases as e.g. a store of value or alternativ to\u00a0cash This will be achieved by comparing daily mean USD transactio value to daily median USD transactio value. This is done by calculatin the mean-media ratio of transactio value (MMR). Hypothesis If a chain has a much smaller median transactio size than mean transactio size, then on chain activity is dominated not by regular users making normal daily transactio but by whales moving large amounts of currency to artificial inflate usage\u00a0metr This could contradict claims that a blockchain has an active user base that the blockchain is meeting user needs. We assume\u00a0tha If a blockchain is functionin as digital cash, then most of its transactio would be small. e.g less than 100 USD. It should be noted that 100 USD is not a particular small amount even in western countries and due to a blockchain borderless nature, it is even futher above a noraml \u2018day-to-da transactio amount in large parts of the\u00a0world. Conversely if a blockchain has relatively little organic use by normal users then whales (users with large holdings) will make up a large proportion of on-chain activity and would have average transactio sizes much larger than a day-to-day transactio An untested guess at a \u201cwhale threshold\u201d could be 100,000USD Where the ratio of mean to median transactio value is relatively high, we have an environmen where the mean value is much higher than the median value, which shows that daily total value transacted is dominated by a few relatively large transactio rather than many small value transactio This would imply that whales dominate the blockchain (and likely market behavior) rather than members of the general public or Chains:\u00b6Th chains that will be analysed here are all forks of BTC. They\u00a0are: BTC BCH BSV LTC DOGE Fields\u00b6usi the coinmetric api, the following metrics will be\u00a0used: The sum USD value of native units transferre divided by the count of transfers (i.e., the mean \u201csize\u201d in USD of a transfer) that\u00a0inter TxTfrValMe The median USD value transferre per transfer (i.e., the median \u201csize\u201d in USD of a transfer) that\u00a0inter In\u00a0[1]: from import HTML function code_toggl { if (code_show } else { } code_show =! code_show } $( document </script> <font> This analysis was made using Python. You can toggle the code visibility by clicking <a ''') Out[1]: function code_toggl { if (code_show } else { } code_show =! code_show } $( document This analysis was made using Python. You can toggle the code visibility by clicking here. In\u00a0[2]: # import and setup import requests import json import pandas as pd import as go import as py import plotly from plotly.off import from import = \"all\" import warnings = 'local'}; if {font: if (typeof require !== 'undefined { exports, module) { /** * plotly.js v1.48.1 * Copyright 2012-2019, Plotly, Inc. * All rights reserved. * Licensed under the MIT license */ function a(o,!0);va c=new Error(\"Can find module 0.3s ease 0.3s ease 0.3s ease 0.3s ease 0.3s ease 0s;\",\"X:ho .modebar-b .modebar-g 0, 0, 0, 0, 0, solid .vertical .vertical .vertical solid solid a in i){var 0-183 41t-147 114q-4 6-4 13t5 11l76 77q6 5 14 5 9-1 13-7 41-53 0 110 23t92 61 61 91 22 111-22 111-61 91-92 61-110 23q-55 0-25 11t-11 25v250q0 24 22 33 22 10 39-8l72-72 57 137 88t159 31q87 0 166-34t137 91-137 0 0 -1 0 0-25 10t-11 26v267q0 1 0 2t0 2l321 264 321-264q1- 1-4z m124 0-12 3l-386 2-12 7l-35 41q-4 5-3 13t6 12l401 334q18 15 42 8 5 13t13 5h107q8 0 0 0 -1 0 386q0 8-5 13t-13 5q-37 5-13t13-5 12 5 5 13q0 23 16 38t38 16q8 0 13 5t5 13z 42-42 101 42 101 101 42 101-42 42-101z m643 320q0 89-62 152t-152 63-151 151-63 152 63 62 151z m-571 m929 0-50 21t-21 51v714q0 30 21 51t50 21h858q29 0 0 0 -1 0 251c40 63 63 138 63 218 0 224-182 406-407 406-224 407-406c80 0 155 22 218 62l250-250 125 125z m-812 250l0 438 437 0 0-438-437 0z m62 375l313 0 0-312-313 0 0 0 0 -1 0 350l-187 188 0-125-250 0 0 250 125 0-188 187-187-18 125 0 0-250-250 0 0 125-188-18 186-187 0 125 252 0 0-250-125 0 187-188 188 188-125 0 0 250 250 0 0-126 187 0 0 -1 0 787l0-875 875 0 0 875-875 0z m687-500l- 0 0-187-125 0 0 187-188 0 0 125 188 0 0 187 125 0 0-187 187 0 0 0 -1 0 788l0-876 875 0 0 876-875 0z m688-500l- 0 0 125 500 0 0 0 -1 0 850l-187 0-63 0 0-62 0-188 63 0 0 188 187 0 0 62z m688 0l-188 0 0-62 188 0 0-188 62 0 0 188 0 62-62 0z m-875-938l 188-63 0 0-188 0-62 63 0 187 0 0 62-187 0z m875 188l0-188- 0 0-62 188 0 62 0 0 62 0 188-62 0z m-125 188l-1 0-93-94-15 156 156 156 92-93 2 0 0 250-250 0 0-2 156 94 92 0 2-250 0 0-250 0 0 93 93 94 0 0 0-250 250 0 0 0-94 93 156 157 156-157-93 0 0 250 0 0 0 0 -1 0 725l0 0-375-375 375-374 0-1 1125 0 0 750-1125 0 0 -1 0 786l0 2-187-188 188-187 0 0 937 0 0 373-938 0z m0-499l0 1-187-188 188-188 0 0 937 0 0 0 0 -1 0 m228 m225 m225 0 0 -1 0 5l-17 108v41l-13 130-66c0 0 0 38 0 39 0-1 36-14 39-25 0-341 34-353 59 3 60 228 110 228 0-120 293-142 474-142 155 0 477 22 477 142 0 50-74 79-163 96z m-374 0-24 65-43 144-43 79 0 143 19 143 43 0 19-42 34-98 40v216h87l m167 515h-136v1 16 31 34 46 52l84 0 0 -1 0 660c-5 4-9 7-14 11-359 28 58-400c0 1 1 1 2 2 118 108 351 249 351 249s-62 27-100 42c88 83 222 183 347 122 16-8 30-17 44-27-2 1-4 2-6 4z m36-329c0 0 64 229-88 296-62 27-124 14-175-11 157-78 225-208 249-266 8-19 11-31 11-31 2 5 6 15 11 117-50 198-32-121 80-199 346-199 0 55-226 155-287z m603 133l-317-1 0 4-4 19-14 7-5 24-15 4c235-287 536-112 536-112l31 100 299-4-1z m-298-153c 14-9 24-15 0 0-17 10-24 0 0 -1 0 450c-83 0-83 67-150 150-150 83 0 150 150 150 0 83-67 150-150 150z m400 150h-120c- 0-34 13-39 29l-31 93c-6 15-23 28-40 28h-340c-1 45-100 100-100h80 0 100 45 100 100v450c0 55-45 100-100 100z m-400-550c 0-250 112-250 250 0 138 112 250 250 250 138 0 250-112 250-250 m365 380c-19 0-35 16-35 35 0 19 16 35 35 35 19 0 35-16 35-35 0 0 -1 0 413l-188-1 37-17 71-44 94 64 38 107 107 107 187 0 121-98 219-219 219-121 0-61 25-117 66-156h-11 33 49 76 49 125 0 103-84 187-187 26-107 56-126 125-126h50 0 125 56 125 126l188-12 0 62 28 62 63v375c0 35-28 63-62 63z m-750 0c-69 0-125 56-125 125s56 125 125 125 125-56 m406-1c-87 0-157 70-157 157 0 86 70 156 157 156s156-70 0 0 -1 0 82v107q0 8-5 13t-13 5h-107q-8 0 13 5t5 13z m143 375q0 49-31 91t-77 65-95 23q-136 0-207-119- 4-24l74-55 10-4 9 0 14 7 30 38 48 51 19 14 48 14 27 0 0 13 5t5 13q0 10 12 27t30 28q18 10 28 16t25 19 25 27 16 34 7 45z 58-155 156-58 215 58 215 155 156 216 58 215-58 156-156 0 0 -1 0 m500 0h72v500q0 8-6 21t-11 20l-157 156q-5 6-19 12t-22 0-37 16t-16 22 16 38t37 16h465q22 0 m-214 518v178q0 8-5 13t-13 5h-107q-7 0 13 5t5 13z 0-38 16t-16 38v750q0 22 16 38t38 16h517q23 0 0 0 -1 0 538c-36 207-290 336-568 10-57 36-108 76-151-13- 11-137 68-183 34-28 75-41 114-42l-55 0 5-45 14-11 34-8 45 4 1 1 2 3 2 5l0 0 113 140c16 11 31 24 45 40 4 3 6 7 8 11 48-3 100 0 151 9 278 48 473 255 436 462z m-624-379c 14-149 48-197 96 42 42 109 47 156 9 33-26 47-66 41-105z m-187-74c- 16-33 37-39 60 50-32 109-55 8z m360 8 62-16 128-68 170-73 59-175 54-244-5-9 20-16 40-20 61-28 159 121 317 333 354s407-60 0 0 -1 0 850l0-143 143 0 0 143-143 0z m286 0l0-143 143 0 0 143-143 0z m285 0l0-143 143 0 0 143-143 0z m286 0l0-143 143 0 0 143-143 0z 143 0 0 143-143 0z m857 0l0-143 143 0 0 143-143 0z 143 0 0 143-143 0z m857 0l0-143 143 0 0 143-143 0z 143 0 0 143-143 0z m286 0l0-143 143 0 0 143-143 0z m285 0l0-143 143 0 0 143-143 0z m286 0l0-143 143 0 0 143-143 0 0 -1 0 0-104 47-104 104 0 57 47 103 104 103 57 0 103-46 103-103z m-327-39l9 0 0 92-92 0z m-185 0l92 0 0 92-92 0z m370-186l9 0 0 93-92 0z m0-184l92 0 0 92-92 0 0 -1.5 0 {fill: #119dff;} .cls-2 {fill: #25fefd;} .cls-3 {fill: strict\";va strict\";va n=i(t,new instanceof _(t,e,r,n) e)throw new argument must be a expected unwanted \"+e.operat n=new t};var e=[];for(v r in l(t,e){var c(t,e){ret t}function a;var e={};retur l=e.name?\" f(e)}var x(e)&&(b=\" \")+\" \"+t.join(\" \")+\" \"+r[1];ret r[0]+e+\" \"+t.join(\" \")+\" o+\": \"+s}functi p(t){retur t}function g(t){retur t}function t}function y(t){retur void 0===t}func x(t){retur b(t)&&\"[ob _(t){retur b(t)&&\"[ob w(t){retur instanceof t}function T(t){retur A(t){retur new Error(\"Inv string. Length must be a multiple of 4\");var strict\";va strict\";va strict\";va c=0;var r)f=o(r);e 52;return strict\";va i=new e=t|t-1;re strict\";va Error(\"For raw data width and height should be provided by instanceof instanceof instanceof instanceof instanceof t instanceof can only safely store up to 53 array length 26;var e=t,r=0;re 0;for(var t&&t>=0);v t&&t>=0);v e=new e=new new a(1);for(v t&&t>=0);v works only with positive a(0),mod:n i=new a(1),o=new a(0),s=new a(0),l=new i,o=new a(1),s=new v[t];var y;else x;else new Error(\"Unk prime \"+t);e=new _}return works only with works only with red works only with works only with red f}}}functi strict\";va _(t){var w=[\"functi k(e,o){var \"+e);var new 0,r)};var n=\"for(var M=f[2*x];v S=f[2*x+1] E=f[2*_];v C=f[2*_+1] L=2*y;var z=2*b;var O=2*w;var I=2*p;var D=2*g;var P=2*d;for( R=0;Rt;){v u(t,e,r,n) l=new EventEmitt memory leak detected. \"+s.length listeners added. Use to increase t}function 0:return 1:return 2:return 3:return t=new instanceof Error)thro e;var l=new \"error\" event. for(var strict\";va new value \"'+t+'\" is invalid for option \"size\"');v e=new e)throw new TypeError( \"string\" argument must be of type string. Received type u(t)}retur t)return new encoding: \"+e);var TypeError( first argument must be one of type string, Buffer, ArrayBuffe Array, or Array-like Object. Received type \"+typeof new to allocate Buffer larger than maximum size: bytes\");re 0|t}functi t)throw new TypeError( \"string\" argument must be one of type string, Buffer, or ArrayBuffe Received type '+typeof t);var 0;for(var d(t,e,r){v new encoding: new must be a ... new TypeError( \"target\" argument must be one of type Buffer or Uint8Array Received type '+typeof t);if(void new of range 0;for(var 0)}var new to write outside buffer new encoding: M(t,e,r){v new to access beyond buffer new argument must be a Buffer new out of new out of new should be a new out of n)throw new must be a new encoding: a}function B(t){retur i}function j(t,e){ret t instanceof V(t){retur strict\";va l(t,e){ret e in strict\";va o(t,e){ret c(m,v,s,f) i=new null;var l(t,v)};va u(t,e){ret strict\";va strict\";va t instanceof Uint8Array instanceof strict\";va strict\";va strict\";va new Error(f+\" map requires nshades to be at least size l(t,e,r){v 0:return 0;case 1:return t[0]-e[0]; 2:return 3:var a;var 4:var t}(o,r)}}; strict\";va strict\";va t)throw new Error(\"Fon argument must be a new Error(\"Can parse an empty new Error(\"Mis required new Error(\"Unk or unsupporte font token: new Error(\"Mis required p(t){var strict\";va Error(\"Unk keyword t}function g(t){for(v a}return a}return strict\";va e=new a=0;a0)thr new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array args\")}els new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array index\")}el new Error(\"cwi Too many arguments in pre() new Error(\"cwi Too many arguments in body() new Error(\"cwi Too many arguments in post() block\");re strict\";va i(t,e,r){v x=new cwise routine for e=new new return strict\";va e=[\"'use strict'\",\" function (!(\"+l.joi && \")+\")) throw new Error('cwi Arrays do not all have the same {\"),e.push (!(\"+c.joi && \")+\")) throw new Error('cwi Arrays do not all have the same r(t){var r;return n}}}var s(t){retur l(t,e){var c(t,e){var u(t,e){var _(t,e,r){v T(t,e){var n}function n(n){var d,g=new r;var e=[];for(v r in e=[];for(v r in e=[];for(v r in r(t,e){var n in r}function n(){}var v(t){var e;return j(t){retur V(t){retur new Error(\"unk type: r,n,i=new h(){if(r){ o(t){for(v a(t,e){var o(t){retur l(t){var t;var t=[];retur t=[];retur f(t){for(v g(t,e){for b(t,e){var _(t){var 0;var w(e),r=new w(r),n=new X=function t(e){funct s(e){var i(i){retur i(i){var new a}return i(i){var i}function a(e){var u(t){retur f(e){var u=s[e];ret n(t){var +r+\"v\"+ t;var n(t){retur t[0]}funct i(t){retur t[1]}funct a(t,e,r){v a=new 0}function s(t){for(v e}var t,e,r=new this;var 1:do{(o=ne 2:do{(o=ne 3:do{(o=ne t=[];retur C(t,r,i,a) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 e}(e);else L(t,e){ret z(t,e){ret O(t,e){ret I(t,e){ret D(t,e){ret P(t){retur R(t){retur F(t,e){var B(t,e){var j(t,e){ret 0});var Y(t,e){ret F(){var v(t,r){var w(t){retur new _(t)}funct k(t){retur t[0]}funct T(t){retur t[1]}funct A(){var l(r){var f(){return S(t,e){ret E(t){retur z(t){funct e(e){retur new L(t(e))}re O(t){var I(){return D(){var O(r())},de O(n())},de O(i())},de O(a())},de F(t){retur B(t){retur N(t){var l(){var a=1;a0)for kt=functio t(e){funct r(t){retur e?new wt(t,e):ne gt(t,0)}re At=functio t(e){funct r(t){retur e?new Tt(t,e):ne mt(t,0)}re St=functio t(e){funct r(t){retur e?new Mt(t,e):ne xt(t,0)}re Ct(t){retu r}function Vt(t,e){re t[e]}funct Ut(t){var Ht(t){for( n}function qt(t){var Gt(t){for( v}return t=N(U);ret a(){var new new new new new new new new new new a(r){var y(){var p(t){retur m(t){retur r};var x(t,e){for r in e;var e=new L;if(t)for c(){var r,n=new i in z(){var O(){var d(){var in g(){var v(){var re(t){retu ne(t){retu ie(t){retu se(t){retu new new Vt;functio ce(t){retu fe(t,e,r){ o;if(i)ret i=!1,a;var Me(){for(v t}function Se(){for(v Oe(t){retu t+\"\"}var n(e){var ir(t){var or(t){for( b=u&&h;ret u(t){var r(r){for(v in u}(e)}};va %b %e %X this.s}};v cr=new lr;functio ur(t,e,r){ zr(t){var Or(t,e){re Rr(t){var Br(t,e){re d(t,o){var u}}functio Jr(t){retu Kr(){var r=e;return A(t,a){ret o(t,e){ret s(t,e){var an(t){var on(t,e){va o(t,e){var c(t){var vn(){var r}function 0 1,1 0 1,1 _n(){var t,e;functi r(r,n){var kn(){var i(t,e){var Tn(t){var r}function An(t){var x(r,i){var En(t){retu t})()}func Cn(e){var A(){return a(t,r,n){v Ln(t){retu i(){var b(){return t,e,r;func n(n,i){var r(e,r){var Wn(t,e){va $n;functio Xn(t,e){va n(t);funct _i(t,e){va n;var s;var wi(t,e){va _i(r,e);va Ti(t,e){re Mi(t){var s,l=1/0;re function n}function $i(t,e){va 1;var n(t,n){var function t(e,r,n,i) n}function Ha(t){retu qa(t,e){re Ga(t,e){re c}function o(t){var a(a,o){var f=function t(e){var t(e){var u(t,e){for d(t,e,r,i) l(t){retur function s(t){retur l(t){retur jo(t){var s(a){var Ho(t){retu qo(t){for( c=2;cAt)+\" \"+e}functi 0,0 \"+n}return n(n,i){var \"+l[2]+\" \"+l[3]}ret function() 0,\"+e+\" \"+e+\",\"+e+ u[n]:delet v(){var if(_){var L(){var e=0;es*l){ a}function o(t){for(v r,n;for(r= t;e||(e=t) e}function s(t){var l(t,e,r,n) c(t,e,r){v n=t;do{var n}function l=t;do{for f(t,e){ret r}(t,e)){v v(t,e){ret b(t,e){ret e){e=0;for strict\";va strict\";va strict\";va strict\";va strict\";va t&&(t instanceof strict\";va strict\";va instanceof n))throw new requires strict\";va strict\";va strict\";va instanceof n))throw new requires new specify data as first t[0][0]){v new Error(\"sou length \"+c+\" does not match destinatio length must be defined\"); \")+\"px new o(t){retur new s(t,e){ret new t=[];retur t=[];retur 1:return function t(e,r){var 2:return function function new new new new this.tree; e=new new Error(\"Can update empty node!\");va r=new new U=v,H=w,A= HALF_PI) && (b 2) ? + delta, option) : // option 3-n: round to n directions (option == 2) ? + delta, hv_ratio) : // horizontal or vertical\\n (option == 1) ? rawAngle + delta : // use free angle, and flip to align with one direction of the axis\\n (option == 0) ? : // use free angle, and stay upwards\\n (option ==-1) ? 0.0 : // useful for backward compatibil all texts remains horizontal rawAngle; // otherwise return back raw input isAxisTitl = (axis.x == 0.0) &&\\n (axis.y == 0.0) &&\\n (axis.z == 0.0);\\n\\nv main() {\\n //Compute world offset\\n float axisDistan = position.z vec3 dataPositi = axisDistan * axis + offset;\\n\\ float beta = angle; // i.e. user defined attributes for each tick\\n\\n float axisAngle; float clipAngle; float flip;\\n\\n if (enableAli {\\n axisAngle = (isAxisTit ? HALF_PI :\\n dataPositi + axis);\\n clipAngle = dataPositi + alignDir); axisAngle += 0.0) ? 1.0 : 0.0;\\n\\n beta += flip * PI);\\n }\\n\\n //Compute plane offset\\n vec2 planeCoord = position.x * mat2 planeXform = scale * mat2(\\n cos(beta), sin(beta), -sin(beta) cos(beta)\\ );\\n\\n vec2 viewOffset = 2.0 * planeXform * planeCoord / //Compute clip position\\n vec3 clipPositi = //Apply text offset in clip coordinate clipPositi += 0.0);\\n\\n //Done\\n gl_Positio = highp GLSLIFY 1\\n\\nunifo vec4 color;\\nvo main() {\\n gl_FragCol = highp GLSLIFY 1\\n\\nattri vec3 vec3 mat4 model, view, vec3 vec3 vec3 main() {\\n\\n vec3 signAxis = sign(bound - vec3 realNormal = signAxis * normal;\\n\\ enable) > 0.0) {\\n vec3 minRange = min(bounds bounds[1]) vec3 maxRange = max(bounds bounds[1]) vec3 nPosition = mix(minRan maxRange, 0.5 * (position + 1.0));\\n gl_Positio = projection * view * model * 1.0);\\n } else {\\n gl_Positio = }\\n\\n colorChann = highp GLSLIFY 1\\n\\nunifo vec4 vec3 main() {\\n gl_FragCol = colorChann * colors[0] +\\n colorChann * colors[1] +\\n colorChann * p=new o=[];funct vectorizin text:\"'+t+ s;var r=0;rr)thr new If resizing buffer, must not specify u(t,e){for new Cannot specify offset when resizing r-1;return n.create() void null;var highp highp GLSLIFY 1\\n\\nvec3 v) {\\n // Return up-vector for only-z vector.\\n // Return ax + by + cz = 0, a point that lies on the plane that has v as a normal and that isn't (0,0,0).\\n // From the above if-stateme we have ||a|| > 0 U ||b|| > 0.\\n // Assign z = 0, x = -b, y = a:\\n // a*-b + b*a + c*0 = -ba + ba + 0 = 0\\n if (v.x*v.x > v.z*v.z || v.y*v.y > v.z*v.z) {\\n return v.x, 0.0));\\n } else {\\n return v.z, -v.y));\\n }\\n}\\n\\n// Calculate the cone vertex and normal at the given index.\\n// The returned vertex is for a cone with its top at origin and height of 1.0,\\n// pointing in the direction of the vector Each cone is made up of a top vertex, a center base vertex and base perimeter vertices.\\ These vertices are used to make up the triangles of the cone by the following: segment + 0 top vertex\\n// segment + 1 perimeter vertex a+1\\n// segment + 2 perimeter vertex a\\n// segment + 3 center base vertex\\n// segment + 4 perimeter vertex a\\n// segment + 5 perimeter vertex a+1\\n// Where segment is the number of the radial segment * 6 and a is the angle at that radial segment.\\n To go from index to segment, floor(inde / 6)\\n// To go from segment to angle, 2*pi * To go from index to segment index, index - d, float rawIndex, float coneOffset out vec3 normal) {\\n\\n const float segmentCou = 8.0;\\n\\n float index = rawIndex - floor(rawI /\\n (segmentCo * 6.0)) *\\n (segmentCo * 6.0);\\n\\n float segment = floor(0.00 + index/6.0) float segmentInd = index - normal = if (segmentIn > 2.99 && segmentInd 0.99 && segmentInd 4.99 && segmentInd max(a, b)) || \\n (p 0 U ||b|| > 0.\\n // Assign z = 0, x = -b, y = a:\\n // a*-b + b*a + c*0 = -ba + ba + 0 = 0\\n if (v.x*v.x > v.z*v.z || v.y*v.y > v.z*v.z) {\\n return v.x, 0.0));\\n } else {\\n return v.z, -v.y));\\n }\\n}\\n\\n// Calculate the cone vertex and normal at the given index.\\n// The returned vertex is for a cone with its top at origin and height of 1.0,\\n// pointing in the direction of the vector Each cone is made up of a top vertex, a center base vertex and base perimeter vertices.\\ These vertices are used to make up the triangles of the cone by the following: segment + 0 top vertex\\n// segment + 1 perimeter vertex a+1\\n// segment + 2 perimeter vertex a\\n// segment + 3 center base vertex\\n// segment + 4 perimeter vertex a\\n// segment + 5 perimeter vertex a+1\\n// Where segment is the number of the radial segment * 6 and a is the angle at that radial segment.\\n To go from index to segment, floor(inde / 6)\\n// To go from segment to angle, 2*pi * To go from index to segment index, index - d, float rawIndex, float coneOffset out vec3 normal) {\\n\\n const float segmentCou = 8.0;\\n\\n float index = rawIndex - floor(rawI /\\n (segmentCo * 6.0)) *\\n (segmentCo * 6.0);\\n\\n float segment = floor(0.00 + index/6.0) float segmentInd = index - normal = if (segmentIn > 2.99 && segmentInd 0.99 && segmentInd 4.99 && segmentInd max(a, b)) || \\n (p strict\";va highp GLSLIFY 1\\n\\nattri vec3 position, vec4 mat4 model, view, float vec4 vec3 main() {\\n vec4 worldPosit = model * vec4(posit 1.0);\\n worldPosit = (worldPosi / + vec4(capSi * offset, 0.0);\\n gl_Positio = projection * view * fragColor = color;\\n fragPositi = highp GLSLIFY 1\\n\\nbool a, float b, float p) {\\n return ((p > max(a, b)) || \\n (p u||ru)thro new Error(\"gl- Parameters are too large for FBO\");var new Error(\"gl- Multiple draw buffer extension not new Error(\"gl- Context does not support \"+f+\" draw buffers\")} new Error(\"gl- Context does not support floating point g=!0;\"dept new i:throw new Error(\"gl- Framebuffe a:throw new Error(\"gl- Framebuffe incomplete o:throw new Error(\"gl- Framebuffe incomplete s:throw new Error(\"gl- Framebuffe incomplete missing new Error(\"gl- Framebuffe failed for unspecifie null;var new Error(\"gl- Can't resize FBO, invalid null;var max(a, b)) || \\n (p FLOAT_MAX) {\\n return vec4(127.0 128.0, 0.0, 0.0) / 255.0;\\n } else if(v max(a, b)) || \\n (p 0){for(var max(a, b)) || \\n (p max(a, b)) || \\n (p max(a, b)) || \\n (p 0.25) {\\n discard;\\n }\\n gl_FragCol = f_color * f_uv) * highp GLSLIFY 1\\n\\nattri vec3 vec4 id;\\n\\nuni mat4 model, view, vec3 vec4 f_id;\\n\\nv main() {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n f_id = id;\\n f_position = highp GLSLIFY 1\\n\\nbool a, float b, float p) {\\n return ((p > max(a, b)) || \\n (p max(a, b)) || \\n (p t&&r>0){va 1}function M(t){var S(t){var E(t){var C(t){var null;for(v function() u=0;u=0){v mediump GLSLIFY 1\\nattribu vec2 vec2 uv;\\nvoid main() {\\n uv = position;\\ gl_Positio = vec4(posit 0, mediump GLSLIFY 1\\n\\nunifo sampler2D vec2 uv;\\n\\nvoi main() {\\n vec4 accum = 0.5 * (uv + 1.0));\\n gl_FragCol = strict\";va m(t){var null}retur new Error(\"web not q(){for(va c=0;c 1.0) {\\n discard;\\n }\\n baseColor = color, step(radiu gl_FragCol = * baseColor. mediump GLSLIFY 1\\n\\nattri vec2 vec4 mat3 float vec4 vec4 main() {\\n vec3 hgPosition = matrix * vec3(posit 1);\\n gl_Positio = 0, gl_PointSi = pointSize; vec4 id = pickId + pickOffset id.y += floor(id.x / 256.0);\\n id.x -= floor(id.x / 256.0) * 256.0;\\n\\n id.z += floor(id.y / 256.0);\\n id.y -= floor(id.y / 256.0) * 256.0;\\n\\n id.w += floor(id.z / 256.0);\\n id.z -= floor(id.z / 256.0) * 256.0;\\n\\n fragId = mediump GLSLIFY 1\\n\\nvaryi vec4 main() {\\n float radius = length(2.0 * - 1.0);\\n if(radius > 1.0) {\\n discard;\\n }\\n gl_FragCol = fragId / strict\";va e;function r(e,r){ret e in instanceof instanceof null;var strict\";va in a)return a[t];var max(a, b)) || \\n (p max(a, b)) || \\n (p max(a, b)) || \\n (p max(a, b)) || \\n (p max(a, b)) || \\n (p 1?1:t}func O(t,e,r,i) n=0;n0){va new c(t,r,a)}; S=new t=0;t=0){v new n(\"\",\"Inva data type for attribute \"+f+\": new n(\"\",\"Unkn data type for attribute \"+f+\": \"+h);var new n(\"\",\"Inva data type for attribute \"+f+\": a};var new i(\"\",\"Inva uniform dimension type for matrix \"+name+\": new i(\"\",\"Unkn uniform data type for \"+name+\": \"+r)}var new i(\"\",\"Inva data new data type for vector \"+name+\": c(e){for(v n=[\"return function n=[];for(v i in r){var new i(\"\",\"Inva data new i(\"\",\"Inva uniform dimension type for matrix \"+name+\": \"+t);retur o(r*r,0)}t new i(\"\",\"Unkn uniform data type for \"+name+\": p}function f(t){var r=0;r1){l[ c=1;c1)for l=0;l 0 U ||b|| > 0.\\n // Assign z = 0, x = -b, y = a:\\n // a*-b + b*a + c*0 = -ba + ba + 0 = 0\\n if (v.x*v.x > v.z*v.z || v.y*v.y > v.z*v.z) {\\n return v.x, 0.0));\\n } else {\\n return v.z, -v.y));\\n }\\n}\\n\\n// Calculate the tube vertex and normal at the given index.\\n// The returned vertex is for a tube ring with its center at origin, radius of length(d), pointing in the direction of d.\\n//\\n// Each tube segment is made up of a ring of vertices.\\ These vertices are used to make up the triangles of the tube by connecting them together in the vertex array.\\n// The indexes of tube segments run from 0 to 8.\\n//\\nve d, float index, out vec3 normal) {\\n float segmentCou = 8.0;\\n\\n float angle = 2.0 * 3.14159 * (index / vec3 u = vec3 v = d));\\n\\n vec3 x = u * cos(angle) * length(d); vec3 y = v * sin(angle) * length(d); vec3 v3 = x + y;\\n\\n normal = return vec4 vec4 color, vec2 uv;\\nunifo float float mat4 model\\n , view\\n , projection , vec3 eyePositio , vec3 f_normal\\n , , , f_data\\n , vec4 vec2 f_uv;\\n\\nv main() {\\n // Scale the vector magnitude to stay constant with\\n // model & view changes.\\n vec3 normal;\\n vec3 XYZ = * (tubeScale * vector.w * position.w normal);\\n vec4 tubePositi = model * 1.0) + vec4(XYZ, 0.0);\\n\\n //Lighting geometry parameters vec4 = view * /= = lightPosit - f_eyeDirec = eyePositio - f_normal = * // vec4 m_position = model * 1.0);\\n vec4 t_position = view * gl_Positio = projection * f_color = color;\\n f_data = f_position = f_uv = : highp GLSLIFY 1\\n\\nfloat x, float roughness) {\\n float NdotH = max(x, 0.0001);\\n float cos2Alpha = NdotH * NdotH;\\n float tan2Alpha = (cos2Alpha - 1.0) / cos2Alpha; float roughness2 = roughness * roughness; float denom = * roughness2 * cos2Alpha * cos2Alpha; return exp(tan2Al / roughness2 / vec3 vec3 vec3 float roughness, float fresnel) {\\n\\n float VdotN = 0.0);\\n float LdotN = 0.0);\\n\\n //Half angle vector\\n vec3 H = + //Geometri term\\n float NdotH = H), 0.0);\\n float VdotH = H), 0.000001); float LdotH = H), 0.000001); float G1 = (2.0 * NdotH * VdotN) / VdotH;\\n float G2 = (2.0 * NdotH * LdotN) / LdotH;\\n float G = min(1.0, min(G1, G2));\\n \\n //Distribu term\\n float D = //Fresnel term\\n float F = pow(1.0 - VdotN, fresnel);\\ //Multiply terms and done\\n return G * F * D / max(3.1415 * VdotN, a, float b, float p) {\\n return ((p > max(a, b)) || \\n (p 0 U ||b|| > 0.\\n // Assign z = 0, x = -b, y = a:\\n // a*-b + b*a + c*0 = -ba + ba + 0 = 0\\n if (v.x*v.x > v.z*v.z || v.y*v.y > v.z*v.z) {\\n return v.x, 0.0));\\n } else {\\n return v.z, -v.y));\\n }\\n}\\n\\n// Calculate the tube vertex and normal at the given index.\\n// The returned vertex is for a tube ring with its center at origin, radius of length(d), pointing in the direction of d.\\n//\\n// Each tube segment is made up of a ring of vertices.\\ These vertices are used to make up the triangles of the tube by connecting them together in the vertex array.\\n// The indexes of tube segments run from 0 to 8.\\n//\\nve d, float index, out vec3 normal) {\\n float segmentCou = 8.0;\\n\\n float angle = 2.0 * 3.14159 * (index / vec3 u = vec3 v = d));\\n\\n vec3 x = u * cos(angle) * length(d); vec3 y = v * sin(angle) * length(d); vec3 v3 = x + y;\\n\\n normal = return vec4 vec4 vec4 id;\\n\\nuni mat4 model, view, float vec3 vec4 f_id;\\n\\nv main() {\\n vec3 normal;\\n vec3 XYZ = * (tubeScale * vector.w * position.w normal);\\n vec4 tubePositi = model * 1.0) + vec4(XYZ, 0.0);\\n\\n gl_Positio = projection * view * f_id = id;\\n f_position = highp GLSLIFY 1\\n\\nbool a, float b, float p) {\\n return ((p > max(a, b)) || \\n (p null;var strict\";va r-1}return n.create() t-e});for( max(a, b)) || \\n (p 0.0) ||\\n clipBounds discard;\\n vec3 N = vec3 V = vec3 L = {\\n N = -N;\\n }\\n\\n float specular = V, N, roughness) 0.);\\n float diffuse = min(kambie + kdiffuse * max(dot(N, L), 0.0), 1.0);\\n\\n //decide how to interpolat color \\u2014 in vertex or in fragment\\n vec4 surfaceCol =\\n .5) * vec2(value value)) +\\n step(.5, vertexColo * vColor;\\n\\ vec4 litColor = surfaceCol * vec4(diffu * + kspecular * vec3(1,1,1 * specular, 1.0);\\n\\n gl_FragCol = mix(litCol contourCol contourTin * highp GLSLIFY 1\\n\\nattri vec4 uv;\\nattri float f;\\n\\nunif vec3 mat3 mat4 model, view, float height, sampler2D float value, kill;\\nvar vec3 vec2 vec3 eyeDirecti vec4 main() {\\n vec3 dataCoordi = permutatio * vec3(uv.xy height);\\n = objectOffs + vec4 worldPosit = model * 1.0);\\n\\n vec4 clipPositi = projection * view * clipPositi += zOffset;\\n gl_Positio = value = f + kill = -1.0;\\n = uv.zw;\\n\\n vColor = vec2(value value));\\n //Don't do lighting for contours\\n surfaceNor = vec3(1,0,0 eyeDirecti = vec3(0,1,0 lightDirec = highp GLSLIFY 1\\n\\nbool a, float b, float p) {\\n return ((p > max(a, b)) || \\n (p 0.0) ||\\n clipBounds discard;\\n vec2 ux = / shape.x);\\ vec2 uy = / shape.y);\\ gl_FragCol = vec4(pickI ux.x, uy.x, ux.y + v=new k in O(t,e){var new invalid coordinate for kt=0;kt halfCharSt + halfCharWi 2){for(var n)return strict\";va new Invalid texture size\");var new Invalid shape for new Invalid shape for pixel new new Invalid arguments for texture2d instanceof instanceof instanceof ImageData& instanceof ImageData} f(t,e,r){v new Invalid texture size\");ret d(t,e){ret g(t){var new Invalid texture new Floating point textures not supported on this platform\") o=g(t);ret new Invalid ndarray, must be 2d or 3d\");var new Invalid shape for new Invalid shape for pixel new Incompatib texture format for new Error(\"gl- Too many vertex e=new t=new e=new t=new i=new n=new new Error(\"Mus have at least d+1 points\");v orient\");v i=new strict\";va new x(null);re new x(y(t))};v c(t,e){var u(t,e){var f(t,e){var i}}functio d(t,e){for r;return n;return strict\";va e,r,n;func a=\"var sharedChun = {}; r=i;else e=i}return n(t){retur i(t,e){ret a=o;functi new l=c;functi u(t,e,r,n) i=new new t instanceof p(t){for(v k(t){for(v 0,o=void of the original icon Sans Unicode MS t.kind}var i(t){retur null;var 1, 2, or 3 arguments, but found instead.\") i||!(i in ct))return e.error('T item type argument of \"array\" must be one of string, number, rbga value expected an array containing either three or four numeric new new ot(r||\"Cou not parse color from value r=!0;retur expression \"'+r+'\". If you wanted a literal array, use [\"literal\" new ot(\"Input is not a void an array with at least one element. If you wanted a literal array, use [\"literal\" []].');var r)return name must be a string, but found \"+typeof r+' instead. If you wanted a literal array, use [\"literal\" ut(a,i));e null}else instanceof at)&&funct t(e){if(e instanceof yt)return instanceof instanceof r=e instanceof ht||e instanceof lt||e instanceof ut,n=!0;re instanceof s=new dt;try{i=n i}return expression \"'+r+'\". If you wanted a literal array, use [\"literal\" void value invalid. Use null objects invalid. Use [\"literal\" {...}] an array, but found \"+typeof t+\" new pairs for \"step\" expression must be arranged with input values in strictly ascending order.',c) new t};var new e.error(\"C bezier interpolat requires four numeric arguments with values between 0 and pairs for \"interpola expression must be arranged with input values in strictly ascending order.',h) l.N?new \"+$(l)+\" is not new ot(\"Array index out of bounds: \"+e+\" > new ot(\"Array index must be an integer, but found \"+e+\" labels must be integers no larger than branch labels must be integer null}else labels must be null;var g?new Ut(t,e){va r=e[0];thr new instanceof r=e[0];ret typeof i==typeof typeof n==typeof typeof i==typeof typeof n==typeof e[0].value in r=e[0];ret ne(t){retu ie(t){retu me(t,e,r){ n=void 0,t);if(vo 0!==r&&voi 0!==n)retu _e(t){retu t[0]&&t[0] Rt}functio we(t,e){va r=new n?Gt(new in new ot(\"Expect value to be one of \")+\", but found instanceof t;var Yt([new N(\"\",\"prop expression not Yt([new N(\"\",\"zoom expression not a=function t(e){var r=null;if( instanceof if(e instanceof Mt)for(var D(e,r,r+\" is greater than the maximum value ze(t){var function may not have a \"stops\" must have at least one required property required property functions not functions not functions not property is f(t){var D(s,a,\"arr expected, \"+fe(a)+\" D(s,a,\"arr length 2 expected, length \"+a.length D(s,a,\"obj expected, \"+fe(a[0]) D(s,a,\"obj stop key must have D(s,a,\"obj stop key must have zoom values must appear in ascending h(t,n){var D(t.key,c, stop domain type must match previous stop domain type \"+e)]}else domain value must be a number, string, or u=\"number expected, \"+s+\" found\";ret you intended to use a categorica function, specify `\"type\": typeof t!=typeof He(t){retu t(e){var D(n,r,\"arr expected, \"+fe(r)+\" found\")];v D(n,r,'\"$t cannot be use with operator D(n,r,'fil array for operator \"'+r[0]+'\" must have 3 expected, \"+i+\" instanceof new Error(\"can serialize object of type \"+typeof t)}functio t||t instanceof Boolean||t instanceof Number||t instanceof String||t instanceof Date||t instanceof RegExp||t instanceof instanceof fr)return t){var new Error(\"can deserializ object of anonymous class\");va new Error(\"can deserializ unregister class a}throw new Error(\"can deserializ object of type \"+typeof t)}var 1;var t};var e in zr(r,void e(e,r){for n in i in Xr(t,e){vo Zr(t,e){re new must be implemente by each concrete StructArra layout\")}; n=2*r;retu a=4*i;retu s=6*o;retu c=8*l;retu i=3*n;retu r=1*e;retu s=6*o;retu n=4*r;retu r=1*e;retu i=3*n;retu i=3*n;retu n=2*r;retu n=2*r;retu a=4*i;retu new new new new new new vertices per segment is bucket requested exceeds allowed extent, reduce your vector tile buffer size\")}ret r}function Qn={paint: t=new t=new t=new t=new new e=t;return new of range source coordinate for image new of range destinatio coordinate for image copy\");for t;e||(e=t) e}function ki(t){var Ai(t,e,r){ n=t;do{var n}function o=t;do{for Si(t,e){re r}(t,e)){v Oi(t,e){re Ri(t,e){re Wi(t,e){va na(t){retu new Error(\"unk command if(7!==r)t new Error(\"unk command u(t){for(v n=new new Error(\"fea index out of new Mn};functi c=0;cc){va Ta=new r=new new r=[],n=new Da(t,e,r){ new Fa(t,e){va null;var e=new i in t){var oo(t){retu lo(t,e,r){ new type: wo=3;funct if(void if(void e(t,e,n){v r(t,e,r){v new of range source coordinate for DEM new tiles must be _('\"'+e+'\" is not a valid encoding type. Valid types include \"mapbox\" and a=n||i;ret r=[];for(v n in t)n in f(t,e){ret h(e,r,n){v new best %d after %d s=new many glyphs being rendered in a tile. See r=new a in e){var l in o){var _(e,r){for n=new t.id})))}} e=new x(c),r=new n in f){var a=f[n];a instanceof k(e,r){var r(o);var r[n],e()}; e[r]};var S(t){var $(t,e){for lt(t,e){va e,r,n}func ut(t){var ft(t){retu ht(t){var r in t}function dt(t){retu t.x}functi gt(t){retu t.y}functi Et(t){var e=[];retur o}function new Error(\"max should be in the 0-24 range\");va %d clusters in z%d-%d-%d (features: %d, points: %d, simplified null;var down to parent tile i)return e(new Error(\"Inp data is not a valid GeoJSON new e.data)ret r(new Error(\"Inp data is not a valid GeoJSON r(new Error(\"Inp data is not a valid GeoJSON new Error('Wor source with name \"'+t+'\" already new Error(\"RTL text plugin already Error(\"RTL Text Plugin failed to import scripts from self&&self instanceof t,e,r=new void new Error(\"fai to create canvas 2d null;for(v y(t,e){var new Error(\"An API access token is required to use Mapbox GL. new Error(\"Use a public access token (pk.*) with Mapbox GL, not a secret access token (sk.*). \"+m);retur x(t){retur t;var r=A(t);ret i=A(t);ret t;var Error(\"gly > 65535 not r in out of lat: }, or an array of [, ]\")};var this._ne=t instanceof G?new this._sw=t instanceof G?new instanceof instanceof Y))return this}retur new new new instanceof Y?t:new Y(t)};var r=this;ret r.fire(new r=this,n=v in this;var this};var n=!1;for(v i in e=(t-(void n={};for(v i in o in s in new in in n||(r=new r=this;t in t in Xt(e,r){va new $t(){retur new Qt(e,r){va n={};for(v i in in O}var T=new _e=new a in l in i){var u=new 4294967295 l in s){var if(i&&o){v l in i){var if(r)for(v i in new \")+\".\");re this.fire( Error(\"An image with this name already this.fire( Error(\"No image with this name new Error(\"The is already a source with this new Error(\"The type property must be defined, but the only the following properties were given: new Error(\"The is no source with this ID\");for(v r in this.fire( Error('Sou \"'+e+'\" cannot be removed while layer \"'+r+'\" is using it.')));va Error('Lay with id \"'+i+'\" already exists on this map')));el Error('Lay with id \"'+r+'\" does not exist on this Error('Lay with id \"'+r+'\" does not exist on this this.fire( Error(\"The layer '\"+e+\"' does not exist in the map's style and cannot be this.fire( Error(\"The layer '\"+e+\"' does not exist in the map's style and cannot be Error(\"The layer '\"+e+\"' does not exist in the map's style and cannot have zoom 0,void this.fire( Error(\"The layer '\"+e+\"' does not exist in the map's style and cannot be Error(\"The layer '\"+e+\"' does not exist in the map's style and cannot be this.fire( Error(\"The layer '\"+e+\"' does not exist in the map's style and cannot be e=this;ret void 0.5) {\\n gl_FragCol = vec4(0.0, 0.0, 1.0, 0.5) * alpha;\\n }\\n\\n if (v_notUsed > 0.5) {\\n // This box not used, fade it out\\n gl_FragCol *= .1;\\n vec2 vec2 vec2 vec2 mat4 vec2 float float float main() {\\n vec4 projectedP = u_matrix * 0, 1);\\n highp float = highp float = clamp(\\n 0.5 + 0.5 * / 0.0, // Prevents oversized near-field boxes in tiles\\n 4.0);\\n\\n gl_Positio = u_matrix * vec4(a_pos 0.0, 1.0);\\n gl_Positio += a_extrude * * gl_Positio * v_placed = a_placed.x v_notUsed = float float float float vec2 vec2 main() {\\n float alpha = 0.5;\\n\\n // Red = collision, hide label\\n vec4 color = vec4(1.0, 0.0, 0.0, 1.0) * alpha;\\n\\n // Blue = no collision, label is showing\\n if (v_placed > 0.5) {\\n color = vec4(0.0, 0.0, 1.0, 0.5) * alpha;\\n }\\n\\n if (v_notUsed > 0.5) {\\n // This box not used, fade it out\\n color *= .2;\\n }\\n\\n float = float extrude_le = * float stroke_wid = 15.0 * / float radius = v_radius * float = - radius);\\n float opacity_t = 0.0, gl_FragCol = opacity_t * vec2 vec2 vec2 vec2 mat4 vec2 float float float float vec2 vec2 main() {\\n vec4 projectedP = u_matrix * 0, 1);\\n highp float = highp float = clamp(\\n 0.5 + 0.5 * / 0.0, // Prevents oversized near-field circles in tiles\\n 4.0);\\n\\n gl_Positio = u_matrix * vec4(a_pos 0.0, 1.0);\\n\\n highp float padding_fa = 1.2; // Pad the vertices slightly to make room for anti-alias blur\\n gl_Positio += a_extrude * * padding_fa * gl_Positio * v_placed = a_placed.x v_notUsed = a_placed.y v_radius = // We don't pitch the circles, so both units of the extrusion vector are equal in magnitude to the radius\\n\\n v_extrude = a_extrude * = * * highp vec4 main() {\\n gl_FragCol = vec2 mat4 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, mapbox: define highp vec4 color\\n#pr mapbox: define lowp float main() {\\n #pragma mapbox: initialize highp vec4 color\\n #pragma mapbox: initialize lowp float opacity\\n\\ gl_FragCol = color * gl_FragCol = vec2 mat4 mapbox: define highp vec4 color\\n#pr mapbox: define lowp float main() {\\n #pragma mapbox: initialize highp vec4 color\\n #pragma mapbox: initialize lowp float opacity\\n\\ gl_Positio = u_matrix * vec4(a_pos 0, mapbox: define highp vec4 mapbox: define lowp float vec2 v_pos;\\n\\n main() {\\n #pragma mapbox: initialize highp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ float dist = length(v_p - float alpha = 1.0 - 1.0, dist);\\n gl_FragCol = outline_co * (alpha * gl_FragCol = vec2 mat4 vec2 vec2 mapbox: define highp vec4 mapbox: define lowp float main() {\\n #pragma mapbox: initialize highp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n v_pos = / gl_Positio + 1.0) / 2.0 * vec2 vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 vec2 mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp float opacity\\n\\ vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = / u_texsize, u_pattern_ / u_texsize, imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = / u_texsize, u_pattern_ / u_texsize, vec4 color2 = pos2);\\n\\n // find distance to outline for alpha float dist = length(v_p - float alpha = 1.0 - 1.0, dist);\\n\\n gl_FragCol = mix(color1 color2, u_mix) * alpha * gl_FragCol = mat4 vec2 vec2 vec2 vec2 vec2 float float float vec2 vec2 vec2 vec2 mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp float opacity\\n\\ gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n\\n v_pos_a = u_scale_a * a_pos);\\n v_pos_b = u_scale_b * a_pos);\\n\\ v_pos = / gl_Positio + 1.0) / 2.0 * vec2 vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp float opacity\\n\\ vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = / u_texsize, u_pattern_ / u_texsize, imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = / u_texsize, u_pattern_ / u_texsize, vec4 color2 = pos2);\\n\\n gl_FragCol = mix(color1 color2, u_mix) * gl_FragCol = mat4 vec2 vec2 vec2 vec2 float float float vec2 vec2 vec2 mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp float opacity\\n\\ gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n\\n v_pos_a = u_scale_a * a_pos);\\n v_pos_b = u_scale_b * vec4 mapbox: define lowp float base\\n#pra mapbox: define lowp float mapbox: define highp vec4 color\\n\\nv main() {\\n #pragma mapbox: initialize lowp float base\\n #pragma mapbox: initialize lowp float height\\n #pragma mapbox: initialize highp vec4 color\\n\\n gl_FragCol = gl_FragCol = mat4 vec3 lowp vec3 lowp float vec2 vec4 vec4 mapbox: define lowp float base\\n#pra mapbox: define lowp float mapbox: define highp vec4 color\\n\\nv main() {\\n #pragma mapbox: initialize lowp float base\\n #pragma mapbox: initialize lowp float height\\n #pragma mapbox: initialize highp vec4 color\\n\\n vec3 normal = base = max(0.0, base);\\n height = max(0.0, height);\\n float t = mod(normal 2.0);\\n\\n gl_Positio = u_matrix * vec4(a_pos t > 0.0 ? height : base, 1);\\n\\n // Relative luminance (how dark/brigh is the surface color?)\\n float colorvalue = color.r * 0.2126 + color.g * 0.7152 + color.b * 0.0722;\\n\\ v_color = vec4(0.0, 0.0, 0.0, 1.0);\\n\\n // Add slight ambient lighting so no extrusions are totally black\\n vec4 ambientlig = vec4(0.03, 0.03, 0.03, 1.0);\\n color += // Calculate cos(theta) where theta is the angle between surface normal and diffuse light ray\\n float directiona = / 16384.0, u_lightpos 0.0, 1.0);\\n\\n // Adjust directiona so that\\n // the range of values for is narrower\\n // with lower light intensity\\ // and with surface colors\\n directiona = mix((1.0 - max((1.0 - colorvalue + 1.0), // Add gradient along z axis of side surfaces\\n if (normal.y != 0.0) {\\n directiona *= clamp((t + base) * pow(height / 150.0, 0.5), mix(0.7, 0.98, 1.0 - 1.0);\\n }\\n\\n // Assign final color based on surface + ambient light color, diffuse light directiona and light color\\n // with lower bounds adjusted to hue of light\\n // so that shading is tinted with the complement (opposite) color to the light color\\n v_color.r += clamp(colo * directiona * mix(0.0, 0.3, 1.0 - 1.0);\\n v_color.g += clamp(colo * directiona * mix(0.0, 0.3, 1.0 - 1.0);\\n v_color.b += clamp(colo * directiona * mix(0.0, 0.3, 1.0 - vec2 vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 vec4 mapbox: define lowp float base\\n#pra mapbox: define lowp float height\\n\\n main() {\\n #pragma mapbox: initialize lowp float base\\n #pragma mapbox: initialize lowp float height\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = / u_texsize, u_pattern_ / u_texsize, imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = / u_texsize, u_pattern_ / u_texsize, vec4 color2 = pos2);\\n\\n vec4 mixedColor = mix(color1 color2, u_mix);\\n\\ gl_FragCol = mixedColor * gl_FragCol = mat4 vec2 vec2 vec2 vec2 float float float float vec3 lowp vec3 lowp float vec2 vec4 vec2 vec2 vec4 float mapbox: define lowp float base\\n#pra mapbox: define lowp float height\\n\\n main() {\\n #pragma mapbox: initialize lowp float base\\n #pragma mapbox: initialize lowp float height\\n\\n vec3 normal = float edgedistan = base = max(0.0, base);\\n height = max(0.0, height);\\n float t = mod(normal 2.0);\\n float z = t > 0.0 ? height : base;\\n\\n gl_Positio = u_matrix * vec4(a_pos z, 1);\\n\\n vec2 pos = normal.x == 1.0 && normal.y == 0.0 && normal.z == 16384.0\\n ? a_pos // extrusion top\\n : z * // extrusion side\\n\\n v_pos_a = u_scale_a * pos);\\n v_pos_b = u_scale_b * pos);\\n\\n v_lighting = vec4(0.0, 0.0, 0.0, 1.0);\\n float directiona = / 16383.0, u_lightpos 0.0, 1.0);\\n directiona = mix((1.0 - max((0.5 + 1.0), if (normal.y != 0.0) {\\n directiona *= clamp((t + base) * pow(height / 150.0, 0.5), mix(0.7, 0.98, 1.0 - 1.0);\\n }\\n\\n v_lighting += * u_lightcol mix(vec3(0 vec3(0.3), 1.0 - u_lightcol sampler2D float vec2 v_pos;\\n\\n main() {\\n gl_FragCol = v_pos) * gl_FragCol = mat4 vec2 vec2 vec2 v_pos;\\n\\n main() {\\n gl_Positio = u_matrix * vec4(a_pos * u_world, 0, 1);\\n\\n v_pos.x = a_pos.x;\\n v_pos.y = 1.0 - highp sampler2D vec2 vec2 float float coord, float bias) {\\n // Convert encoded elevation value to meters\\n vec4 data = coord) * 255.0;\\n return (data.r + data.g * 256.0 + data.b * 256.0 * 256.0) / main() {\\n vec2 epsilon = 1.0 / // queried pixels:\\n // // | | | |\\n // | a | b | c |\\n // | | | |\\n // // | | | |\\n // | d | e | f |\\n // | | | |\\n // // | | | |\\n // | g | h | i |\\n // | | | |\\n // float a = + -epsilon.y 0.0);\\n float b = + vec2(0, -epsilon.y 0.0);\\n float c = + -epsilon.y 0.0);\\n float d = + 0), 0.0);\\n float e = 0.0);\\n float f = + 0), 0.0);\\n float g = + epsilon.y) 0.0);\\n float h = + vec2(0, epsilon.y) 0.0);\\n float i = + epsilon.y) 0.0);\\n\\n // here we divide the x and y slopes by 8 * pixel size\\n // where pixel size (aka meters/pix is:\\n // circumfere of the world / (pixels per tile * number of tiles)\\n // which is equivalent to: 8 * / (512 * pow(2, u_zoom))\\n // which can be reduced to: pow(2, 19.2561997 - u_zoom)\\n // we want to vertically exaggerate the hillshadin though, because otherwise\\ // it is barely noticeable at low zooms. to do this, we multiply this by some\\n // scale factor pow(2, (u_zoom - u_maxzoom) * a) where a is an arbitrary value\\n // Here we use a=0.3 which works out to the expression below. see \\n // nickidluga awesome breakdown for more info\\n // float exaggerati = u_zoom 0.0 ? 1.0 : -1.0);\\n\\n float intensity = u_light.x; // We add PI to make this property match the global light object, which adds PI/2 to the light's azimuthal\\ // position property to account for 0deg correspond to north/the top of the viewport in the style spec\\n // and the original shader was written to accept - 90) as the azimuthal. float azimuth = u_light.y + PI;\\n\\n // We scale the slope exponentia based on intensity, using a calculatio similar to\\n // the exponentia interpolat function in the style spec:\\n // // so that higher intensity values create more opaque hillshadin float base = 1.875 - intensity * 1.75;\\n float maxValue = 0.5 * PI;\\n float scaledSlop = intensity != 0.5 ? ((pow(base slope) - 1.0) / (pow(base, maxValue) - 1.0)) * maxValue : slope;\\n\\n // The accent color is calculated with the cosine of the slope while the shade color is calculated with the sine\\n // so that the accent color's rate of change eases in while the shade color's eases out.\\n float accent = // We multiply both the accent and shade color by a clamped intensity value\\n // so that intensitie >= 0.5 do not additional affect the color values\\n // while intensity values 0.0 ? ANTIALIASI : 0.0);\\n float outset = gapwidth + halfwidth * (gapwidth > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset2 = offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n vec4 = u_matrix * vec4(dist / u_ratio, 0.0, 0.0);\\n gl_Positio = u_matrix * vec4(pos + offset2 / u_ratio, 0.0, 1.0) + // calculate how much the perspectiv view squishes or stretches the extrude\\n float = float = / gl_Positio * v_gamma_sc = / v_width2 = vec2(outse mapbox: define lowp float blur\\n#pra mapbox: define lowp float sampler2D vec2 vec2 float highp float main() {\\n #pragma mapbox: initialize lowp float blur\\n #pragma mapbox: initialize lowp float opacity\\n\\ // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line (v_width2. or when fading out\\n // (v_width2. float blur2 = (blur + 1.0 / * float alpha = clamp(min( - (v_width2. - blur2), v_width2.s - dist) / blur2, 0.0, 1.0);\\n\\n // For gradient lines, v_lineprog is the ratio along the entire line,\\n // scaled to [0, 2^15), and the gradient ramp is stored in a texture.\\n vec4 color = 0.5));\\n\\n gl_FragCol = color * (alpha * gl_FragCol = the attribute conveying progress along a line is scaled to [0, 2^15)\\n#de 32767.0\\n\\ the distance over which the line edge fades out.\\n// Retina devices need a smaller distance to avoid ANTIALIASI 1.0 / / 2.0\\n\\n// floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale vec4 vec4 mat4 mediump float vec2 vec2 vec2 float highp float mapbox: define lowp float blur\\n#pra mapbox: define lowp float mapbox: define mediump float mapbox: define lowp float mapbox: define mediump float width\\n\\nv main() {\\n #pragma mapbox: initialize lowp float blur\\n #pragma mapbox: initialize lowp float opacity\\n #pragma mapbox: initialize mediump float gapwidth\\n #pragma mapbox: initialize lowp float offset\\n #pragma mapbox: initialize mediump float width\\n\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n\\n v_lineprog = / 4.0) + a_data.w * 64.0) * 2.0 / vec2 pos = // x is 1 if it's a round cap, 0 otherwise\\ // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = v_normal = normal;\\n\\ // these used to be applied in the JS and native code bases.\\n // moved them into the shader for clarity and simplicity gapwidth = gapwidth / 2.0;\\n float halfwidth = width / 2.0;\\n offset = -1.0 * offset;\\n\\ float inset = gapwidth + (gapwidth > 0.0 ? ANTIALIASI : 0.0);\\n float outset = gapwidth + halfwidth * (gapwidth > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset2 = offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n vec4 = u_matrix * vec4(dist / u_ratio, 0.0, 0.0);\\n gl_Positio = u_matrix * vec4(pos + offset2 / u_ratio, 0.0, 1.0) + // calculate how much the perspectiv view squishes or stretches the extrude\\n float = float = / gl_Positio * v_gamma_sc = / v_width2 = vec2(outse vec2 vec2 vec2 vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 float float mapbox: define lowp float blur\\n#pra mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp float blur\\n #pragma mapbox: initialize lowp float opacity\\n\\ // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line (v_width2. or when fading out\\n // (v_width2. float blur2 = (blur + 1.0 / * float alpha = clamp(min( - (v_width2. - blur2), v_width2.s - dist) / blur2, 0.0, 1.0);\\n\\n float x_a = / 1.0);\\n float x_b = / 1.0);\\n\\n // v_normal.y is 0 at the midpoint of the line, -1 at the lower edge, 1 at the upper edge\\n // we clamp the line width outset to be between 0 and half the pattern height plus padding (2.0)\\n // to ensure we don't sample outside the designated symbol on the sprite sheet.\\n // 0.5 is added to shift the component to be bounded between 0 and 1 for interpolat of\\n // the texture coordinate float y_a = 0.5 + (v_normal. * 0.0, + 2.0) / 2.0) / float y_b = 0.5 + (v_normal. * 0.0, + 2.0) / 2.0) / vec2 pos_a = / u_texsize, u_pattern_ / u_texsize, vec2(x_a, y_a));\\n vec2 pos_b = / u_texsize, u_pattern_ / u_texsize, vec2(x_b, y_b));\\n\\n vec4 color = pos_a), pos_b), u_fade);\\n gl_FragCol = color * alpha * gl_FragCol = floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the 2.0\\n\\n// the distance over which the line edge fades out.\\n// Retina devices need a smaller distance to avoid ANTIALIASI 1.0 / / vec4 vec4 mat4 mediump float vec2 vec2 vec2 float float mapbox: define lowp float blur\\n#pra mapbox: define lowp float mapbox: define lowp float mapbox: define mediump float mapbox: define mediump float width\\n\\nv main() {\\n #pragma mapbox: initialize lowp float blur\\n #pragma mapbox: initialize lowp float opacity\\n #pragma mapbox: initialize lowp float offset\\n #pragma mapbox: initialize mediump float gapwidth\\n #pragma mapbox: initialize mediump float width\\n\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * vec2 pos = // x is 1 if it's a round cap, 0 otherwise\\ // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = v_normal = normal;\\n\\ // these used to be applied in the JS and native code bases.\\n // moved them into the shader for clarity and simplicity gapwidth = gapwidth / 2.0;\\n float halfwidth = width / 2.0;\\n offset = -1.0 * offset;\\n\\ float inset = gapwidth + (gapwidth > 0.0 ? ANTIALIASI : 0.0);\\n float outset = gapwidth + halfwidth * (gapwidth > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset2 = offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n vec4 = u_matrix * vec4(dist / u_ratio, 0.0, 0.0);\\n gl_Positio = u_matrix * vec4(pos + offset2 / u_ratio, 0.0, 1.0) + // calculate how much the perspectiv view squishes or stretches the extrude\\n float = float = / gl_Positio * v_gamma_sc = / v_linesofa = a_linesofa v_width2 = vec2(outse sampler2D float float vec2 vec2 vec2 vec2 float mapbox: define highp vec4 color\\n#pr mapbox: define lowp float blur\\n#pra mapbox: define lowp float mapbox: define mediump float width\\n#pr mapbox: define lowp float main() {\\n #pragma mapbox: initialize highp vec4 color\\n #pragma mapbox: initialize lowp float blur\\n #pragma mapbox: initialize lowp float opacity\\n #pragma mapbox: initialize mediump float width\\n #pragma mapbox: initialize lowp float floorwidth // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line (v_width2. or when fading out\\n // (v_width2. float blur2 = (blur + 1.0 / * float alpha = clamp(min( - (v_width2. - blur2), v_width2.s - dist) / blur2, 0.0, 1.0);\\n\\n float sdfdist_a = v_tex_a).a float sdfdist_b = v_tex_b).a float sdfdist = mix(sdfdis sdfdist_b, u_mix);\\n alpha *= smoothstep - u_sdfgamma / floorwidth 0.5 + u_sdfgamma / floorwidth sdfdist);\\ gl_FragCol = color * (alpha * gl_FragCol = floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the 2.0\\n\\n// the distance over which the line edge fades out.\\n// Retina devices need a smaller distance to avoid ANTIALIASI 1.0 / / vec4 vec4 mat4 mediump float vec2 float vec2 float vec2 vec2 vec2 vec2 vec2 float mapbox: define highp vec4 color\\n#pr mapbox: define lowp float blur\\n#pra mapbox: define lowp float mapbox: define mediump float mapbox: define lowp float mapbox: define mediump float width\\n#pr mapbox: define lowp float main() {\\n #pragma mapbox: initialize highp vec4 color\\n #pragma mapbox: initialize lowp float blur\\n #pragma mapbox: initialize lowp float opacity\\n #pragma mapbox: initialize mediump float gapwidth\\n #pragma mapbox: initialize lowp float offset\\n #pragma mapbox: initialize mediump float width\\n #pragma mapbox: initialize lowp float floorwidth vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * vec2 pos = // x is 1 if it's a round cap, 0 otherwise\\ // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = v_normal = normal;\\n\\ // these used to be applied in the JS and native code bases.\\n // moved them into the shader for clarity and simplicity gapwidth = gapwidth / 2.0;\\n float halfwidth = width / 2.0;\\n offset = -1.0 * offset;\\n\\ float inset = gapwidth + (gapwidth > 0.0 ? ANTIALIASI : 0.0);\\n float outset = gapwidth + halfwidth * (gapwidth > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist =outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset2 = offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n vec4 = u_matrix * vec4(dist / u_ratio, 0.0, 0.0);\\n gl_Positio = u_matrix * vec4(pos + offset2 / u_ratio, 0.0, 1.0) + // calculate how much the perspectiv view squishes or stretches the extrude\\n float = float = / gl_Positio * v_gamma_sc = / v_tex_a = * / floorwidth normal.y * + u_tex_y_a) v_tex_b = * / floorwidth normal.y * + v_width2 = vec2(outse float float sampler2D sampler2D vec2 vec2 float float float float vec3 main() {\\n\\n // read and cross-fade colors from the main and parent tiles\\n vec4 color0 = v_pos0);\\n vec4 color1 = v_pos1);\\n if (color0.a > 0.0) {\\n color0.rgb = color0.rgb / color0.a;\\ }\\n if (color1.a > 0.0) {\\n color1.rgb = color1.rgb / color1.a;\\ }\\n vec4 color = mix(color0 color1, u_fade_t); color.a *= u_opacity; vec3 rgb = color.rgb; // spin\\n rgb = vec3(\\n dot(rgb, dot(rgb, dot(rgb, // saturation float average = (color.r + color.g + color.b) / 3.0;\\n rgb += (average - rgb) * // contrast\\n rgb = (rgb - 0.5) * + 0.5;\\n\\n // brightness vec3 u_high_vec = vec3 u_low_vec = gl_FragCol = u_low_vec, rgb) * color.a, gl_FragCol = mat4 vec2 float float vec2 vec2 vec2 vec2 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n // We are using Int16 for texture position coordinate to give us enough precision for\\n // fractional coordinate We use 8192 to scale the texture coordinate in the buffer\\n // as an arbitraril high number to preserve adequate precision when rendering. // This is also the same value as the EXTENT we are using for our tile buffer pos coordinate // so math for modifying either is consistent v_pos0 = / 8192.0) - 0.5) / u_buffer_s ) + 0.5;\\n v_pos1 = (v_pos0 * + sampler2D mapbox: define lowp float vec2 float main() {\\n #pragma mapbox: initialize lowp float opacity\\n\\ lowp float alpha = opacity * gl_FragCol = v_tex) * gl_FragCol = float PI = vec4 vec4 vec3 float bool bool highp float u_size_t; // used to interpolat between zoom stops when size is a composite highp float u_size; // used when size is both zoom and feature highp float highp float bool highp float float mapbox: define lowp float mat4 mat4 mat4 bool bool vec2 vec2 float main() {\\n #pragma mapbox: initialize lowp float opacity\\n\\ vec2 a_pos = vec2 a_offset = vec2 a_tex = a_data.xy; vec2 a_size = a_data.zw; highp float segment_an = float size;\\n if && {\\n size = mix(a_size a_size[1], u_size_t) / 10.0;\\n } else if && {\\n size = a_size[0] / 10.0;\\n } else if && {\\n size = u_size;\\n } else {\\n size = u_size;\\n }\\n\\n vec4 projectedP = u_matrix * vec4(a_pos 0, 1);\\n highp float = // See comments in highp float distance_r = ?\\n / :\\n / highp float = clamp(\\n 0.5 + 0.5 * 0.0, // Prevents oversized near-field symbols in tiles\\n 4.0);\\n\\n size *= float fontScale = u_is_text ? size / 24.0 : size;\\n\\n highp float = 0.0;\\n if {\\n // See comments in vec4 = u_matrix * vec4(a_pos + vec2(1, 0), 0, 1);\\n\\n vec2 a = / vec2 b = / = atan((b.y - a.y) / b.x - a.x);\\n }\\n\\n highp float angle_sin = + highp float angle_cos = + mat2 = -1.0 * angle_sin, angle_sin, vec4 projected_ = * 0.0, 1.0);\\n gl_Positio = * / + * (a_offset / 32.0 * fontScale) 0.0, 1.0);\\n\\n v_tex = a_tex / u_texsize; vec2 fade_opaci = float fade_chang = > 0.5 ? u_fade_cha : v_fade_opa = max(0.0, min(1.0, + SDF_PX 8.0\\n#defi EDGE_GAMMA bool mapbox: define highp vec4 mapbox: define highp vec4 mapbox: define lowp float mapbox: define lowp float mapbox: define lowp float sampler2D highp float bool vec2 vec3 main() {\\n #pragma mapbox: initialize highp vec4 fill_color #pragma mapbox: initialize highp vec4 halo_color #pragma mapbox: initialize lowp float opacity\\n #pragma mapbox: initialize lowp float halo_width #pragma mapbox: initialize lowp float halo_blur\\ vec2 tex = v_data0.xy float gamma_scal = v_data1.x; float size = v_data1.y; float fade_opaci = float fontScale = u_is_text ? size / 24.0 : size;\\n\\n lowp vec4 color = fill_color highp float gamma = EDGE_GAMMA / (fontScale * lowp float buff = (256.0 - 64.0) / 256.0;\\n if (u_is_halo {\\n color = halo_color gamma = (halo_blur * 1.19 / SDF_PX + EDGE_GAMMA / (fontScale * buff = (6.0 - halo_width / fontScale) / SDF_PX;\\n }\\n\\n lowp float dist = tex).a;\\n highp float gamma_scal = gamma * gamma_scal highp float alpha = - gamma_scal buff + gamma_scal dist);\\n\\n gl_FragCol = color * (alpha * opacity * gl_FragCol = float PI = vec4 vec4 vec3 float contents of a_size vary based on the type of property value\\n// used for For constants, a_size is disabled.\\ For source functions, we bind only one value per vertex: the value of evaluated for the current feature.\\n For composite functions: [ feature),\\ feature) ]\\nuniform bool bool highp float u_size_t; // used to interpolat between zoom stops when size is a composite highp float u_size; // used when size is both zoom and feature mapbox: define highp vec4 mapbox: define highp vec4 mapbox: define lowp float mapbox: define lowp float mapbox: define lowp float mat4 mat4 mat4 bool bool highp float bool highp float highp float float vec2 vec2 vec3 main() {\\n #pragma mapbox: initialize highp vec4 fill_color #pragma mapbox: initialize highp vec4 halo_color #pragma mapbox: initialize lowp float opacity\\n #pragma mapbox: initialize lowp float halo_width #pragma mapbox: initialize lowp float halo_blur\\ vec2 a_pos = vec2 a_offset = vec2 a_tex = a_data.xy; vec2 a_size = a_data.zw; highp float segment_an = float size;\\n\\n if && {\\n size = mix(a_size a_size[1], u_size_t) / 10.0;\\n } else if && {\\n size = a_size[0] / 10.0;\\n } else if && {\\n size = u_size;\\n } else {\\n size = u_size;\\n }\\n\\n vec4 projectedP = u_matrix * vec4(a_pos 0, 1);\\n highp float = // If the label is pitched with the map, layout is done in pitched space,\\n // which makes labels in the distance smaller relative to viewport space.\\n // We counteract part of that effect by multiplyin by the perspectiv ratio.\\n // If the label isn't pitched with the map, we do layout in viewport space,\\n // which makes labels in the distance larger relative to the features around\\n // them. We counteract part of that effect by dividing by the perspectiv ratio.\\n highp float distance_r = ?\\n / :\\n / highp float = clamp(\\n 0.5 + 0.5 * 0.0, // Prevents oversized near-field symbols in tiles\\n 4.0);\\n\\n size *= float fontScale = u_is_text ? size / 24.0 : size;\\n\\n highp float = 0.0;\\n if {\\n // Point labels with map' are horizontal with respect to tile units\\n // To figure out that angle in projected space, we draw a short horizontal line in tile\\n // space, project it, and measure its angle in projected space.\\n vec4 = u_matrix * vec4(a_pos + vec2(1, 0), 0, 1);\\n\\n vec2 a = / vec2 b = / = atan((b.y - a.y) / b.x - a.x);\\n }\\n\\n highp float angle_sin = + highp float angle_cos = + mat2 = -1.0 * angle_sin, angle_sin, vec4 projected_ = * 0.0, 1.0);\\n gl_Positio = * / + * (a_offset / 32.0 * fontScale) 0.0, 1.0);\\n float gamma_scal = vec2 tex = a_tex / u_texsize; vec2 fade_opaci = float fade_chang = > 0.5 ? u_fade_cha : float = max(0.0, min(1.0, + v_data0 = vec2(tex.x tex.y);\\n v_data1 = size, mapbox: ([\\w]+) ([\\w]+) ([\\w]+) \"+n+\" \"+i+\" \"+n+\" \"+i+\" \"+n+\" \"+i+\" \"+a+\" = lowp float \"+n+\" \"+o+\" \"+n+\" \"+i+\" \"+n+\" \"+i+\" \"+a+\" = \"+n+\" \"+i+\" \"+a+\" = lowp float \"+n+\" \"+o+\" \"+n+\" \"+i+\" \"+n+\" \"+i+\" \"+a+\" = \"+n+\" \"+i+\" \"+a+\" = rr in Qe)er(rr); Er(e,r,n){ t=new new 0===n&&voi new Error(\"fai to invert 1;var r(r,n,i){v Jr(t){retu 61:case 107:case 171:case 189:case 109:case must be a positive number, or an Object with keys 'bottom', 'left', 'right', e){var k=m*m;func T(t){var | new Error(\"max must be greater than minZoom\"); n=new on;var new not instanceof ln))throw new Error(\"Inv type: 'container must be a String or a in en)t[a]=ne n=new n=new r=new r=new e=new new Error(\"max must be greater than the current 0===n)retu s in 0===n)retu vn(t,e,r){ i in 0 27 41\");var device does not support fullscreen r){var instanceof e(new r=new r};var l(t){var c(t,s){var s;return n(t){retur t)return 1=0)return 1 specify vertex creation specify cell creation specify phase new Invalid boundary t in s){var t in l){var t in c){var return \"+n),s?new n=[\"'use b(e,r){var o=\"__l\"+ a=\"__l\"+ n.push(\"va strict\";va strict\";va o(t,e){ret s(){var [2,1,0];}e [1,0,2];}} [2,0,1];}e new new a.push(\"va function new 0===r){r=n i=new t||\"up\"in i};var u(t,e){var strict\";va r?r+\"\":\" e=new a(e,a,o){v have circular dependency Please, check o=new a?r:functi references in a=new references in i}return t(e,r){ret \"+r+\"is t[r]}}func a(t,e){ret strict\";va l left Left\",top: t top Top\",width width W height W bottom right e=[];retur 0:return r||[];case 1:return 2:return f(t,r){var l};var Array(g),m t-e});var r};var u(t,e){for r=new u=y[a];voi null;retur s=1;a;){va s}};return u(t,n){var n;var v(){if(d){ new Zero-lengt segment detected; your epsilon is probably too small or too r=1;r0){va strict\";va extension should be enabled\"); highp vec2 position, vec4 vec4 vec2 direction, lineOffset vec4 float lineWidth, vec2 scale, scaleFract translate, vec4 main() = color / pixelOffse = lineWidth * lineOffset + (capSize + lineWidth) * dxy = -step(.5, direction. * error.xz + vec2(-.5)) * position = position + pos = (position + translate) * (positionF + * (position + translate) * (positionF + * += pixelOffse / = vec4(pos * 2. - 1., 0, highp vec4 float main() = *= minus src minus dst e)return u=x[c];ret colors cap capsize line-width width line position data t[0]){var a=0;a 0. && baClipping 0. && abClipping cutoff + .5) -= - .5, cutoff + .5, == 1.) = endCutoff. (distToEnd cutoff + .5) -= - .5, cutoff + .5, t = / dashSize) * .5 + .25;\\n\\tfl dash = vec2(t, = *= alpha * opacity * highp GLSLIFY 1\\n\\nattri vec2 position, vec4 vec2 scale, scaleFract translate, float pixelRatio id;\\nunifo vec4 float vec4 float MAX_LINES = 256.;\\n\\nv main() {\\n\\tfloat depth = (MAX_LINES - 4. - id) / position = position * scale + translate\\ + positionFr * scale + + position * scaleFract + positionFr * = vec4(posit * 2.0 - 1.0, depth, = color / *= highp GLSLIFY 1\\n\\nvaryi vec4 main() = points data lineWidth lineWidths line-width linewidth width stroke-wid strokewidt linejoin join type dashes dasharray dash-array colour stroke colors colours stroke-col fill-color crease overlap close closed-pat hole 1.0 + delta) -= smoothstep - delta, 1.0 + delta, borderRadi = ratio = - delta, borderRadi + delta, color = mix(fragCo *= alpha * = highp GLSLIFY 1\\n\\nattri float x, y, xFract, float size, vec4 colorId, float vec2 scale, scaleFract translate, float sampler2D vec2 float maxSize = vec4 fragColor, float isDirect = (paletteSi e[0]){for( a;if(t instanceof Uint8Array instanceof s(){functi 5120:n=new 5121:n=new 5122:n=new 5123:n=new 5124:n=new 5125:n=new 5126:n=new null}retur s}var e||(e=new t&&t._elem instanceof g(t){for(v t}function L(t){for(v t=0;return n(t,e){var f}var h=new a=m();retu for(var u(t){var h(t){retur m(e){var a(t,e){for t=0;return a=i[t];ret a||(a=new in n(t){if(t in i){var in a){var c=a[t];ret in in e?new o=t;t=new e(e,a){if( in r){var s})}else if(t in n){var n(t,n){ret in in in i(t){retur r.def('\"', in in o(){functi t=0;return o(e,r){var c=new blend.equa stencil.fu stencil.op viewport scissor.bo minus src minus src minus dst minus dst minus constant minus constant alpha u(){var null;var r(e){var tt(t,l);el t=0;t=r)re n}function u(t){retur new e=new \",e);var s=new o;n=-(i+a) i(n(t))};v i}function u(t,e){for r=new r}function x(t){for(v e=m(t);;){ t=T[0];ret w(t,e){var r=T[t];ret T=[],A=new l}else if(c)retur l}else if(c)retur c;return a[0]-s[0]} i(t,e){var t;var r}function i=p.index; strict\";va a(t,e){var o(t,e,r,n) i[e];var new unexpected new failed to parse named argument new failed to parse named argument new mixing positional and named placeholde is not (yet) n(t,r){ret strict\";va Error(\"Fir argument should be a should be a string or a b=new Array(y),d c)|0 d=new Array(r),g Array(r),v Array(r),m r};var strict\";va should be valid svg path n;var n=!1;var e=new m=new t(e,r,i){v i=i||{};va \":{data:ne p=new a}function t){var r={};for(v n in e={};for(v r in e}(S);func C(t){retur in o(t){var e=t1)for(v i})}}var new Error(\"n must be new Error(\"alr in h(t){retur new p(t){retur new d(t){retur new g(t){retur new v(t){retur new m(t){retur new y(t){retur new x(t){retur new b(t){retur o?new _(t){retur new null}retur t}).join(\" \");var \",\"italic bold \"):\"bold n}function b(t,e){var _(t,e,r,n) n=y(e);ret n?r in o?r in o&&delete p,d=new al-ahad\",\" {0} not {0} {0} {0} mix {0} and {1} format a date from another number at position name at position literal at position text found at dd M MM d, d M d M d M d M yyyy\",RSS: d M l){y(\"m\"); a=this;ret var _inline_1_ = - var _inline_1_ = - >= 0) !== (_inline_1 >= 0)) {\\n + 0.5 + 0.5 * (_inline_1 + _inline_1_ / (_inline_1 - }\\n r=[];retur 0 1,1 0,-2A2,2 0 0,1 strict\";va strict\";va o(t){var s(t,e){var strict\";va o(t,e){var void void z();var void z();var strict\";va t}var a?\"rgba(\"+ i=n(t);ret t){var 0!==i&&voi 0!==a){var strict\";va strict\";va 0;var strict\";va strict\";va r(t,e){var o(t,i){var r}function strict\";va strict\";va f(){var h(t){retur r;try{r=ne strict\";va strict\";va t){var 0!==u,d=vo e=void 0!==x,w=vo 0!==b;retu E=.5;funct C(t,e,r,i) \")}).split \")}).split scale(\"+e+ strict\";va 0 1,1 0 0,1 \"+a+\",\"+a+ 0 0 1 \"+a+\",\"+a+ 0 0 1 \"+r+\",\"+r+ 0 0 1 \"+r+\",\"+r+ 0 0 1 0 1,1 0 0,1 0 1,1 0 0,1 strict\";va l(t,e,r,i) t.id});var o.remove() strict\";va strict\";va strict\";va o(t){retur d(t){var A(t,e,r){v x,b,_=\"top to delete T(t,e){ret Array(g);v w(t,e){ret strict\";va T(t,e,r){v void t.remove() A(t,e){var t;for(var \";return t}function M(t,e){var S(t,e,r){v if(l){var E(t){var strict\";va y(t,e,r){v x(t,e,r){v i=m(void A=m(void m(t,e,r,n) r[1]}retur o}function y(t){retur strict\";va u(t,e){var v8h2v-8 h8v-2h-8 v-8h-2 if(P){var h2 v-18 v2 h-18 d(t,e,r){v g(t,e){var extra params in segment 0 1,1 0 0,1 r in S(t,e){var E(t,e){var C(t,e,r){v L(t,e){var r(r,i){ret strict\";va p(t){retur c(t,e){ret t){var ms \"+t+\" in calendar \"+r)}var c=new e=new o*o+s*s}va 0;var n[r];var h(e){var new Error(\"No DOM element with id '\"+t+\"' exists on the page.\");re new Error(\"DOM element provided is null or strict\";va f(t,e){var r=t;return c;var e=a(t);ret p(t){retur void void r=new failed n;function i(){return r={};retur strict\";va property strict\";va instanceof if(!(void c(t,e){ret binary r=e%1;retu strict\";va strict\";va s(t,e){ret was an error in the tex void O(),void e();var \");var u(){c++;va s=1;s doesnt match end tag . Pretending it did unexpected end tag null;var n&&E(n)}va r=void E(t){retur e(t);var C(t,e,r){v i(e,r){ret void new 1px l(t){var strict\";va n={};funct s in e=n[t];ret e&&e.timer n[t];else for(var e in strict\";va strict\";va to enter Colorscale title\":\"Cl to enter Colorscale to enter Colorscale title\":\"Cl to enter Colourscal %b %e %X %-d, strict\";va previous rejected promises from t.scene1); in array edits are incompatib with other edits\",f); full array edit out of if(void & removal are incompatib with edits to the same full object edit new Error(\"eac index in \"+r+\" must be new Error(\"gd. must be an e)throw new is a required new Error(\"cur and new indices must be of equal new Error(\"gd. must be an new Error(\"upd must be a key:value r)throw new Error(\"ind must be an integer or array of a in new \"+a+\" must be an array of length equal to indices array in new Error(\"whe maxPoints is set as a key:value object it must contain a 1:1 corrispond with the keys and number of traces in the update d in t[e]}}func 0);var G(t,e,r){v i in a in c in Y(t,e){var i in e){var o in $(t,e){var Y in r;return l(t){retur c(t,e){var r=0;return t()}}retur void m(t){retur addFrames accepts frames with numeric names, but the numbers areimplici cast to new Error(\"gd. must be an e)throw new Error(\"tra must be t;var t=a[i]}els t=a}}retur t}function k(t){retur T(t){retur t(e){for(v r in h(){var t=l;return u(),t}retu i=h();for( a in strict\";va w(t){var x(t,e,r){v t(e,r){for n in strict\";va d(t){retur in new Error(\"Hei and width should be pixel new Error(\"Ima format is not jpeg, png, svg or webp.\");va g={};funct v(t,r){ret S(){return new E(){return new new strict\";va e}}var strict\";va u=new V(t,e,r){v r;function n(t){retur t.dtick){v error: t+o*e;var dtick dtick Y(t,e){var X(t,e){var 0}function Z(t,e){var to zoom back V(t){var strict\";va strict\";va y(t){retur t._id}func S(t){var E(t,e,r){v strict\";va m(t){retur y(t){retur r={},n=0;n g(e,r){var strict\";va o(t,e){var 0===t[r]&& v(t){retur n(t){var i(t){retur in in in strict\";va u(){}var m(t){retur i(t,r){for C(t,e){var z(t,e){var I(t,e){var D(t,e){var strict\";va u(t,e){ret f(t,e,r){v h(t,e){var i(r){var p(t,e){var y(t){retur x(r){var n=e(r);ret d(t,e){var strict\";va strict\";va e=0;e/g,\" l(t){var n=new a(t,e);ret strict\";va Sans Regular, Arial Unicode MS strict\";va r(r,i){ret strict\";va g(t){var v(t){retur new p=s.map=ne b(){var - _(t,e){var o(t){for(v i=0;i0){va in h in l=void delete i[e],delet m in d)g[m]||de d[m];for(v y in M in S(t,e,r){v n=!1;var l(){return t)return e,n,i={};f in i}return r=c(t);ret e&&delete I=(new + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + 0px\",\"1px -1px\",\"-1p 1px\",\"1px \"+t+\" 0 W(t,e){ret \"+n+\" \"+n+\" \"+n+\" u=\"t: \"+c.t+\", r: l;var r in t)r in r in r=e||6;ret t)return null;var r(){var n.mode,del r}};return strict\";va s;function h(r,s){ret i(t,e){ret n=e[r];ret d(t,e,r,n) o(r,n){ret strict\";va D(t){var e={};retur P(t,e){var R(t,e){ret F(t,e){ret N(i,a){var to zoom back V(t,e){var 0 \"+d+\" \"+g));var _=(new strict\";va m(t){for(v x(t){retur e}return void r;return o;return A(e,r,n){v i}var c(t){var e=o?r:n;re _=0;m(\"per of of of strict\";va strict\";va strict\";va strict\";va 0, 0, strict\";va s(t,e,r){v i in l(t,e){ret c(t){retur in t)return s;s=e+\"0\"i \\xb1 strict\";va s(t,e,r,o) u(t,e,r,a) null;var i=1/0;var a=-1/0;var strict\";va b.tickvals I(n){var D(n){var strict\";va strict\";va strict\";va f(e,r){var e>0&&void converged strict\";va strict\";va strict\";va strict\";va c(t,e){for strict\";va strict\";va data invalid for the specified inequality many contours, clipping at strict\";va loop in contour?\") g(t){retur to newendpt is not vert. or perimeter strict\";va if(h){var void strict\";va strict\";va strict\";va strict\";va u(r,i){ret strict\";va k(t){retur to newendpt is not vert. or perimeter strict\";va a=!1;funct o(r,a){ret s=0;s strict\";va left\",\"top center\",\"t strict\";va strict\";va strict\";va l(r,a){ret strict\";va strict\";va r(r,a){ret strict\";va u(){var scale is not scale is not strict\";va iterated with no new in strict\";va void i.error(\"E hovering on heatmap, pointNumbe must be [row,col], didn't converge strict\";va l=0;la){va d(t,e,r){v P(t,e){ret if(k>0){va F(t,e,r,n) strict\";va strict\";va strict\";va strict\";va p(t){for(v f}function f(t,e,r,i) v(t){retur strict\";va strict\";va h(r,i){ret strict\";va u(t,e,r,i) t.color}); -1px 1px 2px, \"+I+\" 1px 1px 2px, \"+I+\" 1px -1px 2px, \"+I+\" -1px -1px 0, f(t){retur t.key}func h(t){var p(t,e){ret \"Courier New\", \"));var \"Courier New\", \\u2229 \"+p+\"): | color): | \"+p+\"): \"Courier New\", r=[];retur S(t){var z(t){for(v \";return P(t){var u in f(t){var h=new y,x=new strict\";va strict\";va strict\";va c(t,e){ret f(t,e){ret w(t,e){ret T(t){for(v strict\";va strict\";va o(t){retur traces support up to \"+u+\" dimensions at the strict\";va highp GLSLIFY 1\\n\\nattri vec4 p0, p1, p2, p3,\\n p4, p5, p6, p7,\\n p8, p9, pa, pb,\\n pc, pd, vec4 pf;\\n\\nuni mat4 dim1A, dim2A, dim1B, dim2B, dim1C, dim2C, dim1D, dim2D,\\n loA, hiA, loB, hiB, loC, hiC, loD, vec2 resolution sampler2D sampler2D mask;\\nuni float vec2 vec4 unit_1 = vec4(1, 1, 1, 1);\\n\\nflo val(mat4 p, mat4 v) {\\n return v) * unit_1, axisY(\\n float x,\\n mat4 d[4],\\n mat4 dim1A, mat4 dim2A, mat4 dim1B, mat4 dim2B, mat4 dim1C, mat4 dim2C, mat4 dim1D, mat4 dim2D\\n ) {\\n\\n float y1 = val(d[0], dim1A) + val(d[1], dim1B) + val(d[2], dim1C) + val(d[3], dim1D);\\n float y2 = val(d[0], dim2A) + val(d[1], dim2B) + val(d[2], dim2C) + val(d[3], dim2D);\\n return y1 * (1.0 - x) + y2 * x;\\n}\\n\\nc int bitsPerByt = 8;\\n\\nint mod2(int a) {\\n return a - 2 * (a / 2);\\n}\\n\\n mod8(int a) {\\n return a - 8 * (a / 8);\\n}\\n\\n zero = vec4(0, 0, 0, 0);\\nvec4 unit_0 = vec4(1, 1, 1, 1);\\nvec2 xyProjecti = vec2(1, 1);\\n\\nmat mclamp(mat m, mat4 lo, mat4 hi) {\\n return lo[0], hi[0]),\\n clamp(m[1] lo[1], hi[1]),\\n clamp(m[2] lo[2], hi[2]),\\n clamp(m[3] lo[3], mshow(mat4 p, mat4 lo, mat4 hi) {\\n return mclamp(p, lo, hi) == p;\\n}\\n\\nb mat4 d[4],\\n mat4 loA, mat4 hiA, mat4 loB, mat4 hiB, mat4 loC, mat4 hiC, mat4 loD, mat4 hiD\\n ) {\\n\\n return mshow(d[0] loA, hiA) &&\\n mshow(d[1] loB, hiB) &&\\n mshow(d[2] loC, hiC) &&\\n mshow(d[3] loD, d[4], sampler2D mask, float height) {\\n bool result = true;\\n int float valY, valueY, scaleX;\\n int hit, bitmask, valX;\\n for(int i = 0; i T(t,e,r){v E(t,e){ret 255, 255, 0)\");var E(t,e){for 1px 1px #fff, -1px -1px 1px #fff, 1px -1px 1px #fff, -1px 1px 1px strict\";va 0===l[s]){ i(t,e,r){v strict\";va left\",\"top center\",\"t right\",\"mi left\",\"bot left\",\"top center\",\"t right\",\"mi left\",\"bot strict\";va strict\";va u(t){retur f(t,e){var c;var g(t,e){ret v(t,e){var m(t,e){var y(t,e){var x(t){var r}function b(t,e){for strict\";va strict\";va strict\";va strict\";va 0, for(r=new P=!1;retur r=c(e);ret strict\";va r(r,a){ret p(r,a){ret m(t,e){ret _(t,e){ret strict\";va y(){var t=.5;retur \"+i.target 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 \"+i.target 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 \"+c+\",\"+h+ \"+l+\",\"+f+ k(t){retur 0 0 1 0 0)\":\"matri 1 1 0 0 0)\")}funct M(t){retur S(t){retur 0 0 1 0 0)\":\"matri 1 1 0 0 0)\"}functi E(t){retur 1)\":\"scale 1)\"}functi C(t){retur L(t){retur O(t,e,r,a) o(){var strict\";va v(r,i){ret strict\";va strict\";va strict\";va H(e){var q(t,e,r,n) st(t,e){re 0)}functio s}}functio ct(t){var strict\";va strict\";va strict\";va y(t){retur $(t){retur J(t){retur K(t){retur Q(t){retur t.id}funct Q}function scatter strict\";va y(t){retur x(t,e){ret b(t){retur p[t]}funct o=0;o=0){v o}function k(t,e){var = strict\";va strict\";va f(t,e){var strict\";va d(r,i){ret 1/0;var u(t){retur strict\";va s=a[0];if( a;var strict\";va c=l[0];if( l;var strict\";va strict\";va strict\";va strict\";va strict\";va d(t){var g(t,e){var g(t,e){var h.remove() d}}(t);ret x(t,e){var r;return k(t){retur T(t,e,r){v A(t,e,r){v r;return strict\";va o(t,e,r){v strict\";va or strict\";va strict\";va void void r}function strict\";va t/E*m});va r=A[t];ret strict\";va r&&r.match strict\";va s(r,a){ret m=(\" function() e,r})}func I(t,e){for r}function D(t,e){for g(t){retur strict\";va a=!1;funct o(r,a){ret strict\";va e=h(t);ret e=h(t);ret e=h(t);ret e=h(t);ret = 'local'}; if {font: if (typeof require !== 'undefined { paths: { 'plotly': } }); { window._Pl = Plotly; }); } In\u00a0[3]: def payload): url = response = requests.g url=url, params=pay ) if == 200: # - success!') return else: return None In\u00a0[4]: payload = { 'metrics': 'PriceUSD, 'TxTfrValU 'start': '2016-01-0 } # PriceUSD and TxTfrValUS are not utilised yet. Because the work needs to be expanded in order to be complete, I will keep them here for now. asset_list = ['btc', 'ltc', 'bch', 'bsv', 'doge'] data = {} for asset in asset_list data[asset = payload) In\u00a0[5]: dataframes = {} cols = ['PriceUSD 'TxTfrValU for asset in data.keys( values = [ each['valu for each in index = [ each['time for each in df = columns = cols) df.index = for col in df.columns df[col] = # create new fields df['TxCoun = df.TxTfrVa / = / = df In\u00a0[6]: # take a look at the wrangled data: Out[6]: .dataframe tbody tr { middle; } .dataframe tbody tr th { top; } .dataframe thead th { text-align right; } PriceUSD TxTfrValMe TxTfrValUS TxCount 2019-07-14 131.077983 1198.88905 0.879414 1.194010e+ 99593.0000 1363.28177 2019-05-29 193.318255 11679.3278 39.889202 5.360111e+ 45894.0000 292.794223 2019-03-28 63.633667 4749.33232 2.107293 6.684210e+ 14074.0000 2253.75966 2018-12-06 104.833018 47784.4041 25.002851 4.480266e+ 9376.00000 1911.15821 2019-08-20 145.397527 403.290224 1.017783 4.721077e+ 117063.999 396.243941 Out[6]: .dataframe tbody tr { middle; } .dataframe tbody tr th { top; } .dataframe thead th { text-align right; } PriceUSD TxTfrValMe TxTfrValUS TxCount 2018-09-17 6259.37814 6375.29005 63.354108 3.529992e+ 553699.0 100.629466 2018-03-08 9334.33005 13564.9724 115.393462 6.573545e+ 484597.0 117.554082 2018-09-18 6341.49459 6596.25081 65.237872 3.774480e+ 572216.0 101.110760 2017-10-18 5577.94935 12797.5675 68.015674 1.002400e+ 783274.0 188.156154 2018-01-21 11392.3086 13715.3752 161.642277 8.149868e+ 594214.0 84.850173 Out[6]: .dataframe tbody tr { middle; } .dataframe tbody tr th { top; } .dataframe thead th { text-align right; } PriceUSD TxTfrValMe TxTfrValUS TxCount 2018-02-17 0.007081 2667.70915 1.382312 1.623514e+ 60858.0 1929.88930 2019-02-26 0.001952 353.988959 0.807753 2.240715e+ 63299.0 438.239326 2019-03-22 0.002017 200.227768 0.443784 1.414069e+ 70623.0 451.182435 2018-08-14 0.002247 184.665415 0.516715 1.255374e+ 67981.0 357.383769 2017-04-09 0.000385 235.941139 0.234514 9.458172e+ 40087.0 1006.08471 Compare daily mean and median USD transactio value for BTC since January 2016\u00b6 In\u00a0[7]: btc_mean = go.Scatter name='BTC mean', ) btc_median = go.Scatter name='BTC median' ) data = [btc_mean, btc_median layout = go.Layout( title=\"BTC median and mean transactio values by day\", value'), ) fig = layout=lay py.iplot(f Out[7]: The chart above shows\u00a0that the daily mean transactio value is higher than the daily median The two averages are\u00a0correl From 2016 to present, the mean is approximat 2 orders of magnitude higher than the median. This relationsh appears to be consistent across the previous 4 years. Note: During the last 4 years, the USD value of 1 BTC has increased from ~400USD to currently ~10000USD. The impact of the changing USD price of the coins on mean and median should be investigat This could be easily achieved using the TxTfrValUS and PriceUSD metrics. We could also then calculate number of Plot the ratio of daily mean to median USD transactio values for each asset since January 2016\u00b6 In\u00a0[8]: def name): return go.Scatter name=name ) data = asset) for asset in layout = go.Layout( title=\"Rat of daily mean to median transactio value\", ) fig = layout=lay py.iplot(f Out[8]: Note: Except for BTC, the time series above are very \u201cchoppy\u201d. If this chart were to be shown to clients I would consider smoothing the time series by using e.g. 7 day moving average. However, this might obscure some features of the data so for the initial data exploratio I will not chart above shows that BTC has the lowest ratio of mean to median daily transactio value. This suggests that compared to the other blockchain in this has relatively strong organic\u00a0us is less influenced by\u00a0whales. MMR has lower Point 3 suggests a wide and regular user base and total daily transactio volumes should be analysed across the 5 chains to futher strengthen or rebutt Using this ratio as a proxy to measure organic use, the chain with the second most organic use is\u00a0Litecoi Since the start of 2019, the influence of whales on the Dogecoin network has Of the two contentiou hard forks, Bitcoin Cash shows two distinct phases with different in\u00a0each: From its inception in August 2017 to November 2018, the influence of whales increased at a steady rate. At the coins genesis, there appears to have been a large organic user base transactin daily, bringing the median transactio value to within 50 - 100x the mean daily transactio value. This was lower than Bitcoin\u2019s, which had a much more consistent but higher MMR of 120 -\u00a0200. After November 10 2018, the ratio increases from an average of approximat 500 to approximat 10,000. This is a stark and abrupt change in the daily ratio, and suggests that\u00a0eithe organic use drasticall decreased, BCH very suddenly started being used to facilitate very large value transfers by relatively few\u00a0users. As of January 2019, Dogecoin appears to have more widespread organic use than either BCH or BSV, despite its status as a \u201cjoke\u201d blockchain However DOGE has had a higher MMR than BTC or LTC in\u00a02019. Next Steps\u00b6This brief investigat was developed over the course of an afternoon, in line with the project brief recommendi only 4 hours of work. In order to be applied in a commercial context, this analyis should be expanded and tested in at least the Test if the central assumption of this analysis is true. Possible approaches could\u00a0incl Removing exchange outflows from the data. Could this be done using known exchange addresses (exchanges aggregate organic retail Quantifyin the influence of \u201cchange\u201d transactio - in aggregate this should be nil for day-to-day \u201ccash\u201d transactio but for whales moving the entire balance of an address there would be no \u201cchange\u201d amount. Depending how the metric is calculated this may or may not For BTC and LTC, are the lightning networks distorting the results by hiding organic low For BTC, is the liquid sidechain hiding the activity of whales to the extent that it is not the \u201chealthies of the 5 Can we infer where the whales and \u201cnormal\u201d users live, by analyzing the time of transactio People are much more likely to make a transactio at midday than midnight, and we could use this to investigat geographic clustering e.g. Is BTC a \u201cwestern\u201d chain, whilst BCH has more organic use in\u00a0Asia? An analysis of daily transactio volume (in USD terms) would be essential to this analysis. It would provide a context in which to interpret the significan of difference between each chain and difference bewtween time\u00a0frame Similarly, comparing the hash power dedicated to mining new blocks on each chain would indicate commercial interests, and abrupt changes in hash power could possibly be correlated with changes in mean-media ratio (MMR). if { var mathjaxscr = = = = ? \"innerHTML : \"text\")] = + \" config: + \" TeX: { extensions { autoNumber 'AMS' } },\" + \" jax: + \" extensions + \" displayAli 'center',\" + \" displayInd '0em',\" + \" showMathMe true,\" + \" tex2jax: { \" + \" inlineMath [ ['$','$'] ], \" + \" displayMat [ ['$$','$$' ],\" + \" true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS' { \" + \" linebreaks { automatic: true, width: '95% container' }, \" + \" styles: { .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important' }\" + \" } \" + \"}); \"; (document. || }"},{"title":"A faster\u00a0shell","url":"shell.html","body":"Opening up a new shell was annoyingly slow. Not terrible, but enough to notice. Its a\u00a0niggle. I wanted to find out which components were causing the most delay, so I used time to measure how long it took to launch a shell. Even though shells might appear to be part of the low level \u2018guts\u2019 of a computer, each shell is just an executable and can be treated as\u00a0such. To measure the startup speed of your shell,\u00a0do: for i in $(seq 1 10); do /usr/bin/t $SHELL -i -c exit; done This shows that it takes 0.84 seconds to start zsh - not terrible, but not\u00a0great: You can compare the performanc of different shells by replacing $SHELL with zsh, bash, fish etc. Here are the results if I used BASH instead of zsh - 9.3x faster! (but without useful tools and plugins): Now that I can measure how long it takes to start, it would be useful to know which proccesses are causing the greatest delays. This could be done with something like zsh -xv which enables verbose output and xtrace. This creates a tonne of output, but doesnt inlcude timestamps All I really want is a summary of how much time each subprocces required to run, i.e. an order Add zmodload zsh/zprof at the start of .zshrc and zprof at the very end. Now when I start zsh I see the\u00a0follow Next steps - make run faster, or asyncronou or not at\u00a0all\u2026 Update: is the biggest cause of slow loading. Using the lazy loading option decreased loading time by\u00a00.3s"},{"title":"Bitcoin\u00a0Lightning","url":"bitcoin-lightning.html","body":"One of the largest obstacles (second only to privacy in my opinion) to widespread adoption of Bitcoin is its limited volume. Bitcoin cannot faciliate payments fast enough such that it could compete with Visa or\u00a0Masterc The most interestin solution to this problem is the Lightning protocol - a separate protocol that sits ontop of the Bitcoin protocol (a so-called \u2018Layer 2\u2019 solution). Lightning uses hashed time locked contracts (HTLCs) to trustlessl and privately move transactio off-chain. This allows payments to be faster, cheaper and more frequent. It also has interestin implicatio for There are a lot of resources about what the Lightning network is, why it\u2019s neccesary and how it works. There are also several good guides available about how to set up and maintain a node. I used a Raspberry Pi with an external hdd. It took a few attempts, mostly because it\u2019s my first time working with an unix operating system and I tried to move a swap file to a disk that wasnt formmated as\u00a0ext4\u2026 Anyway, I\u2019ve opened and closed some channels, connected to peers, and made some transactio I even bought some stickers You can find my node using these\u00a0deta Alias: Public Key: IP address: 85.145.183 Port: 9735 Some Lightning Lightning Interestin things to\u00a0do"},{"title":"Sync a BTC node,\u00a0quickly","url":"sync-bitcoin-core-node.html","body":"In order to run your own bitcoin node, or lightning node, you\u2019ll need to download the entire bitcoin blockchain and then validate it. This takes ages on a magnetic disk due to the random access speed of the contents of the To remove this bottleneck move the chainstate directory to an SSD (its only a few GB) and symlink to it from the bitcoin data directory. More details are on the Bitcoin wiki. When the sync is complete, replace the symlink with the actual"},{"title":"Bakke-Rij","url":"bakkerij.html","body":"I recently began working in a new coworking space, sharing an office unit with another entreprene What makes this coworking space unique is it\u2019s conversion from an industrial bakery to a coworking space. Where the machinery once stood, converted shipping containers with glass walls have become offices for startups The skylights, glass walls and bright colors create a light and airy atmosphere and the space seems popular with designers and founders working in creative industries It\u2019s an energetic space with a creative feel to\u00a0it."},{"title":"Prediction\u00a0Markets","url":"prediction.html","body":"Predicting port traffic using Background A prediction market allows people to bet on an unknown future event. For example, \u201cWhat will the Euro-Dolla exchange rate be on date\u00a0X?\u201d. In several common forecastin scenarios, prediction markets have been more accurate than polls, expert opinions, and statistica methods1 and therefore prediction models are useful for observers (anybody who is interested in the outcome) and not just the market\u2019s participan Prediction markets can be used for categorica events (a specific event that either does or doesn\u2019t happen) or scalar events (when the outcome is between a range of values). The predefined source of truth for the outcomes being predicted is called an\u00a0Oracle. Prediction markets enables participan to purchase shares or tokens tied to the outcome of a specific future event. Once the event has occurred, holders of the tokens representi the actual outcome will receive a reward of predefined value. This creates an incentive to hold tokens correspond to the correct outcome, and the market dynamics of supply and demand allow the price to reflect perceived probabilit of different outcomes. Helpfully, the price of each type of token correspond to the relative probabilit of each outcome occurring which allows for simple interpreta of the results. If the reward for holding a share correspond to the correct outcome is $1, and the present price of this share is 50 cents, then the market\u2019s estimate of the likelihood of this outcome occurring is\u00a050%. Shares can be traded continuous As trading occurs over time the probabilit of different outcomes will change as new informatio becomes known, and the changing price of the shares quantify\u00a0t An\u00a0example Let\u2019s describe how this could work with an\u00a0example There is a large degree of uncertaint around how Britain will continue to trade with the other European countries when it exits the EU on March 29, 2019. A prediction market will likely be a better predictor of the outcome than any other\u00a0meth If an adequate agreement isn\u2019t achieved, Britain\u2019s main port at Dover will certainly experience long delays and large traffic jams. Therefore a prediction market asking \u201cHow many vehicles will be admitted into Britain at the port of Dover between 00:00 and 23:59 on March 29 2019?\u201d will give a useful prediction about the outcome of Britain\u2019s trade negotiatio - a key component and sticking point in the Each type of token in the prediction market will correspond to different quantities of vehicles entering the port3 - for example there could be four (or more) categories Less than 8000, between 8000 and 11000, between 11000 and 14000, and more than 14000. The relative price of a share in each category will correspond to the relative probabilit of each If the number of vehicles is lower than would otherwise be expected, (around 12000) this would likely be due to the impact of Brexit and thus the market will serve as a useful proxy for predicting what kind of Brexit will\u00a0occur Stakeholde incentives are\u00a0aligne One reason that prediction markets work so well is because they aggregate informatio from disparate sources and the price shows not only an impartial assessment of the most likely outcome but the aggregated level of confidence that the participan have. Since no one is obliged to participat those that do believe they have valuable informatio which gives them a competitiv advantage. This creates a mechanism that moves good quality informatio into the prediction market, with the resulting prices reflecting the probabilit of a range of\u00a0outcome In our example, people with relevant informatio would include port employees, business owners in the UK and in Europe, politician civil servants, business analysts, bankers, etc. Whilst it is clear that any of these roles may have useful informatio or judgement about the outcome, it is not clear how useful each participan role is relative to the others. By allowing a participan to bid for as many shares in as many different categories as they want, each participan confidence in their informatio can be\u00a0quantif In this way, prediction markets align the incentives of market participan and observers. If the market is large enough then it becomes prohibitiv expensive to distort the market and promote poor quality informatio including informatio designed to create FUD (Fear, Uncertaint Doubt), fake news, or Prediction markets are nothing new, with political betting being used to make prediction as early as the 1500s. However the adoption of the internet and decentrali networks now allow prediction markets to be used more widely and cheaply than ever before. Gnosis is building a platform on the Ethereum blockchain on which others can build new applicatio which harness the power of By lowering the cost and complexity of creating a prediction market, observers can benefit from high quality and impartial predictive informatio about future events, and market participan are rewarded for accurate assessment of likely outcomes. This will enable better decision making and empower observers with previously unobtainab insight. In our example, the market could be created and funded by any organisati that would benefit from knowing the results of the prediction market. This could be the port of Dover itself, news organisati or market research firms. Any of these businesses would benefit from an K. J. Arrow, R. Forsythe, M. Gorham, R. Hahn, R. Hanson, J. O. Ledyard, S. Levmore, R. Litan, P. Milgrom, F. D. Nelson, G. R. Neumann, M. Ottaviani, T. C. Schelling, R. J. Shiller, V. L. Smith, E. Snowberg, C. R. Sunstein, P. C. Tetlock, P. E. Tetlock, H. R. Varian, J. Wolfers, and E. Zitzewitz. The promise of prediction markets. Science, 320(5878), 2008.\u00a0\u21a9 For example Data from"},{"title":"Reading: April\u00a02018","url":"reading-april-2018.html","body":"Articles Mental models A comprehens list organised around different discipline It a long read but probably one of the most easy to apply and tangibly useful articles I\u2019ve read\u00a0recen Life is short - Is life actually short, or would we always want more time, no matter how much we could\u00a0have Invisible asymptotes - A well written and long look at various facets of Factors from scratch - Investing: A unified framework to explain how factors\u00a0wo This is tragic - \u201c\u2026But I do think this highlights the potential disconnect between mental health & business, publicity & success, and success & happiness. The internet can seem so intimate but ultimately it\u2019s a thin view of an individual or Vim after 15\u00a0years Cryptocurr regulation around the\u00a0world On the 2008 financial crisis - \u201cI\u2019m not sure if it\u2019s possible for an action to be both necessary and a disaster, but that in essence is what the Podcasts Jill Carlson on the \u201cWhat Bitcoin Did\u201d podcast. \u201cFor the first time there is no longer a monopoly on the creation of value or monetary systems, that is really What Bitcoin\u00a0Di Georges St-Pierre on the Joe Rogan podcast. A long and candid conversati from one of the world\u2019s best\u00a0athle Ricardo Spagni a.k.a Fluffypony on Monero vs Bitcoin, EOS, the current bear market, Tari, and\u00a0ASICs. Resources Lots of data\u00a0sets"},{"title":"Ry\u2019s Git\u00a0Tutorial","url":"rys-git-tutorial.html","body":"For tracking changes to a collection of files, Git is the ubiquitous solution. It\u2019s free, robust, comprehens and there is a plethora of resources that are easy to\u00a0find. I usually find the commands difficult to remember though, and the concepts which Git is built on often seem to me. This means I spend a lot of time searching for answers and trying to remember how I can use Git to experiment with a project without fear of losing any hard won\u00a0progre Ry\u2019s Git Tutorial by Ryan Hodson is the best way to learn Git that I have come across. Its simple, practical, and clear. The reader learns how to use Git yy creating and maintainin a simple website. This gives the Git commands a meaningful context, which makes them a lot easier to remember and use in the\u00a0future The tutorial was first published in 2012 and the website which originally hosted the examples no longer exists. Each tutorial chapter starts with a link to download the project files up to that point, so the reader doesn\u2019t need to start at the beginning but can jump into any part of the guide. Unfortunat these links are dead\u00a0now. Therefore I\u2019m hosting the tutorial here so that it continues to be useful. If the author would like to get in touch, please do. I\u2019d like to keep this great resource available so that others can benefit from\u00a0it. Download the .epub file here Download the example files for each module\u00a0bel Chapter 2: Chapter 3: Branches\u00a0I Chapter 4: Branches II Chapter 5:\u00a0Rebasin Chapter 6: Chapter 7:\u00a0Remotes Chapter 8: Chapter 9: Chapter 10: Chapter 11: Tips &\u00a0Tricks Chapter 12:\u00a0Plumbi end"},{"title":"How to buy\u00a0Bitcoin","url":"buying-btc.html","body":"Recently a few friends have asked me how they can buy Bitcoin. I\u2019m not a financial advisor, but here are a few things that come to\u00a0mind: Don\u2019t invest what you can\u2019t afford to\u00a0lose. If the price falls 50%, you need to be able to wait whilst the Write down the\u00a0follow How much can you afford to invest? Consider how much cash you will need over the next year, how long it would take to recover any losses,\u00a0et How long do you want to invest\u00a0for How much profit do you want to make? Don\u2019t have an You will need to get good at identifyin and ignoring the following, even from your\u00a0frien Hype Fake\u00a0news Fear, Uncertaint Doubt (FUD) Don\u2019t trust The previous point is really important, and By putting something you care about in a risky situation (it is risky) you will experience anxiety and excitement You need to control your psychology and identity when other people are trying to manipulate you. This is useful in all areas of\u00a0life. Investing is good mental exercise because money has an intensely psychologi quality about it, and crypto currencies are the most intense trading experience there is right\u00a0now. Almost no one has a clue what\u2019s happening. Convention economists and traders certainly don\u2019t. This is and the rules haven\u2019t been worked out yet. We\u2019ve never had this tech before, and the internet has made everything - communicat and innovation - much quicker. This is a powerful combinatio of factors and we haven\u2019t seen them play out\u00a0before Take some time to read about the fundamenta and understand the tech as much as you can. Think about why people would behave in certain ways and what makes bitcoin useful, or not. Get started on this today, it will take some\u00a0time. YouTube has a tonne of videos, and these two sites are good and detailed: lopp.net Twitter has a lot of current and new informatio but also a lot of bots and scammers spreading hype and FUD (see point\u00a04). If you\u2019re going to lose sleep over your investment invest\u00a0les Don\u2019t buy at an all time high. Do your research, wait for the price to correct. Prices fall. The chart at the top of the page shows the 20 day and 55 day moving average compared to the daily price. My opinion is that by early January the price will have returned to between the 20 day and 55 day average before beginning to rise\u00a0again Coinbase is a user friendly and reputable exchange, there are other good exchanges\u00a0 Don\u2019t make financial decisions when you\u2019re feeling rushed. Check If you\u2019ve bought some Bitcoin or other don\u2019t store them on an exchange. Transfer them to a wallet that you control. If you don\u2019t own your private key, you don\u2019t own the asset. If you don\u2019t know what that means, google it (Point\u00a08)."},{"title":"Live near the\u00a0ocean","url":"ocean.html","body":"California Dorset Acapulco Dublin"},{"title":"Pangea","url":"pangea.html","body":"The\u00a0proble In many countries the ability to create legally binding agreements is not available to average citizens. Legal services are often unaffordab and opaque, or service providers are corrupt. Legal services are in need of\u00a0disrupt The\u00a0soluti Pangea Bitnation (who I consult for) intends to address this problem by empowering people to self-organ and self-gover We are building a platform called Pangea which allows users to create, notarise and arbitrate contracts according to a jurisdicti which each party joins voluntaril irrespecti of their Pangea is a smart phone app that looks and feels like a chat app, the back-end (called Panthalass is an encrypted mesh network hooked up to the Rewarded for doing good, empowered to be a On Pangea, people are incentivis to be good citizens by receiving rewards for doing good, rather than being coerced by the threat of punishment for bad\u00a0behavi This platform would fulfil a vast and unmet need, particular in countries whose legal systems function poorly. On Pangea, a user voluntaril chooses which jurisdicti to be a part of. Contracts are then notarised, executed and arbitrated according to that jurisdicti Users voluntaril join a decentrali borderless voluntary nation (DBVN) and will receive tokens (Pangea Arbitratio Tokens) as a reward for good behaviour. The tokens will be tradable and will be used as payment on Pangea for notarisati and Combining a store of value and access to legal\u00a0serv Societies cannot escape their need to use currency as stores of value. They also cannot escape their need to create reliable and enforceabl agreements (contracts with each\u00a0other Generating Bitcoin through proof of work occurs because individual believe that Bitcoin will be continuous used \u2014 that it will meet an ongoing need to transact using a decentrali and Generating PAT by being a good citizen will occur because individual believe that Pangea will be continuous used \u2014 that it will meet an ongoing need to create enforceabl agreements using a voluntary and geographic agnostic (and Comments There is much still to say about the Pangea platform and the mechanisms which will make it function. Please give feedback and ask questions in the comments. To find out more, visit the website."},{"title":"Bitcoin compared to\u00a0gold","url":"bitcoin-vs-gold.html","body":"A safe haven asset is something to buy during economic uncertaint Historical the safest asset you can buy has been gold. This is not because of anything inherently special about gold, but because that is what people believe to be the best long term method of storing\u00a0va People believe gold is special because they assume that in future other people will believe it\u2019s\u00a0speci Criteria for a safe haven\u00a0asse A safe-haven asset must fulfil the Price isn\u2019t controlled by any single party, including a state or bank. The market is spread out beyond the reach of any one organisati This is important because an asset which is issued, controlled or backed by an organisati has its value tied to the health of Supply isn\u2019t controlled by any single party, including a state, bank or anyone else - it exists naturally and the rate at which it\u2019s produced or traded is beyond the control of any Supply is limited. The effort required to create the asset naturally limits the\u00a0supply The asset doesn\u2019t wear out or\u00a0expire. It\u2019s prohibitiv expensive to\u00a0fake. Almost everyone considers it to be precious and\u00a0valuab It can be stored and transporte simply. It\u2019s not delicate or\u00a0volatil For these reasons, and because of historical consensus, people have been happy to use gold as a store of value in times of economic uncertaint or for long durations. Other assets also meets these requiremen to Bitcoin compared to\u00a0gold Consider why gold is so good as a safe haven asset and long term value store. For all the reasons above, bitcoin is better, except one: At present, not many people consider it to be precious and valuable, so the market is small. This will change as confidence and awareness increases, and the eco-system of services and The fundamenta are A decentrali network ensures that Bitcoin can\u2019t be regulated or manipulate by any single government or organisati The Bitcoin network can\u2019t be turned\u00a0off The present and future rate of supply is publicly available and unchangeab This increases market efficiency and creates more rational pricing than a market where the rate of supply Supply is naturally limited using proof of Bitcoin doesn\u2019t corrode or wear\u00a0out. Bitcoin is impossible to\u00a0fake. Bitcoin can be stored and transporte more easily than gold. - If you can remember 24 words then you can access your bitcoin for free from any Read\u00a0more This article, published a week after I wrote this post, looks at different factors to consider when evaluating Bitcoin\u2019s value. It goes into a lot of detail, relative to what I\u2019ve seen in"},{"title":"Hardware\u00a0Wallets","url":"wallet.html","body":"What is a A hardware wallet (HW wallet) is a physical device that stores the informatio required to access digital currency or assets. It is plugged into a computer via USB in order to initiate or confirm transactio on the Bitcoin, Ethereum or other digital asset They are a secure method of storing cryptograp data. They are so secure that they can be used on a compromise computer. All that is needed to access funds using a HW wallet (in addition to the device itself) is a PIN code which the user chooses. A single HW wallet can store multiple currencies in HW wallets are an easier solution than rememberin a good password, and safer than storing the data in a file on my computer or\u00a0online. The best known hardware wallet brands are Ledger and Trezor. The\u00a0proble HW wallets are technicall great, but their size and shape creates a bad user experience A good hardware wallet should be convenient to use multiple times each day, like a credit card\u00a0is. Current hardware wallets don\u2019t fit into a (money) wallet and people don\u2019t want to carry more any objects in their pockets. They are too big and are a bad\u00a0shape. HW wallets look like they might belong on a keyring, but it\u2019s inconvenie and insecure to attach a credit card to a keyring and the same is true for an HW wallet. I might store my keys on a hook by my door, but I would never leave my wallet there overnight. I often want to keep my keys and money separate because I need my keys when I\u2019m near my house, where I don\u2019t need to buy stuff, and I need my money when I\u2019m away from my house where I don\u2019t need to I want to keep my bank cards and cash together in one safe place, and I don\u2019t want to carry around a dongle as well. It\u2019s easier to have a separate dongle lost or stolen than something that would fit next to my credit card in my wallet. The inconvenie is a barrier to enjoying the advantages of The\u00a0goal Create a HW wallet that is the size and shape of a credit card, it could be 3 times thicker than a credit card and still fit in a normal wallet. It\u00a0needs a display - it could be a low resolution b&w\u00a0displa two or more\u00a0butto to plug into a USB\u00a0port securely sign transactio and"},{"title":"Trading digital\u00a0assets","url":"algo-trading.html","body":"Table of and import data3\u00a0\u00a0For data4\u00a0\u00a0Ass - \u00a34.2\u00a0\u00a0Ethe - \u00a34.3\u00a0\u00a0Ethe - - \u00a34.5\u00a0\u00a0Lite - BTC5\u00a0\u00a0SMA gains w/ different SMA through time for one combinatio of sma1 and sma26\u00a0\u00a0Nex steps: In\u00a0[1]: from import HTML function code_toggl { if (code_show } else { } code_show =! code_show } $( document </script> <font> This analysis was made using Python. If you'd like to see the code used, click <a ''') Out[1]: function code_toggl { if (code_show } else { } code_show =! code_show } $( document This analysis was made using Python. If you\u2019d like to see the code used, click here. window.onl = function() { code_toggl }; notebook shows the process of investigat the price history of Bitcoin, Ethereum and Litecoin using Simple Moving I noticed that the 7 day and 30 day SMAs would cross each other occasional and I wondered how profitable it would be to use this as a I generate a heat map to show how profitabil varies across different pairs of\u00a0SMAs. For a given SMA pair I show the trading algorithms performanc between two\u00a0dates. Setup and import data\u00b6Setup involves importing the python packages required and changing the default notebook settings. I use Plotly figures rather than a simpler method of visualisin data and whilst creating the notebook I use the offline Plotly options. If I use other visualisat packages I\u2019ll set figures to appear below the code cell that called the plot\u00a0comma The price data is downloaded from Quandl. In order to keep my Quandl and Plot.ly credential private, I keep my account credential in a separate .py\u00a0file. The Pickle package and get_data() functions are used to download Quandl data only once and then store it locally in a .pkl file. This is quicker than downloadin it every time I (re)run the\u00a0notebo In\u00a0[2]: ## Setup - libraries %matplotli inline import os import pickle import quandl import as plt import pandas as pd import datetime as dt import numpy as np import credential # keep my quandl and plot.ly api keys private import plotly #import plotly.off as py import plotly.plo as py import as go import as ff #from import Axes3D exports, module) {/** * plotly.js v1.28.3 * Copyright 2012-2017, Plotly, Inc. * All rights reserved. * Licensed under the MIT license */ t;return function a(o,!0);va u=new Error(\"Can find module i(t,e){ret t.y-e.y}va r(t){retur r(t){retur u(){functi t(t,e){ret e(t,e){ret c(t){retur h(t){retur t.value}va t(t){var \"+s+\",\"+c+ \"+o+\",\"+u+ e=.5;retur n(t){var n=a(t,new e?e:1,r=r| \";var n(t,e){for r=new instanceof function y(t){var e}function b(t,e,r,n) e)throw new argument must be a expected unwanted i}var r=new t};var e=[];for(v r in n(t){for(v n(t){retur n(t){var i=new e=t|t-1;re new i}function l(t){for(v e=new e}function ffffffff ffffffff ffffffff ffffffff ffffffff fffffffe ffffffff ffffffff ffffffff 00000000 00000000 ffffffff ffffffff fffffffe ffffffff t){var must be greater than t instanceof can only safely store up to 53 n(void array length 26;var e=t,r=0;re 0;for(var t&&t>=0);v t&&t>=0);v new a(1);for(v t&&t>=0);v works only with positive a(0),mod:n a(0)};var i,o,s;retu i=new a(1),o=new a(0),s=new a(0),l=new i=new a(1),o=new f;return A[t];var p;else m;else new Error(\"Unk prime \"+t);e=new g}return works only with works only with red works only with works only with red l}}}functi s(t){retur l(t,e){ret 1:return s(t);case 3:return new Invalid n(t,e,r){v \"+r);var new n(t,e){var i=\"for(var i=n[t];ret var P=C+1;PZ)t new typed array length\");v e=new e)throw new Error(\"If encoding is specified then the first argument must be a l(t)}retur t)throw new argument must not be a t instanceof t)throw new argument must be a new to allocate Buffer larger than maximum size: bytes\");re 0|t}functi instanceof 0;for(var void 0:return v(t,e,r){v n=!1;if((v new hex E(n)}funct E(t){var new to access beyond buffer new argument must be a Buffer new out of new out of a}function U(t){for(v a}function H(t){retur i}function Y(t){retur t!==t}var t=new browser lacks typed array (Uint8Arra support which is required by `buffer` v5.x. Use `buffer` v4.x if you require old browser new must be 0;for(var ... new must be a new of range 0;for(var 0)}var new to write outside buffer new encoding: new out of new Error(\"Inv string. Length must be a multiple of i(t){retur a(t){var o(t){retur i(t,e){ret a(t,e){for i(g,d,v,h) n(t){var i(t,e){for r=new s}function m(t,e,r){v i=new n(t){var strict\";va t;var new Error(f+\" map requires nshades to be at least size i(t,e,r,i) 0}return n(t,e){ret t-e}functi i(t,e){var 0:return 0;case 1:return t[0]-e[0]; 2:return 3:var i;var 4:var n(t){var t}function i(t){retur a(t){retur o(t){retur null}var null;var a}return a}return i(t){var e=new i=0;i0)thr new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array args\")}els new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array index\")}el new Error(\"cwi Too many arguments in pre() new Error(\"cwi Too many arguments in body() new Error(\"cwi Too many arguments in post() block\");re n(t,e,r){v l(t,e){for w=new cwise routine for new return n(t){var e=[\"'use strict'\",\" function (!(\"+l.joi && \")+\")) throw new Error('cwi Arrays do not all have the same {\"),e.push (!(\"+u.joi && \")+\")) throw new Error('cwi Arrays do not all have the same i(t,e,r){v m,v=new t;var e=[];for(v r in e=[];for(v r in e=[];for(v r in n&&void e(t,e){var n in r}function r(){}funct n(t){var e;return y(t){retur b(t){retur new Error(\"unk type: i(t,e){for r,n,i=new i(){if(s){ t(t){var r(t){var Array(o),l this;var 1:do{o=new 2:do{o=new 3:do{o=new t=[];retur s(){var l(){for(va a(t){retur n}}}functi l(t){retur u(t){for(v e}function c(t,e){for r in p(t){retur f(t)in this._&&de v(){var t=[];for(v e in t}function g(){var t=0;for(va e in t}function y(){for(va t in x(t){retur t}function function() w(t,e){if( in t)return Z(t,e){ret J(t,e){var K(t){var vt(t){retu gt(t){retu yt(t){retu _t(t){retu wt(t){retu kt(t,e,r){ Ot(){for(v t}function Ft(){for(v Nt(t){var b=u&&h;ret Bt(t){retu t+\"\"}funct n(e){var in e}function Gt(t,e,r){ le(t){var ce(t){for( ge(t){var ye(t,e){re we(t){var ke(t,e){re y}}functio Fe(t){retu Re(){var r=e;return r(t){var a(t,e){ret o(t,e){var l(t){for(v c(i,a){ret Je(){funct s}function 0 1,1 0 1,1 $e(){funct t(t,n){var er(){funct t(t,e){var rr(t){func s}function nr(t){func r(e){retur n(e){funct a(r,n){var k}function ir(t){var sr(t){retu t})()}func lr(t){func e(t){retur i(){return ur(t){retu r(t,e){var Vr(t,e){va n;var s;var Hr(t,e){va Vr(r,e);va Gr(t){for( r}function wn(t,e){va kn(t){retu An(t){retu 1;var Zn(t,e){va n}function bi(t){retu xi(t,e){re _i(t,e){re Ei(t){func Ni(t){retu t.y})}func Bi(t){retu Ui(t){var Vi(t){var qi(t,e){va a(t){retur o(t)}var o,s;return Qi(t,e){re $i(t,e){re a(t){retur o(e){retur t(i(e))}re _a(t){func e(e){funct Ma(t){retu ka(t){for( Aa(t){for( p[n]:delet t[r],1}var io(t){retu n(e){retur t(e)}funct i(t,r){var r};var t;var e=new b;if(t)for h(){functi f(){functi t(){var in r(){var n(){var o;if(i)ret i=!1,a;var e=new ms={\"-\":\"\" %b %e %X this.s}};v bs=new e(e,r){var t(){var e(){return }var new t(e,r,n,i) c}function e(t){for(v r}function t(t,a){var t(t,e){for i(t,e,r,n) \"+e}functi 0,0 \"+n}var t(t,i){var \"+l[2]+\" \"+l[3]}var 0,\"+e+\" \"+e+\",\"+e+ a(){functi v(){var l;var t;e||(e=t) e}function s(t){var l(t,e,r,n) u(t,e,r){v n=t;do{var n}function l=t;do{for h(t,e,r,n) r}function m(t,e,r,n) v(t){var t}function x(t,e){ret w(t,e){ret k(t,e){var A(t,e){ret n(t,e){var e){e=0;for warning: possible EventEmitt memory leak detected. %d listeners added. Use to increase must be a function\") n=!1;retur must be a e=typeof o(t,e,r,n) \"+i+\"=== typeof s(t,e){ret e.length> 1; if (a[m] === v) return true; if (a[m] > v) j = m - 1; else i = m + 1;}return false; }(\"+n+\", u(t){retur in p\"}functio h(t,e){ret c[1]){var s[e][t];va i(t){retur new a(t,e){ret new r}var 0)}functio d(t){for(v m(t){retur new t=[];retur t=[];retur 1:return 2:return new new new new this.tree; e=new i=0;i0)ret new Error(\"Can update empty node!\");va r=new new s(t){for(v z%d-%d-%d (features: %d, points: %d, simplified down to parent tile down\");var i(t,e,r){v s}function i(t,e,r,n) s(t,e){var r=new i(t);retur e(e,r,n){i in t){var U=g,V=_,k= 0.0) {\\n vec3 nPosition = mix(bounds bounds[1], 0.5 * (position + 1.0));\\n gl_Positio = projection * view * model * 1.0);\\n } else {\\n gl_Positio = }\\n colorChann = mediump GLSLIFY 1\\n\\nunifo vec4 vec3 main() {\\n gl_FragCol = colorChann * colors[0] + \\n colorChann * colors[1] +\\n colorChann * vectorizin d=new o(t,e,r,n) s;var r}function a(t,e){for r=0;rr)thr new If resizing buffer, must not specify a(t,e){for new Invalid type for webgl buffer, must be either or new Invalid usage for buffer, must be either gl.STATIC_ or t&&void new Cannot specify offset when resizing new Error(\"gl- Can't resize FBO, invalid new Error(\"gl- Parameters are too large for new Error(\"gl- Multiple draw buffer extension not new Error(\"gl- Context does not support \"+s+\" draw buffers\")} new Error(\"gl- Context does not support floating point h=!0;\"dept new Error(\"gl- Shape vector must be length 2\");var null;var 0.25) {\\n discard;\\n }\\n gl_FragCol = highp GLSLIFY 1\\n\\nattri vec2 aHi, aLo, vec4 pick0, vec2 scaleHi, translateH scaleLo, translateL float vec4 pickA, scHi, vec2 trHi, vec2 scLo, vec2 trLo, vec2 posHi, vec2 posLo) {\\n return (posHi + trHi) * scHi\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * main() {\\n vec2 p = translateH scaleLo, translateL aHi, aLo);\\n vec2 n = width * * vec2(dHi.y -dHi.x)) / gl_Positio = vec4(p + n, 0, 1);\\n pickA = pick0;\\n pickB = mediump GLSLIFY 1\\n\\nunifo vec4 vec4 pickA, pickB;\\n\\n main() {\\n vec4 fragId = 0.0);\\n if(pickB.w > pickA.w) {\\n fragId.xyz = pickB.xyz; }\\n\\n fragId += fragId.y += floor(frag / 256.0);\\n fragId.x -= floor(frag / 256.0) * 256.0;\\n\\n fragId.z += floor(frag / 256.0);\\n fragId.y -= floor(frag / 256.0) * 256.0;\\n\\n fragId.w += floor(frag / 256.0);\\n fragId.z -= floor(frag / 256.0) * 256.0;\\n\\n gl_FragCol = fragId / highp GLSLIFY 1\\n\\nattri vec2 aHi, aLo, vec2 scaleHi, translateH scaleLo, translateL float projectVal scHi, vec2 trHi, vec2 scLo, vec2 trLo, vec2 posHi, vec2 posLo) {\\n return (posHi + trHi) * scHi\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * main() {\\n vec2 p = translateH scaleLo, translateL aHi, aLo);\\n if(dHi.y e+n;var null;var FLOAT_MAX) {\\n return vec4(127.0 128.0, 0.0, 0.0) / 255.0;\\n } else if(v \"+t[1]+\", \"+t[2]+\", t=new e=new r=new \"+t[1]+\", n=\"precisi mediump GLSLIFY 1\\n\\nunifo vec3 float vec3 vec4 f_id;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n gl_FragCol = vec4(pickI mediump GLSLIFY 1\\n\\nattri vec3 position, vec4 vec2 uv;\\n\\nuni mat4 model\\n , view\\n , vec3 eyePositio , vec3 f_normal\\n , , , vec4 vec2 f_uv;\\n\\nv main() {\\n vec4 m_position = model * vec4(posit 1.0);\\n vec4 t_position = view * m_position gl_Positio = projection * t_position f_color = color;\\n f_normal = normal;\\n f_data = position;\\ f_eyeDirec = eyePositio - position;\\ = lightPosit - position;\\ f_uv = mediump GLSLIFY 1\\n\\nfloat x, float roughness) {\\n float NdotH = max(x, 0.0001);\\n float cos2Alpha = NdotH * NdotH;\\n float tan2Alpha = (cos2Alpha - 1.0) / cos2Alpha; float roughness2 = roughness * roughness; float denom = * roughness2 * cos2Alpha * cos2Alpha; return exp(tan2Al / roughness2 / vec3 vec3 vec3 float roughness, float fresnel) {\\n\\n float VdotN = 0.0);\\n float LdotN = 0.0);\\n\\n //Half angle vector\\n vec3 H = + //Geometri term\\n float NdotH = H), 0.0);\\n float VdotH = H), 0.000001); float LdotH = H), 0.000001); float G1 = (2.0 * NdotH * VdotN) / VdotH;\\n float G2 = (2.0 * NdotH * LdotN) / LdotH;\\n float G = min(1.0, min(G1, G2));\\n \\n //Distribu term\\n float D = //Fresnel term\\n float F = pow(1.0 - VdotN, fresnel);\\ //Multiply terms and done\\n return G * F * D / max(3.1415 * VdotN, vec3 float roughness\\ , fresnel\\n , kambient\\n , kdiffuse\\n , kspecular\\ , sampler2D vec3 f_normal\\n , , , vec4 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n\\n vec3 N = vec3 L = vec3 V = \\n {\\n N = -N;\\n }\\n\\n float specular = V, N, roughness, fresnel);\\ float diffuse = min(kambie + kdiffuse * max(dot(N, L), 0.0), 1.0);\\n\\n vec4 surfaceCol = f_color * f_uv);\\n vec4 litColor = surfaceCol * vec4(diffu * + kspecular * vec3(1,1,1 * specular, 1.0);\\n\\n gl_FragCol = litColor * mediump GLSLIFY 1\\n\\nattri vec3 vec4 vec2 uv;\\n\\nuni mat4 model, view, vec4 vec3 vec2 f_uv;\\n\\nv main() {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n f_color = color;\\n f_data = position;\\ f_uv = mediump GLSLIFY 1\\n\\nunifo vec3 sampler2D float vec4 vec3 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n\\n gl_FragCol = f_color * f_uv) * mediump GLSLIFY 1\\n\\nattri vec3 vec4 vec2 uv;\\nattri float mat4 model, view, vec3 vec4 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n gl_Positio = } else {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n }\\n gl_PointSi = pointSize; f_color = color;\\n f_uv = mediump GLSLIFY 1\\n\\nunifo sampler2D float vec4 vec2 f_uv;\\n\\nv main() {\\n vec2 pointR = - if(dot(poi pointR) > 0.25) {\\n discard;\\n }\\n gl_FragCol = f_color * f_uv) * mediump GLSLIFY 1\\n\\nattri vec3 vec4 id;\\n\\nuni mat4 model, view, vec3 vec4 f_id;\\n\\nv main() {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n f_id = id;\\n f_position = mediump GLSLIFY 1\\n\\nattri vec3 float vec4 id;\\n\\nuni mat4 model, view, vec3 vec3 vec4 f_id;\\n\\nv main() {\\n || \\n {\\n gl_Positio = } else {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n gl_PointSi = pointSize; }\\n f_id = id;\\n f_position = mediump GLSLIFY 1\\n\\nattri vec3 mat4 model, view, main() {\\n gl_Positio = projection * view * model * vec4(posit mediump GLSLIFY 1\\n\\nunifo vec3 main() {\\n gl_FragCol = i(t){for(v null;for(v function() E=new new s(\"\",\"Inva data type for attribute \"+h+\": new s(\"\",\"Unkn data type for attribute \"+h+\": \"+f);var new s(\"\",\"Inva data type for attribute \"+h+\": n(t){retur new i(t,e){for r=new new s(\"\",\"Inva uniform dimension type for matrix \"+name+\": new s(\"\",\"Unkn uniform data type for \"+name+\": \"+r)}var new s(\"\",\"Inva data new data type for vector \"+name+\": r=[];for(v n in e){var r}function h(e){for(v n=[\"return function new s(\"\",\"Inva data new s(\"\",\"Inva uniform dimension type for matrix \"+name+\": \"+t);retur i(r*r,0)}t new s(\"\",\"Unkn uniform data type for \"+name+\": \"+t)}}func i){var p(t){var r=0;r1){l[ u=1;u1)for l=0;l=0){v t||t}funct s(t){funct r(){for(va u=0;u 1.0) {\\n discard;\\n }\\n baseColor = color, step(radiu gl_FragCol = * baseColor. mediump GLSLIFY 1\\n\\nattri vec2 vec4 mat3 float vec4 vec4 main() {\\n vec3 hgPosition = matrix * vec3(posit 1);\\n gl_Positio = 0, gl_PointSi = pointSize; vec4 id = pickId + pickOffset id.y += floor(id.x / 256.0);\\n id.x -= floor(id.x / 256.0) * 256.0;\\n\\n id.z += floor(id.y / 256.0);\\n id.y -= floor(id.y / 256.0) * 256.0;\\n\\n id.w += floor(id.z / 256.0);\\n id.z -= floor(id.z / 256.0) * 256.0;\\n\\n fragId = mediump GLSLIFY 1\\n\\nvaryi vec4 main() {\\n float radius = length(2.0 * - 1.0);\\n if(radius > 1.0) {\\n discard;\\n }\\n gl_FragCol = fragId / i(t,e){var instanceof instanceof null;var n(t,e,r,n) highp GLSLIFY 1\\n\\n\\nvec posHi, vec2 posLo, vec2 scHi, vec2 scLo, vec2 trHi, vec2 trLo) {\\n return vec4((posH + trHi) * scHi\\n \\t\\t\\t//FI this thingy does not give noticeable precision gain, need test\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * scLo\\n , 0, vec2 positionHi float size, vec2 char, is 64-bit form of scale and vec2 scaleHi, scaleLo, translateH float vec4 sampler2D vec4 charColor, vec2 vec2 float float main() {\\n charColor = vec2(color / 255., 0));\\n borderColo = vec2(color / 255., 0));\\n\\n gl_PointSi = size * pixelRatio pointSize = size * charId = char;\\n borderWidt = border;\\n\\ gl_Positio = positionHi positionLo scaleHi, scaleLo,\\n translateH pointCoord = viewBox.xy + (viewBox.z - viewBox.xy * * .5 + highp GLSLIFY 1\\n\\nunifo sampler2D vec2 float charsStep, pixelRatio vec4 vec4 vec2 vec2 float float main() {\\n\\tvec2 pointUV = (pointCoor - + pointSize * .5) / = 1. - texCoord = ((charId + pointUV) * charsStep) / dist = alpha\\n\\ti (dist t;){var w.push(new i(){var a(t,e){var e=void null;var number of characters is more than maximum texture size. Try reducing x=0;x 1.0) {\\n discard;\\n }\\n vec4 baseColor = color, float alpha = 1.0 - pow(1.0 - baseColor. fragWeight gl_FragCol = * alpha, highp GLSLIFY 1\\n\\nvec4 pfx_1_0(ve scaleHi, vec2 scaleLo, vec2 translateH vec2 translateL vec2 positionHi vec2 positionLo {\\n return + translateH * scaleHi\\n + (positionL + translateL * scaleHi\\n + (positionH + translateH * scaleLo\\n + (positionL + translateL * scaleLo, 0.0, vec2 positionHi vec4 vec2 scaleHi, scaleLo, translateH float vec4 vec4 main() {\\n\\n vec4 id = pickId + pickOffset id.y += floor(id.x / 256.0);\\n id.x -= floor(id.x / 256.0) * 256.0;\\n\\n id.z += floor(id.y / 256.0);\\n id.y -= floor(id.y / 256.0) * 256.0;\\n\\n id.w += floor(id.z / 256.0);\\n id.z -= floor(id.z / 256.0) * 256.0;\\n\\n gl_Positio = scaleLo, translateH translateL positionHi positionLo gl_PointSi = pointSize; fragId = mediump GLSLIFY 1\\n\\nvaryi vec4 main() {\\n float radius = length(2.0 * - 1.0);\\n if(radius > 1.0) {\\n discard;\\n }\\n gl_FragCol = fragId / i(t,e){var e(e,r){ret e in n(t,e){var in r)return r[t];for(v o=r.gl d(t){var null;var a(t,e){ret new E=new i(t,e){var r=new n(t);retur 0.0 ||\\n || {\\n discard;\\n }\\n\\n vec3 N = vec3 V = vec3 L = {\\n N = -N;\\n }\\n\\n float specular = V, N, roughness) float diffuse = min(kambie + kdiffuse * max(dot(N, L), 0.0), 1.0);\\n\\n //decide how to interpolat color \\u2014 in vertex or in fragment\\n vec4 surfaceCol = .5) * vec2(value value)) + step(.5, vertexColo * vColor;\\n\\ vec4 litColor = surfaceCol * vec4(diffu * + kspecular * vec3(1,1,1 * specular, 1.0);\\n\\n gl_FragCol = mix(litCol contourCol contourTin * mediump GLSLIFY 1\\n\\nattri vec4 uv;\\nattri float f;\\n\\nunif mat3 mat4 model, view, float height, sampler2D float value, kill;\\nvar vec3 vec2 vec3 eyeDirecti vec4 main() {\\n vec3 dataCoordi = permutatio * vec3(uv.xy height);\\n vec4 worldPosit = model * 1.0);\\n\\n vec4 clipPositi = projection * view * clipPositi = clipPositi + zOffset;\\n gl_Positio = value = f;\\n kill = -1.0;\\n = = uv.zw;\\n\\n vColor = vec2(value value));\\n //Don't do lighting for contours\\n surfaceNor = vec3(1,0,0 eyeDirecti = vec3(0,1,0 lightDirec = mediump GLSLIFY 1\\n\\nunifo vec2 vec3 float float value, kill;\\nvar vec3 vec2 vec3 v) {\\n float vh = 255.0 * v;\\n float upper = floor(vh); float lower = fract(vh); return vec2(upper / 255.0, floor(lowe * 16.0) / main() {\\n if(kill > 0.0 ||\\n || {\\n discard;\\n }\\n vec2 ux = / shape.x);\\ vec2 uy = / shape.y);\\ gl_FragCol = vec4(pickI ux.x, uy.x, ux.y + i(t){var o(t,e){var new invalid coordinate for new Invalid texture size\");ret s(t,e){ret new Invalid ndarray, must be 2d or 3d\");var new Invalid shape for new Invalid shape for pixel new Incompatib texture format for new Invalid texture new Floating point textures not supported on this platform\") s=u(t);ret s=u(t);ret f(t,e){var new Invalid texture size\");var new Invalid shape for new Invalid shape for pixel b=u(t);ret new Error(\"gl- Too many vertex n(t,e,r){v i=new n(t){for(v n(t,e){var n(t,e,r){v instanceof a=new a(t,e){ret o(t){for(v e=[\"functi orient(){v orient\");v n=new a(t,e){var o(t,e){var s(t,e){var i}}functio c(t,e){for s(this,t); s(this,t); b}for(var r}return n}return l}function i(t,e,r,n) n(t,e){var r;if(h(t)) new Error('Unk function type -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n\\n // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def float vec2 vec2 vec2 vec2 vec2 vec2 float float sampler2D vec2 vec2 float float main() {\\n // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line or when fading out\\n // float blur = u_blur * float alpha = clamp(min( - (v_linewid - blur), v_linewidt - dist) / blur, 0.0, 1.0);\\n\\n float x_a = / 1.0);\\n float x_b = / 1.0);\\n float y_a = 0.5 + (v_normal. * v_linewidt / float y_b = 0.5 + (v_normal. * v_linewidt / vec2 pos_a = vec2(x_a, y_a));\\n vec2 pos_b = vec2(x_b, y_b));\\n\\n vec4 color = pos_a), pos_b), u_fade);\\n alpha *= u_opacity; gl_FragCol = color * gl_FragCol = highp lowp\\n#def floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the vec2 vec4 mat4 mediump float mediump float mediump float mediump float mediump float mat2 mediump float vec2 vec2 float float main() {\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * // We store the texture normals in the most insignific bit\\n // transform y so that 0 => -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n v_linesofa = // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def lowp vec4 lowp float float sampler2D float float vec2 vec2 vec2 vec2 float main() {\\n // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line or when fading out\\n // float blur = u_blur * float alpha = clamp(min( - (v_linewid - blur), v_linewidt - dist) / blur, 0.0, 1.0);\\n\\n float sdfdist_a = v_tex_a).a float sdfdist_b = v_tex_b).a float sdfdist = mix(sdfdis sdfdist_b, u_mix);\\n alpha *= smoothstep - u_sdfgamma 0.5 + u_sdfgamma sdfdist);\\ gl_FragCol = u_color * (alpha * gl_FragCol = highp lowp\\n#def floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the vec2 vec4 mat4 mediump float mediump float mediump float mediump float vec2 float vec2 float float mat2 mediump float vec2 vec2 vec2 vec2 float main() {\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * // We store the texture normals in the most insignific bit\\n // transform y so that 0 => -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n\\n v_tex_a = * normal.y * + u_tex_y_a) v_tex_b = * normal.y * + // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def mapbox: define lowp vec4 mapbox: define lowp float vec2 v_pos;\\n\\n main() {\\n #pragma mapbox: initialize lowp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ float dist = length(v_p - float alpha = 0.0, dist);\\n gl_FragCol = outline_co * (alpha * gl_FragCol = highp lowp\\n#def vec2 mat4 vec2 vec2 mapbox: define lowp vec4 mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n v_pos = / gl_Positio + 1.0) / 2.0 * mediump lowp\\n#def float vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 vec2 v_pos;\\n\\n main() {\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = vec4 color2 = pos2);\\n\\n // find distance to outline for alpha float dist = length(v_p - float alpha = 0.0, dist);\\n \\n\\n gl_FragCol = mix(color1 color2, u_mix) * alpha * gl_FragCol = highp lowp\\n#def vec2 vec2 vec2 vec2 float float float vec2 mat4 vec2 vec2 vec2 vec2 v_pos;\\n\\n main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n vec2 scaled_siz = u_scale_a * vec2 scaled_siz = u_scale_b * // the correct offset needs to be calculated //\\n // The offset depends on how many pixels are between the world origin and\\n // the edge of the tile:\\n // vec2 offset = size)\\n //\\n // At high zoom levels there are a ton of pixels between the world origin\\n // and the edge of the tile. The glsl spec only guarantees 16 bits of\\n // precision for highp floats. We need more than that.\\n //\\n // The pixel_coor is passed in as two 16 bit values:\\n // = / 2^16)\\n // = 2^16)\\n //\\n // The offset is calculated in a series of steps that should preserve this precision: vec2 offset_a = scaled_siz * 256.0, scaled_siz * 256.0 + vec2 offset_b = scaled_siz * 256.0, scaled_siz * 256.0 + v_pos_a = * a_pos + offset_a) / v_pos_b = * a_pos + offset_b) / v_pos = / gl_Positio + 1.0) / 2.0 * mediump lowp\\n#def float vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 main() {\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = vec4 color2 = pos2);\\n\\n gl_FragCol = mix(color1 color2, u_mix) * gl_FragCol = highp lowp\\n#def mat4 vec2 vec2 vec2 vec2 float float float vec2 vec2 vec2 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n vec2 scaled_siz = u_scale_a * vec2 scaled_siz = u_scale_b * // the correct offset needs to be calculated //\\n // The offset depends on how many pixels are between the world origin and\\n // the edge of the tile:\\n // vec2 offset = size)\\n //\\n // At high zoom levels there are a ton of pixels between the world origin\\n // and the edge of the tile. The glsl spec only guarantees 16 bits of\\n // precision for highp floats. We need more than that.\\n //\\n // The pixel_coor is passed in as two 16 bit values:\\n // = / 2^16)\\n // = 2^16)\\n //\\n // The offset is calculated in a series of steps that should preserve this precision: vec2 offset_a = scaled_siz * 256.0, scaled_siz * 256.0 + vec2 offset_b = scaled_siz * 256.0, scaled_siz * 256.0 + v_pos_a = * a_pos + offset_a) / v_pos_b = * a_pos + offset_b) / mediump lowp\\n#def float float sampler2D sampler2D vec2 vec2 float float float float vec3 main() {\\n\\n // read and cross-fade colors from the main and parent tiles\\n vec4 color0 = v_pos0);\\n vec4 color1 = v_pos1);\\n vec4 color = color0 * u_opacity0 + color1 * u_opacity1 vec3 rgb = color.rgb; // spin\\n rgb = vec3(\\n dot(rgb, dot(rgb, dot(rgb, // saturation float average = (color.r + color.g + color.b) / 3.0;\\n rgb += (average - rgb) * // contrast\\n rgb = (rgb - 0.5) * + 0.5;\\n\\n // brightness vec3 u_high_vec = vec3 u_low_vec = gl_FragCol = u_low_vec, rgb), gl_FragCol = highp lowp\\n#def mat4 vec2 float float vec2 vec2 vec2 vec2 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n v_pos0 = / 32767.0) - 0.5) / u_buffer_s ) + 0.5;\\n v_pos1 = (v_pos0 * + mediump lowp\\n#def sampler2D sampler2D lowp float vec2 vec2 main() {\\n lowp float alpha = v_fade_tex * u_opacity; gl_FragCol = v_tex) * gl_FragCol = highp lowp\\n#def vec2 vec2 vec2 vec4 matrix is for the vertex mat4 mediump float bool vec2 vec2 vec2 vec2 main() {\\n vec2 a_tex = mediump float a_labelmin = a_data[0]; mediump vec2 a_zoom = a_data.pq; mediump float a_minzoom = a_zoom[0]; mediump float a_maxzoom = a_zoom[1]; // u_zoom is the current zoom level adjusted for the change in font size\\n mediump float z = 2.0 - u_zoom) - (1.0 - u_zoom));\\ vec2 extrude = * (a_offset / 64.0);\\n if {\\n gl_Positio = u_matrix * vec4(a_pos + extrude, 0, 1);\\n gl_Positio += z * } else {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n }\\n\\n v_tex = a_tex / u_texsize; v_fade_tex = / 255.0, mediump lowp\\n#def sampler2D sampler2D lowp vec4 lowp float lowp float lowp float vec2 vec2 float main() {\\n lowp float dist = v_tex).a;\\ lowp float fade_alpha = lowp float gamma = u_gamma * lowp float alpha = - gamma, u_buffer + gamma, dist) * gl_FragCol = u_color * (alpha * gl_FragCol = highp lowp\\n#def float PI = vec2 vec2 vec2 vec4 matrix is for the vertex mat4 mediump float bool bool mediump float mediump float mediump float vec2 vec2 vec2 vec2 float main() {\\n vec2 a_tex = mediump float a_labelmin = a_data[0]; mediump vec2 a_zoom = a_data.pq; mediump float a_minzoom = a_zoom[0]; mediump float a_maxzoom = a_zoom[1]; // u_zoom is the current zoom level adjusted for the change in font size\\n mediump float z = 2.0 - u_zoom) - (1.0 - u_zoom));\\ // map\\n // map | viewport\\n if {\\n lowp float angle = ? (a_data[1] / 256.0 * 2.0 * PI) : u_bearing; lowp float asin = sin(angle) lowp float acos = cos(angle) mat2 RotationMa = mat2(acos, asin, -1.0 * asin, acos);\\n vec2 offset = RotationMa * a_offset;\\ vec2 extrude = * (offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos + extrude, 0, 1);\\n gl_Positio += z * // viewport\\n // map\\n } else if {\\n // foreshorte factor to apply on pitched maps\\n // as a label goes from horizontal vertical in angle\\n // it goes from 0% foreshorte to up to around 70% lowp float pitchfacto = 1.0 - cos(u_pitc * sin(u_pitc * 0.75));\\n\\ lowp float lineangle = a_data[1] / 256.0 * 2.0 * PI;\\n\\n // use the lineangle to position points a,b along the line\\n // project the points and calculate the label angle in projected space\\n // this calculatio allows labels to be rendered unskewed on pitched maps\\n vec4 a = u_matrix * vec4(a_pos 0, 1);\\n vec4 b = u_matrix * vec4(a_pos + 0, 1);\\n lowp float angle = - b[0]/b[3] - a[0]/a[3]) lowp float asin = sin(angle) lowp float acos = cos(angle) mat2 RotationMa = mat2(acos, -1.0 * asin, asin, acos);\\n\\n vec2 offset = RotationMa * 1.0) * a_offset); vec2 extrude = * (offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n gl_Positio += z * // viewport\\n // viewport\\n } else {\\n vec2 extrude = * (a_offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n }\\n\\n v_gamma_sc = (gl_Positi - 0.5);\\n\\n v_tex = a_tex / u_texsize; v_fade_tex = / 255.0, mediump lowp\\n#def float float float float main() {\\n\\n float alpha = 0.5;\\n\\n gl_FragCol = vec4(0.0, 1.0, 0.0, 1.0) * alpha;\\n\\n if > u_zoom) {\\n gl_FragCol = vec4(1.0, 0.0, 0.0, 1.0) * alpha;\\n }\\n\\n if (u_zoom >= v_max_zoom {\\n gl_FragCol = vec4(0.0, 0.0, 0.0, 1.0) * alpha * 0.25;\\n }\\n\\n if >= u_maxzoom) {\\n gl_FragCol = vec4(0.0, 0.0, 1.0, 1.0) * alpha * 0.2;\\n highp lowp\\n#def vec2 vec2 vec2 mat4 float float float main() {\\n gl_Positio = u_matrix * vec4(a_pos + a_extrude / u_scale, 0.0, 1.0);\\n\\n v_max_zoom = a_data.x;\\ = vec4 values, const float t) {\\n if (t 7)return[n have been deprecated as of v8\")];if(! in \"%s\" not strict\";va a(l,e,\"arr expected, %s a(l,e,\"arr length %d expected, length %d r?[new have been deprecated as of v8\")]:[];v n(e,r,\"obj expected, %s found\",a)] o=[];for(v s in must start with \"@\"'));ret strict\";va one of [%s], %s strict\";va t(e){var n(l,s,\"arr expected, %s n(l,s,'\"$t cannot be use with operator n(l,s,'fil array for operator \"%s\" must have 3 expected, %s key cannot be a functions not functions not strict\";va url must include a \"{fontstac url must include a \"{range}\" strict\";va n(c,r,'eit \"type\" or \"ref\" is i(e,r,\"%s is greater than the maximum value strict\";va n(e,r,\"obj expected, %s f in r){var property in n(e,r,'mis required property strict\";va i(e,o,'unk property strict\";va n(r,e,'\"ty is e)for(var c in a(t){retur Sans Unicode MS new new M=new in n){for(var many symbols being rendered in a tile. See many glyphs being rendered in a tile. See exceeds allowed extent, reduce your vector tile buffer size\")}ret new new Error(\"Inv LngLat object: (\"+t+\", new new x(){return y(){return point(){re new new new new instanceof 0===s&&voi a(void new Error(\"fai to invert strict\";va n={\" strict\";va s(t){retur l(t,e,r,n) o=(new out of n(t,e){ret mapbox: ([\\w]+) ([\\w]+) ([\\w]+) a=new n?e(new Error(\"Inp data is not a valid GeoJSON t.data)ret e(new Error(\"Inp data is not a valid GeoJSON e(new Error(\"Inp data is not a valid GeoJSON e=0;ee)){v y;for(y in in p)c[y]=!0; t in new new i(t,e,i){v r(t,r){ret delete e(t);var n=new o(new new e=new in tile source layer \"'+M+'\" does not use vector tile spec v2 and therefore may have some rendering g(t,L);var F in B in n=new t.time>=(n void void t=new new i;var strict\";va new Error(\"Inv color o[e]}throw new Error(\"Inv color void n in r in Error('Sou layer does not exist on source \"'+e.id+'\" as specified by style layer t in t.id});for new Error(\"Sty is not done new Error(\"The is no source with this ID\");var delete instanceof this;var 0===e)thro new Error(\"The is no layer with this ID\");for(v r in this;var void 0===i||voi 0===a?void strict\";va i(t){retur t.value}va r,n;for(va i in t){var in for(n in in in 0===e)dele 0===e)dele o}var strict\";va new t){var this.grid= a}if(r){va _=u;for(va a}}}return r=new r(\"glyphs > 65535 not i=!t&&new l(new c(new g(e,r){var y(e,r){var i(0,0));re M in a)t[M]=new strict\";va t){var | n(){}var i(t){retur new 61:case 107:case 171:case 189:case 109:case t=0,e=0;re t=new null!==t&& new Error(\"max must be between the current minZoom and 20, t,e={};ret t instanceof e;if(t instanceof instanceof c?t:new i(this,e); void Error(\"Fai to initialize s in if(void if(void n(t){var r=new n(t){for(v e=0;e1)for delete error c(t,e,r){v f(t,e){for t in null;var delete new Error(\"An API access token is required to use Mapbox GL. See new Error(\"Use a public access token (pk.*) with Mapbox GL JS, not a secret access token (sk.*). See t}function i(t){retur a(t){retur t;var n(t){funct v[n];void in t=0;t=1)re 1;var void t={};for(v e in =0.22.0 =0.22.0 No README data run build-docs # invoked by publisher when publishing docs on the mb-pages --debug --standalo mapboxgl > && tap --no-cover build --github --format html -c --theme ./docs/_th --output --debug -t unassertif --plugin [minifyify --map --output --standalo mapboxgl > && tap --no-cover --debug -t envify > --ignore-p .gitignore js test bench diff --name-onl mb-pages HEAD -- | awk '{print | xargs build-toke watch-dev watch-benc build-toke watch-benc build-toke watch-dev run build-min && npm run build-docs && jekyll serve --no-cache --localhos --port 9966 --index index.html .\",test:\"n run lint && tap --reporter dot test/js/*/ && node && watchify bench/inde --plugin [minifyify --no-map] -t [babelify --presets react] -t unassertif -t envify -o bench/benc --debug --standalo mapboxgl -o n=new r=new r(t){var n(t,n){var i(t){retur t)return t){var 1=0)return V=1;V specify vertex creation specify cell creation specify phase strict\";va n(t){if(t in l)return l[t];for(v new Invalid boundary dst;};retu t in l){var t in u){var t in c){var return \"+s),u){va p=new p=new p()}functi for(var o=0;o1)for f(e,r){var s=\"__l\"+ i=\"__l\"+ _=[\"'use L=new L=new L(r)}funct s(t,e){var r=[\"'use [2,1,0];}e [1,0,2];}} [2,0,1];}e new new function new o=new 0===t){var 0===r){r=n o(t,e){var s(t,e){ret a(t,e){var i=new t||\"up\"in strict\";va r=void 0!==r?r+\"\" e(t,e){for t}function o)throw new to path.resol must be t)throw new to path.join must be n(t){for(v new Error(\"Giv varint doesn't fit into 10 bytes\");va o(t,e,r){v s(t,e){for new type: void n(t){var 0:return r||[];case 1:return 2:return Array(t);v r}var r(t,e){var Array(a),n n(t,e){for a(t){for(v t-e});var new t instanceof i(t){retur a(t){for(v a=1;i;){va l(t){for(v c(t){retur d(t){var u(m)}funct p(t){var 0x80 (not a basic code x});else for(_ in n(t,e){ret o;var o};var n(t,e){for n&&void e(t){var e=new Error(\"(re \"+t);throw n(t){retur t?\": i(t,r,i){t in r||e(\"unkn parameter possible values: parameter type\"+n(r) must be a typed parameter type\"+n(i) expected \"+r+\", got \"+typeof t)}functio parameter type, must be a nonnegativ shader source must be a string\",a) number \"+t+\": r=0;e(c(\"| compiling \"+s+\" shader, linking program with vertex shader, and fragment shader i(t){retur M(t,r){var n=m();e(t+ in command called from \"+n))}func A(t,e,r,i) in e||M(\"unkn parameter possible values: parameter type\"+n(r) expected \"+e+\", got \"+typeof texture format for renderbuff format for L(t,e){ret z(t,e,n){v pixel arguments to document,\" manually specify webgl context outside of DOM not supported, try upgrading your browser or graphics drivers name must be string\");v $(t){var et(t,e){va _e:r=new we:r=new Me:r=new ke:r=new Ae:r=new Te:r=new Se:r=new null}retur n=0;n0){va t[0]){var buffer data\")}els shape\");va data for buffer p=new n(a);retur d=[];retur t=0;return t&&t._buff instanceof a(t){var e||(e=new Ge:case Xe:case Ze:case type for element bit element buffers not supported, enable first\");va vertex count for buffer a}var t&&t._elem instanceof pt(t){for( At(t){retu Tt(t,e){va Or:case Fr:case Rr:case jr:var texture type, must specify a typed St(t,e){re for(var s}return o*r*n}func texture texture unpack n){var must enable the extension in order to use floating point must enable the extension in order to use 16-bit floating point must enable the extension in order to use depth/sten texture must be an extension not extension not d(e,r,i){v m(){return K.pop()||n h}function y(t,e,r){v b(t,e){var e){var e){var e){var e){var e){var i(t,e){var arguments to format for c=new T(nr);retu format for C=new z=new I(){for(va for(var P={\"don't care\":$r,\" mipmap mipmap mipmap mipmap s3tc dxt1\":Mr,\" s3tc dxt1\":kr,\" s3tc dxt3\":Ar,\" s3tc atc\":Sr,\"r atc explicit atc interpolat pvrtc pvrtc pvrtc pvrtc etc1\"]=Pr) r=B[e];ret null});ret number of texture shape for z||\"colors render targets not color buffer must enable in order to use floating point framebuffe must enable in order to use 16-bit floating point framebuffe must enable to use 16-bit render must enable in order to use 32-bit floating point color color format for color format for extension not u=d=1;var for(D=new color attachment \"+a+\" is color attachment much have the same number of bits per depth attachment for framebuffe stencil attachment for framebuffe depth-sten attachment for framebuffe not resize a framebuffe which is currently in use\");var i;for(var shape for framebuffe must be be d||\"colors render targets not color buffer color color format for l=1;var a(t){var t=0;return vertex fragment shader\",n) a=i[t];ret a||(a=new o(o){var must create a webgl context with in order to read pixels from the drawing cannot read from a from a framebuffe is only allowed for the types 'uint8' and from a framebuffe is only allowed for the type 'uint8'\")) arguments to buffer for regl.read( too s(t){var r;return l(t){retur l}function jt(t){retu Nt(t){retu Bt(){funct t(t){for(v r(){functi n(){var e=a();retu n(){var new new m(t){retur v(t,e,r){v g(t,e,r){v y(){var ei:var ri:return ni:return ii:return ai:return c={};retur n=e.id(t); in c)return c[n];var b(t){var in r){var if(Di in n){var e}function x(t,e){var in r){var i=r[Pi];re framebuffe in n){var a=n[Pi];re framebuffe null}funct n(t){if(t in i){var in a){var \"+t)});var in in e?new s=o;o=new w(t){funct r(t){if(t in i){var r});return n.id=r,n}i in a){var o=a[t];ret null}var r(t,r){if( in n){var in i){var s=i[t];ret in n){var in i){var o=i[Ri];re in n){var t=n[ji];re Be[t]})}if in i){var r=i[ji];re in \"+n,\"inval primitive, must be one of Aa}):new in n){var vertex t})}if(Ni in i){var r=i[Ni];re vertex s?new vertex offset/ele buffer too l=new k(t,e){var o(e,n){if( in r){var o})}else if(t in i){var vi:case si:case oi:case Ai:case hi:case Ci:case xi:case wi:case Mi:case pi:return flag fi:return in \"+i,\"inval \"+t+\", must be one of di:return color attachment for framebuffe sent to uniform data for uniform a[r],\"inva uniform or missing data for uniform T(t,r){var a&&a,\"inva data for attribute offset for attribute divisor for attribute parameter \"'+r+'\" for attribute pointer \"'+t+'\" (valid parameters are in r)return r[s];var in '+a+\"&&(ty dynamic attribute if(\"consta in \"+a+'.cons === in S(t){var a(t){var parameter L(t,e,r){v C(t,e,r,n) z(t,e,r){v n=m(e);if( in r.state)){ c,h;if(n in in I(t,e,r,n) if(mt(u)){ l(t){var ua:case da:case ga:return 2;case ca:case pa:case ya:return 3;case ha:case ma:case ba:return 1}}functio attribute i(i){var a=c[i];ret a(){functi o(){functi vertex vertex vertex i(t){retur n(e){var n=r.draw[e s(t){funct e(t){var args to args to e(t){if(t in r){var e=r[t];del delete l(t,e){var regl.clear with no buffer takes an object as cancel a frame callback must be a h(){var callback must be a function\") event, must be one of Kt={\"[obje renderbuff renderbuff arguments to renderbuff r(){return i(t){var s(){return p.pop()||n o}function u(t,e,r){v c(){var t(){var new requires at least one argument; got none.\");va e.href;var \",e);var s=new o;n=-(i+a) null;var n(t){retur n(t){for(v R;};return i(t){var e=s[t];ret strict\";\"u n(t){for(v i}function h(t,e){for r=new r}function r=new l(e)}funct u(t){for(v e=s(t);;){ t=k[0];ret f(t,e){var r=k[t];ret n(t,e){var l}else if(u)retur l}else if(u)retur u;return i(t,e){ret t.y-e}func a(t,e){for r=null;t;) t;var r}function l(t){for(v n=d.index; n(t,e){var i(t,e,r,n) o(t,e){for r}function s(t,e){for m}function s[t];for(v new unexpected new failed to parse named argument new failed to parse named argument new mixing positional and named placeholde is not (yet) s[t]=n}var n(t){for(v Array(e),n Array(e),i Array(e),a Array(e),o Array(e),s x=new u(t){retur c(t){var h(t){retur f(t){var d(t,e){for r in t}function p(t){retur t.x}functi m(t){retur t.y}var time\");var r=\"prepare \"+t.length %d clusters in c)|0 p=new Array(r),m Array(r),v Array(r),g p=new o}function s}function T(t){retur n=z(t);ret t){var r={};for(v i in e={};for(v r in n(t,e){var i(t,e){var s/6}return 1}var n&&void e(t,e){var for(a=0,n= n})}}var s;var in new Error(\"n must be new Error(\"alr s(t){retur new l(t){retur new u(t){retur new c(t){retur new h(t){retur new f(t){retur new d(t){retur new p(t){retur new m(t){retur x?new v(t){retur new n(t)}var null}retur t=0;tn)ret instanceof n)return t;var i=new n;return a(t){retur instanceof o(t,e){ret s(t,e){ret new 'url' must be a string, not \"+typeof t);var i(t,e){var a(t,e){var o(t,e){ret t}function s(t){var e={};retur a;var v=e.name?\" c(e)}var o+\": \"+s}functi d(t,e,r){v n=0;return \")+\" \"+t.join(\" \")+\" \"+t.join(\" \")+\" p(t){retur t}function v(t){retur g(t){retur t}function t}function t}function _(t){retur void 0===t}func w(t){retur M(t)&&\"[ob k(t){retur M(t)&&\"[ob A(t){retur instanceof t}function S(t){retur t||void 0===t}func E(t){retur L(t){retur t=a)return new Error(\"unk command if(7!==r)t new Error(\"unk command i(t){for(v e}var new Error(\"fea index out of new new String too long (sorry, this will get fixed later)\");v l(t){for(v e(t){var e=n(t);ret e?u in r(t,e){var o(t){var i?u in i&&delete t){var r?r[0]:\"\"} n?!r&&en)t al-ahad\",\" {0} not {0} {0} {0} mix {0} and {1} a(t,e){ret ;var format a date from another number at position name at position literal at position text found at dd M MM d, d M d M d M d M yyyy\",RSS: d M a=this;ret var _inline_1_ = - var _inline_1_ = - >= 0) !== (_inline_1 >= 0)) {\\n + 0.5 + 0.5 * (_inline_1 + _inline_1_ / (_inline_1 - }\\n n(t,e){var r=[];retur strict\";va u(r,i){ret i(t,e){var void E.remove() void null;var strict\";va void c();var t}function i(t){var e=x[t];ret a(t){retur the calendar system to use with `\"+t+\"` date data.\"}var i={};retur t}var i?\"rgba(\"+ n=i(t);ret t){var A(e,r){var T(){var void strict\";va strict\";va strict\";va strict\";va strict\";va strict\";va n(){var e(e){retur r;try{r=ne strict\";va i(t,e,r,n) a(t){var void n.remove() void \")}).split \")}).split scale(\"+e+ n,i,a;retu strict\";va 0 1,1 0 0,1 \"+a+\",\"+a+ 0 0 1 \"+a+\",\"+a+ 0 0 1 \"+r+\",\"+r+ 0 0 1 \"+r+\",\"+r+ 0 0 1 0 1,1 0 0,1 0 1,1 0 0,1 n(t,e,r,n) t.id});var strict\";va strict\";va i(t,e,r){v r(t){var void r.remove() r(e,r,o){v if(i[r]){v o;if(void strict\";va n(t){var n(r){retur strict\";va n(t){for(v \");var i(t,e){var click on legend to isolate individual l(t){var u(t){var strict\";va r[1]}retur i}function i(t){retur t[0]}var h(t){var f(t){var d(t){var n(t,e){var i(t){for(v n(t){for(v 0}}var o(t,e){var 0 1,1 0 0,1 extra params in segment t(e).repla strict\";va strict\";va u(r,i){ret r(t,e){ret l(t,e,r){v u(t,e,r){v c(t,e){var n(){return p(t,e){var g(t,e){ret y(t,e){ret b(t,e,r){v x(t,e){var _(t){for(v r(t,e){ret strict\";va strict\";va strict\";va t){var void t)return void void n}function l(t){retur u(t){retur c(t){retur d\")}functi h(t){retur d, yyyy\")}var t.getTime} r={};retur n=new a(t){retur o(t){for(v r={};retur n(){return strict\";va for(var c(t){retur void property r(t,e){var instanceof RegExp){va void o(t,e){ret t>=e}var binary r=e%1;retu n(t){var e=i(t);ret n(t,e){ret i(t){retur \")}functio a(t,e,r){v was an error in the tex null;var r=0;r1)for i=1;i doesnt match end tag . Pretending it did s}function c(t,e,r){v o(),void e();var 0,\":\"],k=n t(t,e){ret void n(t){var i(){var 1px new strict\";va strict\";va n(t,e){for r=new new Error(\"No DOM element with id '\"+t+\"' exists on the page.\");re 0===t)thro new Error(\"DOM element provided is null or previous rejected promises from t.yaxis1); array edits are incompatib with other edits\",h); full array edit out of if(void & removal are incompatib with edits to the same full object edit new Error(\"eac index in \"+r+\" must be new Error(\"gd. must be an 0===e)thro new is a required new Error(\"cur and new indices must be of equal u(t,e,r){v new Error(\"gd. must be an 0===e)thro new Error(\"tra must be in in i(t){retur a(t,e){var r=0;return new Error(\"Thi element is not a Plotly plot: \"+t+\". It's likely that you've failed to create a plot before animating it. For more details, see void c()}functi d(t){retur overwritin frame with a frame whose name of type \"number\" also equates to \"'+f+'\". This is valid but may potentiall lead to unexpected behavior since all plotly.js frame names are stored internally as This API call has yielded too many warnings. For the rest of this call, further warnings about numeric frame names will be addFrames accepts frames with numeric names, but the numbers areimplici cast to n(t){var i}function i(){var t={};retur a(t){var o(){var s(t){retur l(t){funct u(t){funct c(t){retur h(t,e,r){v f(t,e,r){v e={};retur t&&void n(t){retur Error(\"Hei and width should be pixel values.\")) l(t,e,r){v u(t,e,r,n) \"+o:s=o+(s dtick p(t,e){var c=new t.dtick){v error: t+i*e;var dtick a(t){for(v strict\";va v(r,n){ret to enter axis\")+\" e;var n(t,r){for n(t,e,r,n) u(t,e){ret y(t){var b(t,e,r){v back X(e,r){var K()}functi W(e){funct n(e){retur void k.log(\"Did not find wheel motion attributes \",e);var strict\";va n(t){retur t._id}func went wrong with axis Error(\"axi in in in o){var t(t){var e(t){retur strict\";va r(r,n){var e/2}}funct v(t,e){var g(t,e){var b(t,e){var x(t,e){var new Error(\"not yet r(t,r){for i(){for(va a(t,e){for n(t,e){var n(t){retur i(t,e,r,n) a(t,e){ret i(t,e){var r(t){retur n(t){var l(t,e){var u(t){var c(t,e){var f(t,e,r){v strict\";va i(t){var e=new n;return a(t){var o(t){var i=new n(t,e);ret strict\";va Sans Regular, Arial Unicode MS r(t,e){ret - delete t)return e,n,i={};f in i}return r=a(t);ret e&&delete P=(new + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + 0px\",\"1px -1px\",\"-1p 1px\",\"1px \"+t+\" 0 \"+n+\" \"+n+\" \"+n+\" void c=\"t: \"+u.t+\", r: l;var r in t)r in r in r=e||6;ret 0===t)retu null;var void t(){var t={};retur n.mode,del strict\";va e(e,i){ret s;return t(t,e){ret i=r[n];ret strict\";va t(t,e,r){v e(t,e){ret r(t,e){ret a(t,i){var tozoom back f(t,e){var i(t){retur y,b;return o,s;return ii))return e}return void h(t){retur f(t,e){ret void strict\";va strict\";va 0, 0, strict\";va s;return r in void strict\";va null;for(v strict\";va o(e){var s(e){var strict\";va n(t,e,r){v i(t,e,r){v strict\";va converged strict\";va strict\";va strict\";va strict\";va s(r,i){ret n(t,e,r,n) strict\";va n(t,e){for o(t){retur strict\";va void strict\";va strict\";va c(r,i){ret loop in contour?\") s(t,e,r){v 15===r?0:r many contours, clipping at i}function a(t,e,r){v o(t,e,r){v s(t,e,r,n) e=l(t,r) r(t){retur to newendpt is not vert. or perimeter scale is not scale is not void data invalid for the specified inequality many contours, clipping at strict\";va strict\";va h(t){retur to newendpt is not vert. or perimeter o(t,e,r){v s(t,e,r){v scale is not scale is not strict\";va iterated with no new in strict\";va g}var didn't converge strict\";va s=0;sa){va in strict\";va l(r,n){ret u(t){var e=l(t);ret strict\";va strict\";va e(e){var strict\";va strict\";va void r(t,e){ret traces support up to \"+u+\" dimensions at the c}var l(r,n){ret strict\";va l(n){var i}function c(t,e,r){v l(t,e,r){v n=o(r);ret u(t,e){ret c(t){retur h(t){var e=o(t);ret f(t){var d(t){retur t[0]}funct p(t,e,r){v m(t){var v(t){retur l(t){var u(t){retur c(t,e){for e.t+\"px \"+e.r+\"px \"+e.b+\"px 255, 255, 0)\");var 1px 1px #fff, -1px -1px 1px #fff, 1px -1px 1px #fff, -1px 1px 1px strict\";va i(t,e,r){v strict\";va n(t,e){for m};var strict\";va o(r,a){ret strict\";va strict\";va strict\";va n(t,e,r){v u;var 1;var a(t,e){var r(t,e){ret n(t,e){ret s(t,e){var 1;var t+\" void strict\";va strict\";va strict\";va 0, i(t,e){var r=new for(r=new is present in the Sankey data. Removing all nodes and strict\";va u(r,a){ret n(t){retur t.key}func a(t){retur t[0]}funct o(t){var 0 0 1 0 0)\":\"matri 1 1 0 0 0)\")}funct M(t){retur k(t){retur 0 0 1 0 0)\":\"matri 1 1 0 0 0)\"}functi A(t){retur 1)\":\"scale 1)\"}functi T(t){retur S(t){retur L(t,e,r){v var C(t,e,r){v i(){for(va e={};retur 1px 1px #fff, 1px 1px 1px #fff, 1px -1px 1px #fff, -1px -1px 1px strict\";va _=new strict\";va void strict\";va strict\";va m(r,a){ret strict\";va strict\";va strict\";va r(e){var i(t){var strict\";va n(t,e){var + m(t){retur v(t){retur g(t){retur t.id}funct g}function x(e){var scatter strict\";va s(t,e){ret l(t){retur M[t]}funct o=0;o=0){v n(t,e,r,n) strict\";va d(r,i){ret s=o[0];if( 0;var v.push(\"y: strict\";va strict\";va e(t){retur r(t){var 1/0;var strict\";va n(t,e){var n}function s(t,e,r,n) n=new s(t){var 1/0;var strict\";va strict\";va strict\";va strict\";va d(r,i){ret strict\";va strict\";va e=f(t);ret e=f(t);ret e=f(t);ret e=f(t);ret In\u00a0[3]: ## Setup - appearance # get rid of the annoying warning = None # default='w # more than one print of an unassigned variable from import = \"all\"; # offline plotly color1 = 'red' color2 = '#137a28' # dark green exports, module) {/** * plotly.js v1.28.3 * Copyright 2012-2017, Plotly, Inc. * All rights reserved. * Licensed under the MIT license */ t;return function a(o,!0);va u=new Error(\"Can find module i(t,e){ret t.y-e.y}va r(t){retur r(t){retur u(){functi t(t,e){ret e(t,e){ret c(t){retur h(t){retur t.value}va t(t){var \"+s+\",\"+c+ \"+o+\",\"+u+ e=.5;retur n(t){var n=a(t,new e?e:1,r=r| \";var n(t,e){for r=new instanceof function y(t){var e}function b(t,e,r,n) e)throw new argument must be a expected unwanted i}var r=new t};var e=[];for(v r in n(t){for(v n(t){retur n(t){var i=new e=t|t-1;re new i}function l(t){for(v e=new e}function ffffffff ffffffff ffffffff ffffffff ffffffff fffffffe ffffffff ffffffff ffffffff 00000000 00000000 ffffffff ffffffff fffffffe ffffffff t){var must be greater than t instanceof can only safely store up to 53 n(void array length 26;var e=t,r=0;re 0;for(var t&&t>=0);v t&&t>=0);v new a(1);for(v t&&t>=0);v works only with positive a(0),mod:n a(0)};var i,o,s;retu i=new a(1),o=new a(0),s=new a(0),l=new i=new a(1),o=new f;return A[t];var p;else m;else new Error(\"Unk prime \"+t);e=new g}return works only with works only with red works only with works only with red l}}}functi s(t){retur l(t,e){ret 1:return s(t);case 3:return new Invalid n(t,e,r){v \"+r);var new n(t,e){var i=\"for(var i=n[t];ret var P=C+1;PZ)t new typed array length\");v e=new e)throw new Error(\"If encoding is specified then the first argument must be a l(t)}retur t)throw new argument must not be a t instanceof t)throw new argument must be a new to allocate Buffer larger than maximum size: bytes\");re 0|t}functi instanceof 0;for(var void 0:return v(t,e,r){v n=!1;if((v new hex E(n)}funct E(t){var new to access beyond buffer new argument must be a Buffer new out of new out of a}function U(t){for(v a}function H(t){retur i}function Y(t){retur t!==t}var t=new browser lacks typed array (Uint8Arra support which is required by `buffer` v5.x. Use `buffer` v4.x if you require old browser new must be 0;for(var ... new must be a new of range 0;for(var 0)}var new to write outside buffer new encoding: new out of new Error(\"Inv string. Length must be a multiple of i(t){retur a(t){var o(t){retur i(t,e){ret a(t,e){for i(g,d,v,h) n(t){var i(t,e){for r=new s}function m(t,e,r){v i=new n(t){var strict\";va t;var new Error(f+\" map requires nshades to be at least size i(t,e,r,i) 0}return n(t,e){ret t-e}functi i(t,e){var 0:return 0;case 1:return t[0]-e[0]; 2:return 3:var i;var 4:var n(t){var t}function i(t){retur a(t){retur o(t){retur null}var null;var a}return a}return i(t){var e=new i=0;i0)thr new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array args\")}els new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array index\")}el new Error(\"cwi Too many arguments in pre() new Error(\"cwi Too many arguments in body() new Error(\"cwi Too many arguments in post() block\");re n(t,e,r){v l(t,e){for w=new cwise routine for new return n(t){var e=[\"'use strict'\",\" function (!(\"+l.joi && \")+\")) throw new Error('cwi Arrays do not all have the same {\"),e.push (!(\"+u.joi && \")+\")) throw new Error('cwi Arrays do not all have the same i(t,e,r){v m,v=new t;var e=[];for(v r in e=[];for(v r in e=[];for(v r in n&&void e(t,e){var n in r}function r(){}funct n(t){var e;return y(t){retur b(t){retur new Error(\"unk type: i(t,e){for r,n,i=new i(){if(s){ t(t){var r(t){var Array(o),l this;var 1:do{o=new 2:do{o=new 3:do{o=new t=[];retur s(){var l(){for(va a(t){retur n}}}functi l(t){retur u(t){for(v e}function c(t,e){for r in p(t){retur f(t)in this._&&de v(){var t=[];for(v e in t}function g(){var t=0;for(va e in t}function y(){for(va t in x(t){retur t}function function() w(t,e){if( in t)return Z(t,e){ret J(t,e){var K(t){var vt(t){retu gt(t){retu yt(t){retu _t(t){retu wt(t){retu kt(t,e,r){ Ot(){for(v t}function Ft(){for(v Nt(t){var b=u&&h;ret Bt(t){retu t+\"\"}funct n(e){var in e}function Gt(t,e,r){ le(t){var ce(t){for( ge(t){var ye(t,e){re we(t){var ke(t,e){re y}}functio Fe(t){retu Re(){var r=e;return r(t){var a(t,e){ret o(t,e){var l(t){for(v c(i,a){ret Je(){funct s}function 0 1,1 0 1,1 $e(){funct t(t,n){var er(){funct t(t,e){var rr(t){func s}function nr(t){func r(e){retur n(e){funct a(r,n){var k}function ir(t){var sr(t){retu t})()}func lr(t){func e(t){retur i(){return ur(t){retu r(t,e){var Vr(t,e){va n;var s;var Hr(t,e){va Vr(r,e);va Gr(t){for( r}function wn(t,e){va kn(t){retu An(t){retu 1;var Zn(t,e){va n}function bi(t){retu xi(t,e){re _i(t,e){re Ei(t){func Ni(t){retu t.y})}func Bi(t){retu Ui(t){var Vi(t){var qi(t,e){va a(t){retur o(t)}var o,s;return Qi(t,e){re $i(t,e){re a(t){retur o(e){retur t(i(e))}re _a(t){func e(e){funct Ma(t){retu ka(t){for( Aa(t){for( p[n]:delet t[r],1}var io(t){retu n(e){retur t(e)}funct i(t,r){var r};var t;var e=new b;if(t)for h(){functi f(){functi t(){var in r(){var n(){var o;if(i)ret i=!1,a;var e=new ms={\"-\":\"\" %b %e %X this.s}};v bs=new e(e,r){var t(){var e(){return }var new t(e,r,n,i) c}function e(t){for(v r}function t(t,a){var t(t,e){for i(t,e,r,n) \"+e}functi 0,0 \"+n}var t(t,i){var \"+l[2]+\" \"+l[3]}var 0,\"+e+\" \"+e+\",\"+e+ a(){functi v(){var l;var t;e||(e=t) e}function s(t){var l(t,e,r,n) u(t,e,r){v n=t;do{var n}function l=t;do{for h(t,e,r,n) r}function m(t,e,r,n) v(t){var t}function x(t,e){ret w(t,e){ret k(t,e){var A(t,e){ret n(t,e){var e){e=0;for warning: possible EventEmitt memory leak detected. %d listeners added. Use to increase must be a function\") n=!1;retur must be a e=typeof o(t,e,r,n) \"+i+\"=== typeof s(t,e){ret e.length> 1; if (a[m] === v) return true; if (a[m] > v) j = m - 1; else i = m + 1;}return false; }(\"+n+\", u(t){retur in p\"}functio h(t,e){ret c[1]){var s[e][t];va i(t){retur new a(t,e){ret new r}var 0)}functio d(t){for(v m(t){retur new t=[];retur t=[];retur 1:return 2:return new new new new this.tree; e=new i=0;i0)ret new Error(\"Can update empty node!\");va r=new new s(t){for(v z%d-%d-%d (features: %d, points: %d, simplified down to parent tile down\");var i(t,e,r){v s}function i(t,e,r,n) s(t,e){var r=new i(t);retur e(e,r,n){i in t){var U=g,V=_,k= 0.0) {\\n vec3 nPosition = mix(bounds bounds[1], 0.5 * (position + 1.0));\\n gl_Positio = projection * view * model * 1.0);\\n } else {\\n gl_Positio = }\\n colorChann = mediump GLSLIFY 1\\n\\nunifo vec4 vec3 main() {\\n gl_FragCol = colorChann * colors[0] + \\n colorChann * colors[1] +\\n colorChann * vectorizin d=new o(t,e,r,n) s;var r}function a(t,e){for r=0;rr)thr new If resizing buffer, must not specify a(t,e){for new Invalid type for webgl buffer, must be either or new Invalid usage for buffer, must be either gl.STATIC_ or t&&void new Cannot specify offset when resizing new Error(\"gl- Can't resize FBO, invalid new Error(\"gl- Parameters are too large for new Error(\"gl- Multiple draw buffer extension not new Error(\"gl- Context does not support \"+s+\" draw buffers\")} new Error(\"gl- Context does not support floating point h=!0;\"dept new Error(\"gl- Shape vector must be length 2\");var null;var 0.25) {\\n discard;\\n }\\n gl_FragCol = highp GLSLIFY 1\\n\\nattri vec2 aHi, aLo, vec4 pick0, vec2 scaleHi, translateH scaleLo, translateL float vec4 pickA, scHi, vec2 trHi, vec2 scLo, vec2 trLo, vec2 posHi, vec2 posLo) {\\n return (posHi + trHi) * scHi\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * main() {\\n vec2 p = translateH scaleLo, translateL aHi, aLo);\\n vec2 n = width * * vec2(dHi.y -dHi.x)) / gl_Positio = vec4(p + n, 0, 1);\\n pickA = pick0;\\n pickB = mediump GLSLIFY 1\\n\\nunifo vec4 vec4 pickA, pickB;\\n\\n main() {\\n vec4 fragId = 0.0);\\n if(pickB.w > pickA.w) {\\n fragId.xyz = pickB.xyz; }\\n\\n fragId += fragId.y += floor(frag / 256.0);\\n fragId.x -= floor(frag / 256.0) * 256.0;\\n\\n fragId.z += floor(frag / 256.0);\\n fragId.y -= floor(frag / 256.0) * 256.0;\\n\\n fragId.w += floor(frag / 256.0);\\n fragId.z -= floor(frag / 256.0) * 256.0;\\n\\n gl_FragCol = fragId / highp GLSLIFY 1\\n\\nattri vec2 aHi, aLo, vec2 scaleHi, translateH scaleLo, translateL float projectVal scHi, vec2 trHi, vec2 scLo, vec2 trLo, vec2 posHi, vec2 posLo) {\\n return (posHi + trHi) * scHi\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * main() {\\n vec2 p = translateH scaleLo, translateL aHi, aLo);\\n if(dHi.y e+n;var null;var FLOAT_MAX) {\\n return vec4(127.0 128.0, 0.0, 0.0) / 255.0;\\n } else if(v \"+t[1]+\", \"+t[2]+\", t=new e=new r=new \"+t[1]+\", n=\"precisi mediump GLSLIFY 1\\n\\nunifo vec3 float vec3 vec4 f_id;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n gl_FragCol = vec4(pickI mediump GLSLIFY 1\\n\\nattri vec3 position, vec4 vec2 uv;\\n\\nuni mat4 model\\n , view\\n , vec3 eyePositio , vec3 f_normal\\n , , , vec4 vec2 f_uv;\\n\\nv main() {\\n vec4 m_position = model * vec4(posit 1.0);\\n vec4 t_position = view * m_position gl_Positio = projection * t_position f_color = color;\\n f_normal = normal;\\n f_data = position;\\ f_eyeDirec = eyePositio - position;\\ = lightPosit - position;\\ f_uv = mediump GLSLIFY 1\\n\\nfloat x, float roughness) {\\n float NdotH = max(x, 0.0001);\\n float cos2Alpha = NdotH * NdotH;\\n float tan2Alpha = (cos2Alpha - 1.0) / cos2Alpha; float roughness2 = roughness * roughness; float denom = * roughness2 * cos2Alpha * cos2Alpha; return exp(tan2Al / roughness2 / vec3 vec3 vec3 float roughness, float fresnel) {\\n\\n float VdotN = 0.0);\\n float LdotN = 0.0);\\n\\n //Half angle vector\\n vec3 H = + //Geometri term\\n float NdotH = H), 0.0);\\n float VdotH = H), 0.000001); float LdotH = H), 0.000001); float G1 = (2.0 * NdotH * VdotN) / VdotH;\\n float G2 = (2.0 * NdotH * LdotN) / LdotH;\\n float G = min(1.0, min(G1, G2));\\n \\n //Distribu term\\n float D = //Fresnel term\\n float F = pow(1.0 - VdotN, fresnel);\\ //Multiply terms and done\\n return G * F * D / max(3.1415 * VdotN, vec3 float roughness\\ , fresnel\\n , kambient\\n , kdiffuse\\n , kspecular\\ , sampler2D vec3 f_normal\\n , , , vec4 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n\\n vec3 N = vec3 L = vec3 V = \\n {\\n N = -N;\\n }\\n\\n float specular = V, N, roughness, fresnel);\\ float diffuse = min(kambie + kdiffuse * max(dot(N, L), 0.0), 1.0);\\n\\n vec4 surfaceCol = f_color * f_uv);\\n vec4 litColor = surfaceCol * vec4(diffu * + kspecular * vec3(1,1,1 * specular, 1.0);\\n\\n gl_FragCol = litColor * mediump GLSLIFY 1\\n\\nattri vec3 vec4 vec2 uv;\\n\\nuni mat4 model, view, vec4 vec3 vec2 f_uv;\\n\\nv main() {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n f_color = color;\\n f_data = position;\\ f_uv = mediump GLSLIFY 1\\n\\nunifo vec3 sampler2D float vec4 vec3 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n\\n gl_FragCol = f_color * f_uv) * mediump GLSLIFY 1\\n\\nattri vec3 vec4 vec2 uv;\\nattri float mat4 model, view, vec3 vec4 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n gl_Positio = } else {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n }\\n gl_PointSi = pointSize; f_color = color;\\n f_uv = mediump GLSLIFY 1\\n\\nunifo sampler2D float vec4 vec2 f_uv;\\n\\nv main() {\\n vec2 pointR = - if(dot(poi pointR) > 0.25) {\\n discard;\\n }\\n gl_FragCol = f_color * f_uv) * mediump GLSLIFY 1\\n\\nattri vec3 vec4 id;\\n\\nuni mat4 model, view, vec3 vec4 f_id;\\n\\nv main() {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n f_id = id;\\n f_position = mediump GLSLIFY 1\\n\\nattri vec3 float vec4 id;\\n\\nuni mat4 model, view, vec3 vec3 vec4 f_id;\\n\\nv main() {\\n || \\n {\\n gl_Positio = } else {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n gl_PointSi = pointSize; }\\n f_id = id;\\n f_position = mediump GLSLIFY 1\\n\\nattri vec3 mat4 model, view, main() {\\n gl_Positio = projection * view * model * vec4(posit mediump GLSLIFY 1\\n\\nunifo vec3 main() {\\n gl_FragCol = i(t){for(v null;for(v function() E=new new s(\"\",\"Inva data type for attribute \"+h+\": new s(\"\",\"Unkn data type for attribute \"+h+\": \"+f);var new s(\"\",\"Inva data type for attribute \"+h+\": n(t){retur new i(t,e){for r=new new s(\"\",\"Inva uniform dimension type for matrix \"+name+\": new s(\"\",\"Unkn uniform data type for \"+name+\": \"+r)}var new s(\"\",\"Inva data new data type for vector \"+name+\": r=[];for(v n in e){var r}function h(e){for(v n=[\"return function new s(\"\",\"Inva data new s(\"\",\"Inva uniform dimension type for matrix \"+name+\": \"+t);retur i(r*r,0)}t new s(\"\",\"Unkn uniform data type for \"+name+\": \"+t)}}func i){var p(t){var r=0;r1){l[ u=1;u1)for l=0;l=0){v t||t}funct s(t){funct r(){for(va u=0;u 1.0) {\\n discard;\\n }\\n baseColor = color, step(radiu gl_FragCol = * baseColor. mediump GLSLIFY 1\\n\\nattri vec2 vec4 mat3 float vec4 vec4 main() {\\n vec3 hgPosition = matrix * vec3(posit 1);\\n gl_Positio = 0, gl_PointSi = pointSize; vec4 id = pickId + pickOffset id.y += floor(id.x / 256.0);\\n id.x -= floor(id.x / 256.0) * 256.0;\\n\\n id.z += floor(id.y / 256.0);\\n id.y -= floor(id.y / 256.0) * 256.0;\\n\\n id.w += floor(id.z / 256.0);\\n id.z -= floor(id.z / 256.0) * 256.0;\\n\\n fragId = mediump GLSLIFY 1\\n\\nvaryi vec4 main() {\\n float radius = length(2.0 * - 1.0);\\n if(radius > 1.0) {\\n discard;\\n }\\n gl_FragCol = fragId / i(t,e){var instanceof instanceof null;var n(t,e,r,n) highp GLSLIFY 1\\n\\n\\nvec posHi, vec2 posLo, vec2 scHi, vec2 scLo, vec2 trHi, vec2 trLo) {\\n return vec4((posH + trHi) * scHi\\n \\t\\t\\t//FI this thingy does not give noticeable precision gain, need test\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * scLo\\n , 0, vec2 positionHi float size, vec2 char, is 64-bit form of scale and vec2 scaleHi, scaleLo, translateH float vec4 sampler2D vec4 charColor, vec2 vec2 float float main() {\\n charColor = vec2(color / 255., 0));\\n borderColo = vec2(color / 255., 0));\\n\\n gl_PointSi = size * pixelRatio pointSize = size * charId = char;\\n borderWidt = border;\\n\\ gl_Positio = positionHi positionLo scaleHi, scaleLo,\\n translateH pointCoord = viewBox.xy + (viewBox.z - viewBox.xy * * .5 + highp GLSLIFY 1\\n\\nunifo sampler2D vec2 float charsStep, pixelRatio vec4 vec4 vec2 vec2 float float main() {\\n\\tvec2 pointUV = (pointCoor - + pointSize * .5) / = 1. - texCoord = ((charId + pointUV) * charsStep) / dist = alpha\\n\\ti (dist t;){var w.push(new i(){var a(t,e){var e=void null;var number of characters is more than maximum texture size. Try reducing x=0;x 1.0) {\\n discard;\\n }\\n vec4 baseColor = color, float alpha = 1.0 - pow(1.0 - baseColor. fragWeight gl_FragCol = * alpha, highp GLSLIFY 1\\n\\nvec4 pfx_1_0(ve scaleHi, vec2 scaleLo, vec2 translateH vec2 translateL vec2 positionHi vec2 positionLo {\\n return + translateH * scaleHi\\n + (positionL + translateL * scaleHi\\n + (positionH + translateH * scaleLo\\n + (positionL + translateL * scaleLo, 0.0, vec2 positionHi vec4 vec2 scaleHi, scaleLo, translateH float vec4 vec4 main() {\\n\\n vec4 id = pickId + pickOffset id.y += floor(id.x / 256.0);\\n id.x -= floor(id.x / 256.0) * 256.0;\\n\\n id.z += floor(id.y / 256.0);\\n id.y -= floor(id.y / 256.0) * 256.0;\\n\\n id.w += floor(id.z / 256.0);\\n id.z -= floor(id.z / 256.0) * 256.0;\\n\\n gl_Positio = scaleLo, translateH translateL positionHi positionLo gl_PointSi = pointSize; fragId = mediump GLSLIFY 1\\n\\nvaryi vec4 main() {\\n float radius = length(2.0 * - 1.0);\\n if(radius > 1.0) {\\n discard;\\n }\\n gl_FragCol = fragId / i(t,e){var e(e,r){ret e in n(t,e){var in r)return r[t];for(v o=r.gl d(t){var null;var a(t,e){ret new E=new i(t,e){var r=new n(t);retur 0.0 ||\\n || {\\n discard;\\n }\\n\\n vec3 N = vec3 V = vec3 L = {\\n N = -N;\\n }\\n\\n float specular = V, N, roughness) float diffuse = min(kambie + kdiffuse * max(dot(N, L), 0.0), 1.0);\\n\\n //decide how to interpolat color \\u2014 in vertex or in fragment\\n vec4 surfaceCol = .5) * vec2(value value)) + step(.5, vertexColo * vColor;\\n\\ vec4 litColor = surfaceCol * vec4(diffu * + kspecular * vec3(1,1,1 * specular, 1.0);\\n\\n gl_FragCol = mix(litCol contourCol contourTin * mediump GLSLIFY 1\\n\\nattri vec4 uv;\\nattri float f;\\n\\nunif mat3 mat4 model, view, float height, sampler2D float value, kill;\\nvar vec3 vec2 vec3 eyeDirecti vec4 main() {\\n vec3 dataCoordi = permutatio * vec3(uv.xy height);\\n vec4 worldPosit = model * 1.0);\\n\\n vec4 clipPositi = projection * view * clipPositi = clipPositi + zOffset;\\n gl_Positio = value = f;\\n kill = -1.0;\\n = = uv.zw;\\n\\n vColor = vec2(value value));\\n //Don't do lighting for contours\\n surfaceNor = vec3(1,0,0 eyeDirecti = vec3(0,1,0 lightDirec = mediump GLSLIFY 1\\n\\nunifo vec2 vec3 float float value, kill;\\nvar vec3 vec2 vec3 v) {\\n float vh = 255.0 * v;\\n float upper = floor(vh); float lower = fract(vh); return vec2(upper / 255.0, floor(lowe * 16.0) / main() {\\n if(kill > 0.0 ||\\n || {\\n discard;\\n }\\n vec2 ux = / shape.x);\\ vec2 uy = / shape.y);\\ gl_FragCol = vec4(pickI ux.x, uy.x, ux.y + i(t){var o(t,e){var new invalid coordinate for new Invalid texture size\");ret s(t,e){ret new Invalid ndarray, must be 2d or 3d\");var new Invalid shape for new Invalid shape for pixel new Incompatib texture format for new Invalid texture new Floating point textures not supported on this platform\") s=u(t);ret s=u(t);ret f(t,e){var new Invalid texture size\");var new Invalid shape for new Invalid shape for pixel b=u(t);ret new Error(\"gl- Too many vertex n(t,e,r){v i=new n(t){for(v n(t,e){var n(t,e,r){v instanceof a=new a(t,e){ret o(t){for(v e=[\"functi orient(){v orient\");v n=new a(t,e){var o(t,e){var s(t,e){var i}}functio c(t,e){for s(this,t); s(this,t); b}for(var r}return n}return l}function i(t,e,r,n) n(t,e){var r;if(h(t)) new Error('Unk function type -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n\\n // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def float vec2 vec2 vec2 vec2 vec2 vec2 float float sampler2D vec2 vec2 float float main() {\\n // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line or when fading out\\n // float blur = u_blur * float alpha = clamp(min( - (v_linewid - blur), v_linewidt - dist) / blur, 0.0, 1.0);\\n\\n float x_a = / 1.0);\\n float x_b = / 1.0);\\n float y_a = 0.5 + (v_normal. * v_linewidt / float y_b = 0.5 + (v_normal. * v_linewidt / vec2 pos_a = vec2(x_a, y_a));\\n vec2 pos_b = vec2(x_b, y_b));\\n\\n vec4 color = pos_a), pos_b), u_fade);\\n alpha *= u_opacity; gl_FragCol = color * gl_FragCol = highp lowp\\n#def floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the vec2 vec4 mat4 mediump float mediump float mediump float mediump float mediump float mat2 mediump float vec2 vec2 float float main() {\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * // We store the texture normals in the most insignific bit\\n // transform y so that 0 => -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n v_linesofa = // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def lowp vec4 lowp float float sampler2D float float vec2 vec2 vec2 vec2 float main() {\\n // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line or when fading out\\n // float blur = u_blur * float alpha = clamp(min( - (v_linewid - blur), v_linewidt - dist) / blur, 0.0, 1.0);\\n\\n float sdfdist_a = v_tex_a).a float sdfdist_b = v_tex_b).a float sdfdist = mix(sdfdis sdfdist_b, u_mix);\\n alpha *= smoothstep - u_sdfgamma 0.5 + u_sdfgamma sdfdist);\\ gl_FragCol = u_color * (alpha * gl_FragCol = highp lowp\\n#def floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the vec2 vec4 mat4 mediump float mediump float mediump float mediump float vec2 float vec2 float float mat2 mediump float vec2 vec2 vec2 vec2 float main() {\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * // We store the texture normals in the most insignific bit\\n // transform y so that 0 => -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n\\n v_tex_a = * normal.y * + u_tex_y_a) v_tex_b = * normal.y * + // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def mapbox: define lowp vec4 mapbox: define lowp float vec2 v_pos;\\n\\n main() {\\n #pragma mapbox: initialize lowp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ float dist = length(v_p - float alpha = 0.0, dist);\\n gl_FragCol = outline_co * (alpha * gl_FragCol = highp lowp\\n#def vec2 mat4 vec2 vec2 mapbox: define lowp vec4 mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n v_pos = / gl_Positio + 1.0) / 2.0 * mediump lowp\\n#def float vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 vec2 v_pos;\\n\\n main() {\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = vec4 color2 = pos2);\\n\\n // find distance to outline for alpha float dist = length(v_p - float alpha = 0.0, dist);\\n \\n\\n gl_FragCol = mix(color1 color2, u_mix) * alpha * gl_FragCol = highp lowp\\n#def vec2 vec2 vec2 vec2 float float float vec2 mat4 vec2 vec2 vec2 vec2 v_pos;\\n\\n main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n vec2 scaled_siz = u_scale_a * vec2 scaled_siz = u_scale_b * // the correct offset needs to be calculated //\\n // The offset depends on how many pixels are between the world origin and\\n // the edge of the tile:\\n // vec2 offset = size)\\n //\\n // At high zoom levels there are a ton of pixels between the world origin\\n // and the edge of the tile. The glsl spec only guarantees 16 bits of\\n // precision for highp floats. We need more than that.\\n //\\n // The pixel_coor is passed in as two 16 bit values:\\n // = / 2^16)\\n // = 2^16)\\n //\\n // The offset is calculated in a series of steps that should preserve this precision: vec2 offset_a = scaled_siz * 256.0, scaled_siz * 256.0 + vec2 offset_b = scaled_siz * 256.0, scaled_siz * 256.0 + v_pos_a = * a_pos + offset_a) / v_pos_b = * a_pos + offset_b) / v_pos = / gl_Positio + 1.0) / 2.0 * mediump lowp\\n#def float vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 main() {\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = vec4 color2 = pos2);\\n\\n gl_FragCol = mix(color1 color2, u_mix) * gl_FragCol = highp lowp\\n#def mat4 vec2 vec2 vec2 vec2 float float float vec2 vec2 vec2 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n vec2 scaled_siz = u_scale_a * vec2 scaled_siz = u_scale_b * // the correct offset needs to be calculated //\\n // The offset depends on how many pixels are between the world origin and\\n // the edge of the tile:\\n // vec2 offset = size)\\n //\\n // At high zoom levels there are a ton of pixels between the world origin\\n // and the edge of the tile. The glsl spec only guarantees 16 bits of\\n // precision for highp floats. We need more than that.\\n //\\n // The pixel_coor is passed in as two 16 bit values:\\n // = / 2^16)\\n // = 2^16)\\n //\\n // The offset is calculated in a series of steps that should preserve this precision: vec2 offset_a = scaled_siz * 256.0, scaled_siz * 256.0 + vec2 offset_b = scaled_siz * 256.0, scaled_siz * 256.0 + v_pos_a = * a_pos + offset_a) / v_pos_b = * a_pos + offset_b) / mediump lowp\\n#def float float sampler2D sampler2D vec2 vec2 float float float float vec3 main() {\\n\\n // read and cross-fade colors from the main and parent tiles\\n vec4 color0 = v_pos0);\\n vec4 color1 = v_pos1);\\n vec4 color = color0 * u_opacity0 + color1 * u_opacity1 vec3 rgb = color.rgb; // spin\\n rgb = vec3(\\n dot(rgb, dot(rgb, dot(rgb, // saturation float average = (color.r + color.g + color.b) / 3.0;\\n rgb += (average - rgb) * // contrast\\n rgb = (rgb - 0.5) * + 0.5;\\n\\n // brightness vec3 u_high_vec = vec3 u_low_vec = gl_FragCol = u_low_vec, rgb), gl_FragCol = highp lowp\\n#def mat4 vec2 float float vec2 vec2 vec2 vec2 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n v_pos0 = / 32767.0) - 0.5) / u_buffer_s ) + 0.5;\\n v_pos1 = (v_pos0 * + mediump lowp\\n#def sampler2D sampler2D lowp float vec2 vec2 main() {\\n lowp float alpha = v_fade_tex * u_opacity; gl_FragCol = v_tex) * gl_FragCol = highp lowp\\n#def vec2 vec2 vec2 vec4 matrix is for the vertex mat4 mediump float bool vec2 vec2 vec2 vec2 main() {\\n vec2 a_tex = mediump float a_labelmin = a_data[0]; mediump vec2 a_zoom = a_data.pq; mediump float a_minzoom = a_zoom[0]; mediump float a_maxzoom = a_zoom[1]; // u_zoom is the current zoom level adjusted for the change in font size\\n mediump float z = 2.0 - u_zoom) - (1.0 - u_zoom));\\ vec2 extrude = * (a_offset / 64.0);\\n if {\\n gl_Positio = u_matrix * vec4(a_pos + extrude, 0, 1);\\n gl_Positio += z * } else {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n }\\n\\n v_tex = a_tex / u_texsize; v_fade_tex = / 255.0, mediump lowp\\n#def sampler2D sampler2D lowp vec4 lowp float lowp float lowp float vec2 vec2 float main() {\\n lowp float dist = v_tex).a;\\ lowp float fade_alpha = lowp float gamma = u_gamma * lowp float alpha = - gamma, u_buffer + gamma, dist) * gl_FragCol = u_color * (alpha * gl_FragCol = highp lowp\\n#def float PI = vec2 vec2 vec2 vec4 matrix is for the vertex mat4 mediump float bool bool mediump float mediump float mediump float vec2 vec2 vec2 vec2 float main() {\\n vec2 a_tex = mediump float a_labelmin = a_data[0]; mediump vec2 a_zoom = a_data.pq; mediump float a_minzoom = a_zoom[0]; mediump float a_maxzoom = a_zoom[1]; // u_zoom is the current zoom level adjusted for the change in font size\\n mediump float z = 2.0 - u_zoom) - (1.0 - u_zoom));\\ // map\\n // map | viewport\\n if {\\n lowp float angle = ? (a_data[1] / 256.0 * 2.0 * PI) : u_bearing; lowp float asin = sin(angle) lowp float acos = cos(angle) mat2 RotationMa = mat2(acos, asin, -1.0 * asin, acos);\\n vec2 offset = RotationMa * a_offset;\\ vec2 extrude = * (offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos + extrude, 0, 1);\\n gl_Positio += z * // viewport\\n // map\\n } else if {\\n // foreshorte factor to apply on pitched maps\\n // as a label goes from horizontal vertical in angle\\n // it goes from 0% foreshorte to up to around 70% lowp float pitchfacto = 1.0 - cos(u_pitc * sin(u_pitc * 0.75));\\n\\ lowp float lineangle = a_data[1] / 256.0 * 2.0 * PI;\\n\\n // use the lineangle to position points a,b along the line\\n // project the points and calculate the label angle in projected space\\n // this calculatio allows labels to be rendered unskewed on pitched maps\\n vec4 a = u_matrix * vec4(a_pos 0, 1);\\n vec4 b = u_matrix * vec4(a_pos + 0, 1);\\n lowp float angle = - b[0]/b[3] - a[0]/a[3]) lowp float asin = sin(angle) lowp float acos = cos(angle) mat2 RotationMa = mat2(acos, -1.0 * asin, asin, acos);\\n\\n vec2 offset = RotationMa * 1.0) * a_offset); vec2 extrude = * (offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n gl_Positio += z * // viewport\\n // viewport\\n } else {\\n vec2 extrude = * (a_offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n }\\n\\n v_gamma_sc = (gl_Positi - 0.5);\\n\\n v_tex = a_tex / u_texsize; v_fade_tex = / 255.0, mediump lowp\\n#def float float float float main() {\\n\\n float alpha = 0.5;\\n\\n gl_FragCol = vec4(0.0, 1.0, 0.0, 1.0) * alpha;\\n\\n if > u_zoom) {\\n gl_FragCol = vec4(1.0, 0.0, 0.0, 1.0) * alpha;\\n }\\n\\n if (u_zoom >= v_max_zoom {\\n gl_FragCol = vec4(0.0, 0.0, 0.0, 1.0) * alpha * 0.25;\\n }\\n\\n if >= u_maxzoom) {\\n gl_FragCol = vec4(0.0, 0.0, 1.0, 1.0) * alpha * 0.2;\\n highp lowp\\n#def vec2 vec2 vec2 mat4 float float float main() {\\n gl_Positio = u_matrix * vec4(a_pos + a_extrude / u_scale, 0.0, 1.0);\\n\\n v_max_zoom = a_data.x;\\ = vec4 values, const float t) {\\n if (t 7)return[n have been deprecated as of v8\")];if(! in \"%s\" not strict\";va a(l,e,\"arr expected, %s a(l,e,\"arr length %d expected, length %d r?[new have been deprecated as of v8\")]:[];v n(e,r,\"obj expected, %s found\",a)] o=[];for(v s in must start with \"@\"'));ret strict\";va one of [%s], %s strict\";va t(e){var n(l,s,\"arr expected, %s n(l,s,'\"$t cannot be use with operator n(l,s,'fil array for operator \"%s\" must have 3 expected, %s key cannot be a functions not functions not strict\";va url must include a \"{fontstac url must include a \"{range}\" strict\";va n(c,r,'eit \"type\" or \"ref\" is i(e,r,\"%s is greater than the maximum value strict\";va n(e,r,\"obj expected, %s f in r){var property in n(e,r,'mis required property strict\";va i(e,o,'unk property strict\";va n(r,e,'\"ty is e)for(var c in a(t){retur Sans Unicode MS new new M=new in n){for(var many symbols being rendered in a tile. See many glyphs being rendered in a tile. See exceeds allowed extent, reduce your vector tile buffer size\")}ret new new Error(\"Inv LngLat object: (\"+t+\", new new x(){return y(){return point(){re new new new new instanceof 0===s&&voi a(void new Error(\"fai to invert strict\";va n={\" strict\";va s(t){retur l(t,e,r,n) o=(new out of n(t,e){ret mapbox: ([\\w]+) ([\\w]+) ([\\w]+) a=new n?e(new Error(\"Inp data is not a valid GeoJSON t.data)ret e(new Error(\"Inp data is not a valid GeoJSON e(new Error(\"Inp data is not a valid GeoJSON e=0;ee)){v y;for(y in in p)c[y]=!0; t in new new i(t,e,i){v r(t,r){ret delete e(t);var n=new o(new new e=new in tile source layer \"'+M+'\" does not use vector tile spec v2 and therefore may have some rendering g(t,L);var F in B in n=new t.time>=(n void void t=new new i;var strict\";va new Error(\"Inv color o[e]}throw new Error(\"Inv color void n in r in Error('Sou layer does not exist on source \"'+e.id+'\" as specified by style layer t in t.id});for new Error(\"Sty is not done new Error(\"The is no source with this ID\");var delete instanceof this;var 0===e)thro new Error(\"The is no layer with this ID\");for(v r in this;var void 0===i||voi 0===a?void strict\";va i(t){retur t.value}va r,n;for(va i in t){var in for(n in in in 0===e)dele 0===e)dele o}var strict\";va new t){var this.grid= a}if(r){va _=u;for(va a}}}return r=new r(\"glyphs > 65535 not i=!t&&new l(new c(new g(e,r){var y(e,r){var i(0,0));re M in a)t[M]=new strict\";va t){var | n(){}var i(t){retur new 61:case 107:case 171:case 189:case 109:case t=0,e=0;re t=new null!==t&& new Error(\"max must be between the current minZoom and 20, t,e={};ret t instanceof e;if(t instanceof instanceof c?t:new i(this,e); void Error(\"Fai to initialize s in if(void if(void n(t){var r=new n(t){for(v e=0;e1)for delete error c(t,e,r){v f(t,e){for t in null;var delete new Error(\"An API access token is required to use Mapbox GL. See new Error(\"Use a public access token (pk.*) with Mapbox GL JS, not a secret access token (sk.*). See t}function i(t){retur a(t){retur t;var n(t){funct v[n];void in t=0;t=1)re 1;var void t={};for(v e in =0.22.0 =0.22.0 No README data run build-docs # invoked by publisher when publishing docs on the mb-pages --debug --standalo mapboxgl > && tap --no-cover build --github --format html -c --theme ./docs/_th --output --debug -t unassertif --plugin [minifyify --map --output --standalo mapboxgl > && tap --no-cover --debug -t envify > --ignore-p .gitignore js test bench diff --name-onl mb-pages HEAD -- | awk '{print | xargs build-toke watch-dev watch-benc build-toke watch-benc build-toke watch-dev run build-min && npm run build-docs && jekyll serve --no-cache --localhos --port 9966 --index index.html .\",test:\"n run lint && tap --reporter dot test/js/*/ && node && watchify bench/inde --plugin [minifyify --no-map] -t [babelify --presets react] -t unassertif -t envify -o bench/benc --debug --standalo mapboxgl -o n=new r=new r(t){var n(t,n){var i(t){retur t)return t){var 1=0)return V=1;V specify vertex creation specify cell creation specify phase strict\";va n(t){if(t in l)return l[t];for(v new Invalid boundary dst;};retu t in l){var t in u){var t in c){var return \"+s),u){va p=new p=new p()}functi for(var o=0;o1)for f(e,r){var s=\"__l\"+ i=\"__l\"+ _=[\"'use L=new L=new L(r)}funct s(t,e){var r=[\"'use [2,1,0];}e [1,0,2];}} [2,0,1];}e new new function new o=new 0===t){var 0===r){r=n o(t,e){var s(t,e){ret a(t,e){var i=new t||\"up\"in strict\";va r=void 0!==r?r+\"\" e(t,e){for t}function o)throw new to path.resol must be t)throw new to path.join must be n(t){for(v new Error(\"Giv varint doesn't fit into 10 bytes\");va o(t,e,r){v s(t,e){for new type: void n(t){var 0:return r||[];case 1:return 2:return Array(t);v r}var r(t,e){var Array(a),n n(t,e){for a(t){for(v t-e});var new t instanceof i(t){retur a(t){for(v a=1;i;){va l(t){for(v c(t){retur d(t){var u(m)}funct p(t){var 0x80 (not a basic code x});else for(_ in n(t,e){ret o;var o};var n(t,e){for n&&void e(t){var e=new Error(\"(re \"+t);throw n(t){retur t?\": i(t,r,i){t in r||e(\"unkn parameter possible values: parameter type\"+n(r) must be a typed parameter type\"+n(i) expected \"+r+\", got \"+typeof t)}functio parameter type, must be a nonnegativ shader source must be a string\",a) number \"+t+\": r=0;e(c(\"| compiling \"+s+\" shader, linking program with vertex shader, and fragment shader i(t){retur M(t,r){var n=m();e(t+ in command called from \"+n))}func A(t,e,r,i) in e||M(\"unkn parameter possible values: parameter type\"+n(r) expected \"+e+\", got \"+typeof texture format for renderbuff format for L(t,e){ret z(t,e,n){v pixel arguments to document,\" manually specify webgl context outside of DOM not supported, try upgrading your browser or graphics drivers name must be string\");v $(t){var et(t,e){va _e:r=new we:r=new Me:r=new ke:r=new Ae:r=new Te:r=new Se:r=new null}retur n=0;n0){va t[0]){var buffer data\")}els shape\");va data for buffer p=new n(a);retur d=[];retur t=0;return t&&t._buff instanceof a(t){var e||(e=new Ge:case Xe:case Ze:case type for element bit element buffers not supported, enable first\");va vertex count for buffer a}var t&&t._elem instanceof pt(t){for( At(t){retu Tt(t,e){va Or:case Fr:case Rr:case jr:var texture type, must specify a typed St(t,e){re for(var s}return o*r*n}func texture texture unpack n){var must enable the extension in order to use floating point must enable the extension in order to use 16-bit floating point must enable the extension in order to use depth/sten texture must be an extension not extension not d(e,r,i){v m(){return K.pop()||n h}function y(t,e,r){v b(t,e){var e){var e){var e){var e){var e){var i(t,e){var arguments to format for c=new T(nr);retu format for C=new z=new I(){for(va for(var P={\"don't care\":$r,\" mipmap mipmap mipmap mipmap s3tc dxt1\":Mr,\" s3tc dxt1\":kr,\" s3tc dxt3\":Ar,\" s3tc atc\":Sr,\"r atc explicit atc interpolat pvrtc pvrtc pvrtc pvrtc etc1\"]=Pr) r=B[e];ret null});ret number of texture shape for z||\"colors render targets not color buffer must enable in order to use floating point framebuffe must enable in order to use 16-bit floating point framebuffe must enable to use 16-bit render must enable in order to use 32-bit floating point color color format for color format for extension not u=d=1;var for(D=new color attachment \"+a+\" is color attachment much have the same number of bits per depth attachment for framebuffe stencil attachment for framebuffe depth-sten attachment for framebuffe not resize a framebuffe which is currently in use\");var i;for(var shape for framebuffe must be be d||\"colors render targets not color buffer color color format for l=1;var a(t){var t=0;return vertex fragment shader\",n) a=i[t];ret a||(a=new o(o){var must create a webgl context with in order to read pixels from the drawing cannot read from a from a framebuffe is only allowed for the types 'uint8' and from a framebuffe is only allowed for the type 'uint8'\")) arguments to buffer for regl.read( too s(t){var r;return l(t){retur l}function jt(t){retu Nt(t){retu Bt(){funct t(t){for(v r(){functi n(){var e=a();retu n(){var new new m(t){retur v(t,e,r){v g(t,e,r){v y(){var ei:var ri:return ni:return ii:return ai:return c={};retur n=e.id(t); in c)return c[n];var b(t){var in r){var if(Di in n){var e}function x(t,e){var in r){var i=r[Pi];re framebuffe in n){var a=n[Pi];re framebuffe null}funct n(t){if(t in i){var in a){var \"+t)});var in in e?new s=o;o=new w(t){funct r(t){if(t in i){var r});return n.id=r,n}i in a){var o=a[t];ret null}var r(t,r){if( in n){var in i){var s=i[t];ret in n){var in i){var o=i[Ri];re in n){var t=n[ji];re Be[t]})}if in i){var r=i[ji];re in \"+n,\"inval primitive, must be one of Aa}):new in n){var vertex t})}if(Ni in i){var r=i[Ni];re vertex s?new vertex offset/ele buffer too l=new k(t,e){var o(e,n){if( in r){var o})}else if(t in i){var vi:case si:case oi:case Ai:case hi:case Ci:case xi:case wi:case Mi:case pi:return flag fi:return in \"+i,\"inval \"+t+\", must be one of di:return color attachment for framebuffe sent to uniform data for uniform a[r],\"inva uniform or missing data for uniform T(t,r){var a&&a,\"inva data for attribute offset for attribute divisor for attribute parameter \"'+r+'\" for attribute pointer \"'+t+'\" (valid parameters are in r)return r[s];var in '+a+\"&&(ty dynamic attribute if(\"consta in \"+a+'.cons === in S(t){var a(t){var parameter L(t,e,r){v C(t,e,r,n) z(t,e,r){v n=m(e);if( in r.state)){ c,h;if(n in in I(t,e,r,n) if(mt(u)){ l(t){var ua:case da:case ga:return 2;case ca:case pa:case ya:return 3;case ha:case ma:case ba:return 1}}functio attribute i(i){var a=c[i];ret a(){functi o(){functi vertex vertex vertex i(t){retur n(e){var n=r.draw[e s(t){funct e(t){var args to args to e(t){if(t in r){var e=r[t];del delete l(t,e){var regl.clear with no buffer takes an object as cancel a frame callback must be a h(){var callback must be a function\") event, must be one of Kt={\"[obje renderbuff renderbuff arguments to renderbuff r(){return i(t){var s(){return p.pop()||n o}function u(t,e,r){v c(){var t(){var new requires at least one argument; got none.\");va e.href;var \",e);var s=new o;n=-(i+a) null;var n(t){retur n(t){for(v R;};return i(t){var e=s[t];ret strict\";\"u n(t){for(v i}function h(t,e){for r=new r}function r=new l(e)}funct u(t){for(v e=s(t);;){ t=k[0];ret f(t,e){var r=k[t];ret n(t,e){var l}else if(u)retur l}else if(u)retur u;return i(t,e){ret t.y-e}func a(t,e){for r=null;t;) t;var r}function l(t){for(v n=d.index; n(t,e){var i(t,e,r,n) o(t,e){for r}function s(t,e){for m}function s[t];for(v new unexpected new failed to parse named argument new failed to parse named argument new mixing positional and named placeholde is not (yet) s[t]=n}var n(t){for(v Array(e),n Array(e),i Array(e),a Array(e),o Array(e),s x=new u(t){retur c(t){var h(t){retur f(t){var d(t,e){for r in t}function p(t){retur t.x}functi m(t){retur t.y}var time\");var r=\"prepare \"+t.length %d clusters in c)|0 p=new Array(r),m Array(r),v Array(r),g p=new o}function s}function T(t){retur n=z(t);ret t){var r={};for(v i in e={};for(v r in n(t,e){var i(t,e){var s/6}return 1}var n&&void e(t,e){var for(a=0,n= n})}}var s;var in new Error(\"n must be new Error(\"alr s(t){retur new l(t){retur new u(t){retur new c(t){retur new h(t){retur new f(t){retur new d(t){retur new p(t){retur new m(t){retur x?new v(t){retur new n(t)}var null}retur t=0;tn)ret instanceof n)return t;var i=new n;return a(t){retur instanceof o(t,e){ret s(t,e){ret new 'url' must be a string, not \"+typeof t);var i(t,e){var a(t,e){var o(t,e){ret t}function s(t){var e={};retur a;var v=e.name?\" c(e)}var o+\": \"+s}functi d(t,e,r){v n=0;return \")+\" \"+t.join(\" \")+\" \"+t.join(\" \")+\" p(t){retur t}function v(t){retur g(t){retur t}function t}function t}function _(t){retur void 0===t}func w(t){retur M(t)&&\"[ob k(t){retur M(t)&&\"[ob A(t){retur instanceof t}function S(t){retur t||void 0===t}func E(t){retur L(t){retur t=a)return new Error(\"unk command if(7!==r)t new Error(\"unk command i(t){for(v e}var new Error(\"fea index out of new new String too long (sorry, this will get fixed later)\");v l(t){for(v e(t){var e=n(t);ret e?u in r(t,e){var o(t){var i?u in i&&delete t){var r?r[0]:\"\"} n?!r&&en)t al-ahad\",\" {0} not {0} {0} {0} mix {0} and {1} a(t,e){ret ;var format a date from another number at position name at position literal at position text found at dd M MM d, d M d M d M d M yyyy\",RSS: d M a=this;ret var _inline_1_ = - var _inline_1_ = - >= 0) !== (_inline_1 >= 0)) {\\n + 0.5 + 0.5 * (_inline_1 + _inline_1_ / (_inline_1 - }\\n n(t,e){var r=[];retur strict\";va u(r,i){ret i(t,e){var void E.remove() void null;var strict\";va void c();var t}function i(t){var e=x[t];ret a(t){retur the calendar system to use with `\"+t+\"` date data.\"}var i={};retur t}var i?\"rgba(\"+ n=i(t);ret t){var A(e,r){var T(){var void strict\";va strict\";va strict\";va strict\";va strict\";va strict\";va n(){var e(e){retur r;try{r=ne strict\";va i(t,e,r,n) a(t){var void n.remove() void \")}).split \")}).split scale(\"+e+ n,i,a;retu strict\";va 0 1,1 0 0,1 \"+a+\",\"+a+ 0 0 1 \"+a+\",\"+a+ 0 0 1 \"+r+\",\"+r+ 0 0 1 \"+r+\",\"+r+ 0 0 1 0 1,1 0 0,1 0 1,1 0 0,1 n(t,e,r,n) t.id});var strict\";va strict\";va i(t,e,r){v r(t){var void r.remove() r(e,r,o){v if(i[r]){v o;if(void strict\";va n(t){var n(r){retur strict\";va n(t){for(v \");var i(t,e){var click on legend to isolate individual l(t){var u(t){var strict\";va r[1]}retur i}function i(t){retur t[0]}var h(t){var f(t){var d(t){var n(t,e){var i(t){for(v n(t){for(v 0}}var o(t,e){var 0 1,1 0 0,1 extra params in segment t(e).repla strict\";va strict\";va u(r,i){ret r(t,e){ret l(t,e,r){v u(t,e,r){v c(t,e){var n(){return p(t,e){var g(t,e){ret y(t,e){ret b(t,e,r){v x(t,e){var _(t){for(v r(t,e){ret strict\";va strict\";va strict\";va t){var void t)return void void n}function l(t){retur u(t){retur c(t){retur d\")}functi h(t){retur d, yyyy\")}var t.getTime} r={};retur n=new a(t){retur o(t){for(v r={};retur n(){return strict\";va for(var c(t){retur void property r(t,e){var instanceof RegExp){va void o(t,e){ret t>=e}var binary r=e%1;retu n(t){var e=i(t);ret n(t,e){ret i(t){retur \")}functio a(t,e,r){v was an error in the tex null;var r=0;r1)for i=1;i doesnt match end tag . Pretending it did s}function c(t,e,r){v o(),void e();var 0,\":\"],k=n t(t,e){ret void n(t){var i(){var 1px new strict\";va strict\";va n(t,e){for r=new new Error(\"No DOM element with id '\"+t+\"' exists on the page.\");re 0===t)thro new Error(\"DOM element provided is null or previous rejected promises from t.yaxis1); array edits are incompatib with other edits\",h); full array edit out of if(void & removal are incompatib with edits to the same full object edit new Error(\"eac index in \"+r+\" must be new Error(\"gd. must be an 0===e)thro new is a required new Error(\"cur and new indices must be of equal u(t,e,r){v new Error(\"gd. must be an 0===e)thro new Error(\"tra must be in in i(t){retur a(t,e){var r=0;return new Error(\"Thi element is not a Plotly plot: \"+t+\". It's likely that you've failed to create a plot before animating it. For more details, see void c()}functi d(t){retur overwritin frame with a frame whose name of type \"number\" also equates to \"'+f+'\". This is valid but may potentiall lead to unexpected behavior since all plotly.js frame names are stored internally as This API call has yielded too many warnings. For the rest of this call, further warnings about numeric frame names will be addFrames accepts frames with numeric names, but the numbers areimplici cast to n(t){var i}function i(){var t={};retur a(t){var o(){var s(t){retur l(t){funct u(t){funct c(t){retur h(t,e,r){v f(t,e,r){v e={};retur t&&void n(t){retur Error(\"Hei and width should be pixel values.\")) l(t,e,r){v u(t,e,r,n) \"+o:s=o+(s dtick p(t,e){var c=new t.dtick){v error: t+i*e;var dtick a(t){for(v strict\";va v(r,n){ret to enter axis\")+\" e;var n(t,r){for n(t,e,r,n) u(t,e){ret y(t){var b(t,e,r){v back X(e,r){var K()}functi W(e){funct n(e){retur void k.log(\"Did not find wheel motion attributes \",e);var strict\";va n(t){retur t._id}func went wrong with axis Error(\"axi in in in o){var t(t){var e(t){retur strict\";va r(r,n){var e/2}}funct v(t,e){var g(t,e){var b(t,e){var x(t,e){var new Error(\"not yet r(t,r){for i(){for(va a(t,e){for n(t,e){var n(t){retur i(t,e,r,n) a(t,e){ret i(t,e){var r(t){retur n(t){var l(t,e){var u(t){var c(t,e){var f(t,e,r){v strict\";va i(t){var e=new n;return a(t){var o(t){var i=new n(t,e);ret strict\";va Sans Regular, Arial Unicode MS r(t,e){ret - delete t)return e,n,i={};f in i}return r=a(t);ret e&&delete P=(new + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + 0px\",\"1px -1px\",\"-1p 1px\",\"1px \"+t+\" 0 \"+n+\" \"+n+\" \"+n+\" void c=\"t: \"+u.t+\", r: l;var r in t)r in r in r=e||6;ret 0===t)retu null;var void t(){var t={};retur n.mode,del strict\";va e(e,i){ret s;return t(t,e){ret i=r[n];ret strict\";va t(t,e,r){v e(t,e){ret r(t,e){ret a(t,i){var tozoom back f(t,e){var i(t){retur y,b;return o,s;return ii))return e}return void h(t){retur f(t,e){ret void strict\";va strict\";va 0, 0, strict\";va s;return r in void strict\";va null;for(v strict\";va o(e){var s(e){var strict\";va n(t,e,r){v i(t,e,r){v strict\";va converged strict\";va strict\";va strict\";va strict\";va s(r,i){ret n(t,e,r,n) strict\";va n(t,e){for o(t){retur strict\";va void strict\";va strict\";va c(r,i){ret loop in contour?\") s(t,e,r){v 15===r?0:r many contours, clipping at i}function a(t,e,r){v o(t,e,r){v s(t,e,r,n) e=l(t,r) r(t){retur to newendpt is not vert. or perimeter scale is not scale is not void data invalid for the specified inequality many contours, clipping at strict\";va strict\";va h(t){retur to newendpt is not vert. or perimeter o(t,e,r){v s(t,e,r){v scale is not scale is not strict\";va iterated with no new in strict\";va g}var didn't converge strict\";va s=0;sa){va in strict\";va l(r,n){ret u(t){var e=l(t);ret strict\";va strict\";va e(e){var strict\";va strict\";va void r(t,e){ret traces support up to \"+u+\" dimensions at the c}var l(r,n){ret strict\";va l(n){var i}function c(t,e,r){v l(t,e,r){v n=o(r);ret u(t,e){ret c(t){retur h(t){var e=o(t);ret f(t){var d(t){retur t[0]}funct p(t,e,r){v m(t){var v(t){retur l(t){var u(t){retur c(t,e){for e.t+\"px \"+e.r+\"px \"+e.b+\"px 255, 255, 0)\");var 1px 1px #fff, -1px -1px 1px #fff, 1px -1px 1px #fff, -1px 1px 1px strict\";va i(t,e,r){v strict\";va n(t,e){for m};var strict\";va o(r,a){ret strict\";va strict\";va strict\";va n(t,e,r){v u;var 1;var a(t,e){var r(t,e){ret n(t,e){ret s(t,e){var 1;var t+\" void strict\";va strict\";va strict\";va 0, i(t,e){var r=new for(r=new is present in the Sankey data. Removing all nodes and strict\";va u(r,a){ret n(t){retur t.key}func a(t){retur t[0]}funct o(t){var 0 0 1 0 0)\":\"matri 1 1 0 0 0)\")}funct M(t){retur k(t){retur 0 0 1 0 0)\":\"matri 1 1 0 0 0)\"}functi A(t){retur 1)\":\"scale 1)\"}functi T(t){retur S(t){retur L(t,e,r){v var C(t,e,r){v i(){for(va e={};retur 1px 1px #fff, 1px 1px 1px #fff, 1px -1px 1px #fff, -1px -1px 1px strict\";va _=new strict\";va void strict\";va strict\";va m(r,a){ret strict\";va strict\";va strict\";va r(e){var i(t){var strict\";va n(t,e){var + m(t){retur v(t){retur g(t){retur t.id}funct g}function x(e){var scatter strict\";va s(t,e){ret l(t){retur M[t]}funct o=0;o=0){v n(t,e,r,n) strict\";va d(r,i){ret s=o[0];if( 0;var v.push(\"y: strict\";va strict\";va e(t){retur r(t){var 1/0;var strict\";va n(t,e){var n}function s(t,e,r,n) n=new s(t){var 1/0;var strict\";va strict\";va strict\";va strict\";va d(r,i){ret strict\";va strict\";va e=f(t);ret e=f(t);ret e=f(t);ret e=f(t);ret In\u00a0[4]: # Helper function for pulling data from quandl def '''Downloa and cache Quandl data series''' cache_path = try: f = 'rb') df = pickle.loa #print('Lo {} from except (OSError, IOError) as e: {} from df = #print('Ca {} at cache_path return df In\u00a0[5]: # Quandle codes for each dataset. data_sets = # Cool loop to define variables. dict_of_df = {} for item in data_sets: data_code = if item == \"EURGBP\": dataset = get_data( ) else: dataset = get_data( ) = dataset Format data\u00b6After importing the data it needs to be changed into a convenient form. In this notebook I download the price of Bitcoin in GBP, and the price of Ethereum and Litecoin in Bitcoin. Each of these is a different dataset on Quandl, so I copied the relevant data from each data set into one new data frame that contained everything I was interested The price of Ethereum or Litecoin in GBP is the product of their respective prices in Bitcoin and Bitcoin\u2019s price in GBP. I calculated this and created new columns to store the\u00a0result In\u00a0[6]: # helper_fun to take one column from many dfs and merge into a single new df def col): '''Merge a single column of each dataframe into a new combined dataframe' series_dic = {} for key in dict_of_df = return In\u00a0[7]: = # Merge opening price for each currency into a single dataframe df = 'Open') #df.tail() = np.NaN # convert to GBP and rename columns df['btc'] = df['gbp'] df['eth'] = df['gbp'] * df['eth_bt df['ltc'] = df['gbp'] * df['ltc_bt df['eur'] = df['EURGBP #df = axis=1) In\u00a0[8]: # put price data in its own df (\"prices\") to do growth analysis later # keep prices in GBP because GBP varies less than any other common measure. prices = 'eth', 'ltc', 'eth_btc', 'ltc_btc'] In\u00a0[9]: prices = Asset charts\u00b6Bit - \u00a3\u00b6 In\u00a0[10]: #end = #df = # Bitcoin price series1 = go.Scatter name='Pric line = dict( color = ('green'), width = 2)) series2 = go.Scatter name='7 day SMA', line = dict( color = ('blue'), width = 1)) series3 = go.Scatter name='30 day SMA', line = dict( color = ('red'), width = 1)) data = [series1, series2, series3] layout = go.Layout( title='Bit price', yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[10]: Ethereum - \u00a3\u00b6 In\u00a0[11]: #end = #df = # Ethereum price series1 = go.Scatter name='ETH' line = dict( color = ('green'), width = 2)) series2 = go.Scatter name='7 day SMA', line = dict( color = ('blue'), width = 1)) series3 = go.Scatter name='30 day SMA', line = dict( color = ('red'), width = 1)) data = [series1, series2, series3] layout = go.Layout( price', yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[11]: Ethereum - BTC\u00b6 In\u00a0[12]: #end = #df = #prices = 'eth', 'ltc', 'eth_btc', 'ltc_btc'] # Ethereum price series1 = go.Scatter line = dict( color = ('green'), width = 2)) series2 = go.Scatter name='7 day SMA', line = dict( color = ('blue'), width = 1)) series3 = go.Scatter name='30 day SMA', line = dict( color = ('red'), width = 1)) data = [series1, series2, series3] layout = go.Layout( / Bitcoin', yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[12]: LiteCoin - \u00a3\u00b6 In\u00a0[13]: #end = #df = # Litecoin price series1 = go.Scatter name='LTC' line = dict( color = ('green'), width = 2)) series2 = go.Scatter name='7 day SMA', line = dict( color = ('blue'), width = 1)) series3 = go.Scatter name='30 day SMA', line = dict( color = ('red'), width = 2)) data = [series1, series2, series3] layout = go.Layout( price', yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[13]: Litecoin - BTC\u00b6 In\u00a0[14]: # end = # df = win1 = 7 win2 = 30 # Litecoin price series1 = go.Scatter line = dict( color = ('green'), width = 1)) series2 = go.Scatter name='{} day line = dict( color = ('blue'), width = 2)) series3 = go.Scatter name='{} day line = dict( color = ('red'), width = 2)) data = [series1, series2, series3] layout = go.Layout( / Bitcoin', yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[14]: SMA Analysis\u00b6T price data for the digital assets above shows a high degree of variance, so when I first visualized the price history I wanted to smooth it somehow. I plotted a simple moving average and became curious what different types of average would look like. I noticed that the longer and shorter SMAs (Simply Moving Averages) cross each other occasional and I wondered if this would make a useful The code below (if it\u2019s hidden, click the \u2018show code\u2019 link at the beginning of post) shows how to calculate SMAs, identify when two time series cross each other, and calculate the returns from buying or selling depending on when the long SMA moves above the short SMA, or when short moves above\u00a0long I wanted to know which combinatio of SMA periods would yield the best results, and the heat maps show\u00a0this. Finally, for a given date range, asset pair and short and long SMA combinatio I plotted the performanc of the trading strategy with those parameters through time. This was to see if the algorithms performanc was consistent or if large losses could occur. It would also show how much upfront cost would have been needed to realise the\u00a0return Net gains w/ different SMA combinatio In\u00a0[15]: # identify where the two SMAs cross over each other # return the dates where this occurs, and label them buy or sell def sma1, sma2): if sma2 < sma1: sma1, sma2 = sma2, sma1 df = pd.DataFra df['btcPri = prices['bt df['data'] = prices[pai df['sma1'] = df['sma2'] = df['diff'] = df['sma1'] - df['sma2'] df['inGBP' = df['btcPri * df['data'] # sell ltc for btc when diff < 0 && cross == True # buy ltc with btc when diff > 0 && cross == True line = 0 df = df.dropna( df['nextDi = df['cross' = (((df['dif >= line) & < line)) | > line) & (df['diff' <= line)) | (df['diff' == line)) rows = == True] rows['trad = '...' rows['trad (rows['cro == True) & (rows['dif > 0) ] = 'sell' rows['trad (rows['cro == True) & (rows['dif < 0) ] = 'buy' # df = all the data # rows = just the rows of df where SMA1 crosses SMA2 out = {'data':df 'xOver':ro return out In\u00a0[16]: 3, In\u00a0[17]: # take the crossOver data and calculate how much would you would gain or lose between # the selected dates def returns(pa sma1, sma2, dt1, dt2 = # make sure dt1, dt2 are correctly formatted! data = sma1, sma2) # crossOver returns a dictionary with 2 items # just the rows where SMA1 crosses SMA2 # just the rows between the dates we're interested in trades = # must start with a buy. delete the first row if its a sell if != 'buy': trades = # calc the profit # make nice labels for the return dict buys = sells = buysGBP = sellsGBP = p = sells - buys pGBP = sellsGBP - buysGBP results = {'pGBP': pGBP, 'profit': p,'trades' count,'sum of buys': buys,'sum of sells': sells, 'data':tra 'pair':pai return results In\u00a0[18]: # This function calls the other two functions (defined above) # Input the asset pair, start and finish dates, and the range of SMAs to calc # Returns an ok-ish heatmap def pair, maxDays, dt1, dt2 = tbl = maxDays)) for i in for j in if j<=i: tbl[i,j] = np.NaN else: tbl[i,j] = ['proift'] #tbl trace = ) data=[trac layout = go.Layout( title='{} ) fig = layout=lay #out = out = py.iplot(f return out In\u00a0[19]: maxDays=30 maxDays=30 Out[19]: Out[19]: Returns through time for one combinatio of sma1 and sma2\u00b6 In\u00a0[20]: # This function creates a plot showing the profit through time for a given input def pair, sma1, sma2, dt1, dt2 = out = returns(pa sma1, sma2, dt1) ts = out['data' ts['data'] = == 'buy', ts['data'] * -1, ts['data'] ts['dataGB = == 'buy', ts['inGBP' * -1, ts['inGBP' ts['return = = = = #ts series1 = go.Scatter line = dict( color = ('blue'), width = 1)) series2 = go.Scatter name='av', line = dict( color = ('#137a28' width = 2)) data = [series1, series2] layout = go.Layout( title='{}: sma1 = {}d, sma2 = sma1, sma2 ), yanchor='t y=1.1, x=0.5) ) fig = layout=lay plot = py.iplot(f #plot = results = {'data': ts , 'plot':plo } return results In\u00a0[21]: sma1=8, sma2=5, pltly_name = Out[21]: In\u00a0[22]: sma1=10, sma2=6, Out[22]: Next steps:\u00b6 Create a bot to monitor real time price data, calculate the moving averages, and place\u00a0trad If you\u2019d like to collaborat with me to do this, please contact\u00a0me if { var mathjaxscr = = = = ? \"innerHTML : \"text\")] = + \" config: + \" TeX: { extensions { autoNumber 'AMS' } },\" + \" jax: + \" extensions + \" displayAli 'center',\" + \" displayInd '0em',\" + \" showMathMe true,\" + \" tex2jax: { \" + \" inlineMath [ ['$','$'] ], \" + \" displayMat [ ['$$','$$' ],\" + \" true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS' { \" + \" linebreaks { automatic: true, width: '95% container' }, \" + \" styles: { .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important' }\" + \" } \" + \"}); \"; (document. || }"},{"title":"I\u2019m a chartered\u00a0accountant","url":"chartered.html","body":"Earlier this year I qualified as a chartered accountant Qualificat requires passing 15 exams and gaining 3150 hours of To celebrate passing your exams and verify your success, the ICAEW print the names of everyone who passed in an advert in the Financial Times. This happened for me on the 26\u00a0January"},{"title":"Coworking in\u00a0Dublin","url":"coworking.html","body":"Last week I arrived in Dublin and had two days to find a coworking space. I ran around Dublin visiting as many as\u00a0possibl I\u2019m looking to rent a hotdesk, which means I don\u2019t have any storage space at the office and I don\u2019t have a Below are my impression of eight businesses offering either hot desks or dedicated desks, ranked in order of preference It\u2019s subjective The going rate in Dublin seems to be \u20ac200 \u2014 \u20ac300 per month for a hot desk. Hours range from 9am - 7pm Monday - Friday, to\u00a024/7. First place: Dogpatch\u00a0L Dogpatch offer a coworking space and access to a community of tech startups. They run regular networking and mentoring events and occupy 3 floors of a warehouse next to an old dock. The offices have a range of interiors styles. The middle level is fairly standard but pleasant open plan offices, the top floor is a flexible working space with event areas. The lower level is a series of vaults that each contain meeting rooms of various sizes. There\u2019s a lot going on and there\u2019s a good buzz in the\u00a0air. The range of interior spaces is a big plus for me and I\u2019m looking forward to being able to switch up my working environmen through the day. There\u2019s a couple of kitchen areas, and a table football and table-tenn table on the top level. Access is 6am - midnight, and a hot desk is \u20ac200 per month. A dedicated desk is\u00a0\u20ac400. Dogpatch occupy less than a quarter of the warehouse building, the remainder of the space is used as a shopping mall and a museum. In my opinion Dogpatch is by far the best value and if you\u2019re working in tech it\u2019ll probably be your Second place: Studio9 These guys are great. The space is a lot smaller that Dogpatch, comprising only ~12 desks. It\u2019s a basement, but don\u2019t let that put you off \u2014 its open plan and has large windows at either end. The space is well designed and uses a lot of light colours and textures, including wood on the walls and floors. There are also plenty of pot plants. These combine to give a bright and airy atmosphere that feels very natural and\u00a0bright There are alcoves down one side of the main room which serve as meeting rooms, and a garden and basic kitchen at the back. The owners want the space to foster a community, and I expect that this is achieved. There are no dedicated offices and no teams using multiple desks, everyone is an The desks are large, and each has its own little wall to provide some privacy. \u20ac200 per month ongoing, or \u20ac250 for just one month. They offer a trial day for\u00a0free. Third place: OfficeSuit The building looks well run and appears clean and fresh with adequate resources. The building is an old Georgian tenement with high ceilings, cornicing and lots of natural light. The furnishing and decor lean towards corporate rather than something more interestin but this works well to add a profession feel to the homely architectu Access is\u00a024/7. There\u2019s only two rooms for hotdesking and most of the building is given to private offices. There\u2019s a garden with bike storage, meeting rooms and a quiet room for calls. Lockers are extra. They also offer a free day as a\u00a0trial. \u20ac249 p/m for desk access 9am-7pm Monday -\u00a0Friday Fourth place: Element78 Friendly and well resourced, but very corporate. The architectu is corporate glass and steel. They\u2019re situated on the ground floor in a posh business district with large financials for neighbours It\u2019s too corporate for me, but if you wanted an impressive place to meet clients and a nice address, this could be it. There were about 15 hot-desks for rent, plus plenty of dedicated desks. Clients seemed to include mostly young tech companies and I\u2019m hoping to find somewhere with more of a community, more inspiring architectu and more character. I was offered a free day as a\u00a0trial. \u20ac200 for the first month then \u20ac350\u00a0p/m Fifth place: Glandore Glandore offer a relatively luxurious package with a corporate feel. They have a few buildings in Dublin and only the flagship has hotdesking space. There\u2019s a super looking restaurant and a large club room to relax in, but these are features that I don\u2019t need, and the rates aren\u2019t competitiv if all you need is a desk and somewhere to take calls. It\u2019s set up for teams and for impressing clients, and independen tech workers are probably better off\u00a0elsewh \u20ac295\u00a0p/m Sixth place: Regus Regus offer a polished on boarding experience and the friendly staff were quick to respond and generally helpful, but the office space was bland, grey and generic. The rooms I visited didn\u2019t have external windows and it reminded me of the rooms banks often put auditors in. If grey walls and tube lights are your thing then this is for you. 24/7 access, and meeting rooms equipped with\u00a0A/V. \u20ac299.70\u00a0p/ Seventh place: CoCreate I visited the southern branch and thought the building was a bit shabby and needed a new layer of paint. I recognised the desks as some of the cheapest available from IKEA. The rooms were small and needed cleaning, and there was weird art on the\u00a0wall. The thought of paying \u20ac200 a month to sit at a small wobbly desk put me off. The place was also almost deserted. Maybe their other branch is better, but this is not for\u00a0me. \u20ac220\u00a0p/m. Last place: tCube Last and least, tCube seems to be putting in zero effort. When I visited I saw two rooms that needed painting, disorganis furniture and abandoned bits of computers lying around. The kitchen isn\u2019t high spec and the meeting room isn\u2019t big enough. I was also given a speech about how great the wifi was - a prerequisi that was taken for granted everywhere else. At \u20ac300 p/m for a dedicated desk its easy to find better"},{"title":"Bitnation","url":"bitnation.html","body":"I\u2019m\u00a0Consul Next week I begin working with Bitnation as their finance officer. I\u2019m excited about this because I get to work on a really ambitious project using new technology to do something innovative and\u00a0valuab Bitnation Bitnation\u2019 purpose is to offer the same services as government do, in a way that delivers more benefit to\u00a0users. In the West this may not immediatel sound like a big deal. Our government are fairly organised and the services are usually \u201cgood enough\u201d. Most significan we are not used to thinking about ID services (passports visa, drivers licences) or registrati services (land registry, marriage certificat as a service that we are customers of - like our internet service provider, or our In many parts of the world dysfunctio or unjust government represent a huge obstacle to improving everyday life, the progress and achievemen that many people can hope to realise is limited because of\u00a0this. If there was a viable alternativ to a passport from a jurisdicti renowned for forgery, or a credit rating that acknowledg your land holdings despited your government inability to maintain a credible database, then you could begin to travel, trade and enjoy the benefits that citizens of many western states take for\u00a0grante I\u2019m excited that I get to use my skills in an innovative tech company that is aiming to do something Services include secure ID systems, asset registry and dispute resolution Identifica (in particular is an area full of problems, and blockchain tech could offer some really significan improvemen Bitnation wants to create a platform where voluntary nations can be created and administer and where people can choose what jurisdicti system they are part of. If this is widely implemente it will Jurisdicti would offer their own services according to their own principles and because they are easy to create and membership is voluntaril jurisdicti would compete to attract citizens. This should lead to improvemen for the users of each service, and is intended to provide an alternativ to the slow, expensive and opaque processing methods commonly associated with services from geographic"},{"title":"Create a Multi-Signature Ethereum wallet using\u00a0Parity","url":"ethereum-parity-multisig-wallet.html","body":"I recently set up a multi-sig Ethereum wallet and I couldn\u2019t find clear instructio Here they are, I hope these instructio are useful for someone looking to get\u00a0starte You\u2019ll need a way to interact with the Ethereum blockchain in order to deploy a wallet. There are several apps that you can use. I\u2019ve used Parity because I found it simple and\u00a0quick. Wallets are a type of contract and there are two types of wallet, the Multi-Sig wallet and the Watch wallet. An Ethereum account is required to communicat with a contract so if you want a multi-sig wallet with 3 signatorie (for example) then you will need to have set up at least 1 of those 3 Ethereum accounts before creating the\u00a0wallet Parity From their\u00a0webs Integrated directly into your Web browser, Parity is the fastest and most secure way of interactin with the You can do a bunch of stuff with Parity including mining Ether, manage accounts, interact with different dapps, send/recei from different accounts, and set up contracts. On the accounts tab, you can quickly set up wallets. If you use the Chrome plugin you will also get handy notificati when transactio are confirmed or Download and open\u00a0Parit For MacOS you can download and install Parity by visiting the Parity site and downloadin the installer, or from the terminal using curl or\u00a0Homebre Simple\u00a0opt $ bash <(curl -kL) Homebrew Detailed instructio are here. brew tap brew install parity --stable If you used the installer, then you open Parity opening the app and then using the logo in the\u00a0menuba If you used Brew, then start Parity with the parity and then go to the following address in your\u00a0brows You should now see something similar to\u00a0this: Add or Select the Accounts tab from the top of the page and then select \u201c+ Account\u201d. Either create new accounts or import them using your preferred method. You don\u2019t need to import all the accounts that will be part of the multi-sig wallet, but you will need to import or create the account that will own the wallet you are about to create. This account will need to have a large enough Ether balance to pay the transactio costs to deploy the multi-sig wallet onto the Blockchain The costs are tiny, but they are greater than\u00a0zero. Create the Once you\u2019ve either created or imported the account which will deploy the wallet, select \u201c+ Wallet\u201d from the accounts tab and choose \u201cMulti-Sig wallet\u201d. Click\u00a0next Enter a name for the wallet, if you want you can add a local descriptio The \u201cFrom account\u201d will be the contract owner and this account\u00a0wi Be one of Need to have enough Ether to pay for the execution of the contract on Click the \u201c+\u201d button under \u201cOther wallet owners\u201d to add the address of the other signatory accounts. You\u2019ll need to add one line for each signatory and these accounts will also own the wallet once it is\u00a0deploye In the \u201crequired owners\u201d section, specify how many accounts will need to approve a transactio that is above the daily Use the \u201cwallet day limit\u201d to set how much Ether can be spent by each account per day without needing another account to approve the transactio Set an amount of 0 if you want all transactio to require approval, or turn the option off using the slider to the right (which just specifies a huge\u00a0numbe Click \u201cnext\u201d and you\u2019ll be shown a pop-up window to approve the creation of the wallet. You will need to enter the password of the account which is creating the wallet, and once you click \u201cConfirm request\u201d the funds in the creators accounts will be used to deploy the contract on chain and create the Adding an existing Once your wallet is created and deployed, you\u2019ll need to add it to other parity clients so that the other signatorie can make or confirm transactio and view the wallets balance. This is done by adding a watch\u00a0wall Process: Accounts tab > + Wallet > Watch wallet > enter the address of the The other signatorie will now be able to view the wallet\u2019s balance, get notificati about pending confirmati and be able to make and Managing a Anyone can put funds into the wallet, just like a normal account. Just send Ether to the At the top of the page you click \u201cEdit\u201d to change the local name and descriptio of the\u00a0wallet \u201cSettings\u201d allows you add or remove owners (signatori of the wallet and change the required number of approvals and the wallet day limit. If you change these settings then the changes will need to be executed on the blockchain and the account requesting the change will therefore need to pay the required funds. Depending on the settings being changed, other accounts will need to approve the changes before they \u201cForget\u201d will remove the multi-sig wallet from your accounts\u00a0t Moving funds out of a Click on \u201cTransfer\u201d in the wallet management window (pictured above) to begin withdrawin funds from the\u00a0wallet Select the token you want to transfer - Ethereum is the only \u201cSender address\u201d - specify which account wants to withdraw the funds from the \u201cRecipient address\u201d - specify which account will receive the\u00a0funds. \u201cAmount to transfer\u201d - specify how much you want to transfer. If the amount is greater than the remaining daily limit you will get a warning bar telling you the transactio will require confirmati from other wallet\u00a0own If you want to specify the maximum transactio fee (a payment with a lower fee will be confirmed more slowly than usual) tick the \u201cadvanced sending options\u201d\u00a0b Clicking \u201csend\u201d will bring you to the confirmati stage where you can enter the password for the account which is requesting the\u00a0transf If approval from other wallet owners is required and they are also using Parity, then they can see that their approval is required in two ways: The signer tab will show there is a pending request. The wallet management window (accessed from the accounts tab) has a \u201cpending transactio section where any confirmati requests will be shown."},{"title":"Macro analysis of the Bitcoin\u00a0blockchain","url":"macro-btc.html","body":"Table of transactio confirmati time block size (daily, MB)4\u00a0\u00a0Aver number of transactio per (1MB) fees earned by miners each day6\u00a0\u00a0Rati of transactio fees to transactio of transactio per day8\u00a0\u00a0Bitc price9\u00a0\u00a0Ra of unique addresses to between each time series In\u00a0[1]: from import HTML function code_toggl { if (code_show } else { } code_show =! code_show } $( document </script> <font> This analysis was made using Python. If you'd like to see the code used, click <a ''') Out[1]: function code_toggl { if (code_show } else { } code_show =! code_show } $( document This analysis was made using Python. If you\u2019d like to see the code used, click here. window.onl = function() { code_toggl }; In\u00a0[2]: ## Steup - libraries import quandl import as plt import pandas as pd import datetime as dt import numpy as np import credential # keep my quandl and plot.ly api keys private import plotly.plo as py import plotly #from import Scatter, Layout import as go #from import * In\u00a0[3]: ## Setup - appearance # get rid of the annoying warning = None # default='w # more than one print of an unassigned variable from import = \"all\"; # offline plotly color1 = '#137a28' # dark green color2 = '#b3d1b9' # light transparen green exports, module) {/** * plotly.js v1.28.3 * Copyright 2012-2017, Plotly, Inc. * All rights reserved. * Licensed under the MIT license */ t;return function a(o,!0);va u=new Error(\"Can find module i(t,e){ret t.y-e.y}va r(t){retur r(t){retur u(){functi t(t,e){ret e(t,e){ret c(t){retur h(t){retur t.value}va t(t){var \"+s+\",\"+c+ \"+o+\",\"+u+ e=.5;retur n(t){var n=a(t,new e?e:1,r=r| \";var n(t,e){for r=new instanceof function y(t){var e}function b(t,e,r,n) e)throw new argument must be a expected unwanted i}var r=new t};var e=[];for(v r in n(t){for(v n(t){retur n(t){var i=new e=t|t-1;re new i}function l(t){for(v e=new e}function ffffffff ffffffff ffffffff ffffffff ffffffff fffffffe ffffffff ffffffff ffffffff 00000000 00000000 ffffffff ffffffff fffffffe ffffffff t){var must be greater than t instanceof can only safely store up to 53 n(void array length 26;var e=t,r=0;re 0;for(var t&&t>=0);v t&&t>=0);v new a(1);for(v t&&t>=0);v works only with positive a(0),mod:n a(0)};var i,o,s;retu i=new a(1),o=new a(0),s=new a(0),l=new i=new a(1),o=new f;return A[t];var p;else m;else new Error(\"Unk prime \"+t);e=new g}return works only with works only with red works only with works only with red l}}}functi s(t){retur l(t,e){ret 1:return s(t);case 3:return new Invalid n(t,e,r){v \"+r);var new n(t,e){var i=\"for(var i=n[t];ret var P=C+1;PZ)t new typed array length\");v e=new e)throw new Error(\"If encoding is specified then the first argument must be a l(t)}retur t)throw new argument must not be a t instanceof t)throw new argument must be a new to allocate Buffer larger than maximum size: bytes\");re 0|t}functi instanceof 0;for(var void 0:return v(t,e,r){v n=!1;if((v new hex E(n)}funct E(t){var new to access beyond buffer new argument must be a Buffer new out of new out of a}function U(t){for(v a}function H(t){retur i}function Y(t){retur t!==t}var t=new browser lacks typed array (Uint8Arra support which is required by `buffer` v5.x. Use `buffer` v4.x if you require old browser new must be 0;for(var ... new must be a new of range 0;for(var 0)}var new to write outside buffer new encoding: new out of new Error(\"Inv string. Length must be a multiple of i(t){retur a(t){var o(t){retur i(t,e){ret a(t,e){for i(g,d,v,h) n(t){var i(t,e){for r=new s}function m(t,e,r){v i=new n(t){var strict\";va t;var new Error(f+\" map requires nshades to be at least size i(t,e,r,i) 0}return n(t,e){ret t-e}functi i(t,e){var 0:return 0;case 1:return t[0]-e[0]; 2:return 3:var i;var 4:var n(t){var t}function i(t){retur a(t){retur o(t){retur null}var null;var a}return a}return i(t){var e=new i=0;i0)thr new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array args\")}els new Error(\"cwi pre() block may not reference array new Error(\"cwi post() block may not reference array index\")}el new Error(\"cwi Too many arguments in pre() new Error(\"cwi Too many arguments in body() new Error(\"cwi Too many arguments in post() block\");re n(t,e,r){v l(t,e){for w=new cwise routine for new return n(t){var e=[\"'use strict'\",\" function (!(\"+l.joi && \")+\")) throw new Error('cwi Arrays do not all have the same {\"),e.push (!(\"+u.joi && \")+\")) throw new Error('cwi Arrays do not all have the same i(t,e,r){v m,v=new t;var e=[];for(v r in e=[];for(v r in e=[];for(v r in n&&void e(t,e){var n in r}function r(){}funct n(t){var e;return y(t){retur b(t){retur new Error(\"unk type: i(t,e){for r,n,i=new i(){if(s){ t(t){var r(t){var Array(o),l this;var 1:do{o=new 2:do{o=new 3:do{o=new t=[];retur s(){var l(){for(va a(t){retur n}}}functi l(t){retur u(t){for(v e}function c(t,e){for r in p(t){retur f(t)in this._&&de v(){var t=[];for(v e in t}function g(){var t=0;for(va e in t}function y(){for(va t in x(t){retur t}function function() w(t,e){if( in t)return Z(t,e){ret J(t,e){var K(t){var vt(t){retu gt(t){retu yt(t){retu _t(t){retu wt(t){retu kt(t,e,r){ Ot(){for(v t}function Ft(){for(v Nt(t){var b=u&&h;ret Bt(t){retu t+\"\"}funct n(e){var in e}function Gt(t,e,r){ le(t){var ce(t){for( ge(t){var ye(t,e){re we(t){var ke(t,e){re y}}functio Fe(t){retu Re(){var r=e;return r(t){var a(t,e){ret o(t,e){var l(t){for(v c(i,a){ret Je(){funct s}function 0 1,1 0 1,1 $e(){funct t(t,n){var er(){funct t(t,e){var rr(t){func s}function nr(t){func r(e){retur n(e){funct a(r,n){var k}function ir(t){var sr(t){retu t})()}func lr(t){func e(t){retur i(){return ur(t){retu r(t,e){var Vr(t,e){va n;var s;var Hr(t,e){va Vr(r,e);va Gr(t){for( r}function wn(t,e){va kn(t){retu An(t){retu 1;var Zn(t,e){va n}function bi(t){retu xi(t,e){re _i(t,e){re Ei(t){func Ni(t){retu t.y})}func Bi(t){retu Ui(t){var Vi(t){var qi(t,e){va a(t){retur o(t)}var o,s;return Qi(t,e){re $i(t,e){re a(t){retur o(e){retur t(i(e))}re _a(t){func e(e){funct Ma(t){retu ka(t){for( Aa(t){for( p[n]:delet t[r],1}var io(t){retu n(e){retur t(e)}funct i(t,r){var r};var t;var e=new b;if(t)for h(){functi f(){functi t(){var in r(){var n(){var o;if(i)ret i=!1,a;var e=new ms={\"-\":\"\" %b %e %X this.s}};v bs=new e(e,r){var t(){var e(){return }var new t(e,r,n,i) c}function e(t){for(v r}function t(t,a){var t(t,e){for i(t,e,r,n) \"+e}functi 0,0 \"+n}var t(t,i){var \"+l[2]+\" \"+l[3]}var 0,\"+e+\" \"+e+\",\"+e+ a(){functi v(){var l;var t;e||(e=t) e}function s(t){var l(t,e,r,n) u(t,e,r){v n=t;do{var n}function l=t;do{for h(t,e,r,n) r}function m(t,e,r,n) v(t){var t}function x(t,e){ret w(t,e){ret k(t,e){var A(t,e){ret n(t,e){var e){e=0;for warning: possible EventEmitt memory leak detected. %d listeners added. Use to increase must be a function\") n=!1;retur must be a e=typeof o(t,e,r,n) \"+i+\"=== typeof s(t,e){ret e.length> 1; if (a[m] === v) return true; if (a[m] > v) j = m - 1; else i = m + 1;}return false; }(\"+n+\", u(t){retur in p\"}functio h(t,e){ret c[1]){var s[e][t];va i(t){retur new a(t,e){ret new r}var 0)}functio d(t){for(v m(t){retur new t=[];retur t=[];retur 1:return 2:return new new new new this.tree; e=new i=0;i0)ret new Error(\"Can update empty node!\");va r=new new s(t){for(v z%d-%d-%d (features: %d, points: %d, simplified down to parent tile down\");var i(t,e,r){v s}function i(t,e,r,n) s(t,e){var r=new i(t);retur e(e,r,n){i in t){var U=g,V=_,k= 0.0) {\\n vec3 nPosition = mix(bounds bounds[1], 0.5 * (position + 1.0));\\n gl_Positio = projection * view * model * 1.0);\\n } else {\\n gl_Positio = }\\n colorChann = mediump GLSLIFY 1\\n\\nunifo vec4 vec3 main() {\\n gl_FragCol = colorChann * colors[0] + \\n colorChann * colors[1] +\\n colorChann * vectorizin d=new o(t,e,r,n) s;var r}function a(t,e){for r=0;rr)thr new If resizing buffer, must not specify a(t,e){for new Invalid type for webgl buffer, must be either or new Invalid usage for buffer, must be either gl.STATIC_ or t&&void new Cannot specify offset when resizing new Error(\"gl- Can't resize FBO, invalid new Error(\"gl- Parameters are too large for new Error(\"gl- Multiple draw buffer extension not new Error(\"gl- Context does not support \"+s+\" draw buffers\")} new Error(\"gl- Context does not support floating point h=!0;\"dept new Error(\"gl- Shape vector must be length 2\");var null;var 0.25) {\\n discard;\\n }\\n gl_FragCol = highp GLSLIFY 1\\n\\nattri vec2 aHi, aLo, vec4 pick0, vec2 scaleHi, translateH scaleLo, translateL float vec4 pickA, scHi, vec2 trHi, vec2 scLo, vec2 trLo, vec2 posHi, vec2 posLo) {\\n return (posHi + trHi) * scHi\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * main() {\\n vec2 p = translateH scaleLo, translateL aHi, aLo);\\n vec2 n = width * * vec2(dHi.y -dHi.x)) / gl_Positio = vec4(p + n, 0, 1);\\n pickA = pick0;\\n pickB = mediump GLSLIFY 1\\n\\nunifo vec4 vec4 pickA, pickB;\\n\\n main() {\\n vec4 fragId = 0.0);\\n if(pickB.w > pickA.w) {\\n fragId.xyz = pickB.xyz; }\\n\\n fragId += fragId.y += floor(frag / 256.0);\\n fragId.x -= floor(frag / 256.0) * 256.0;\\n\\n fragId.z += floor(frag / 256.0);\\n fragId.y -= floor(frag / 256.0) * 256.0;\\n\\n fragId.w += floor(frag / 256.0);\\n fragId.z -= floor(frag / 256.0) * 256.0;\\n\\n gl_FragCol = fragId / highp GLSLIFY 1\\n\\nattri vec2 aHi, aLo, vec2 scaleHi, translateH scaleLo, translateL float projectVal scHi, vec2 trHi, vec2 scLo, vec2 trLo, vec2 posHi, vec2 posLo) {\\n return (posHi + trHi) * scHi\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * main() {\\n vec2 p = translateH scaleLo, translateL aHi, aLo);\\n if(dHi.y e+n;var null;var FLOAT_MAX) {\\n return vec4(127.0 128.0, 0.0, 0.0) / 255.0;\\n } else if(v \"+t[1]+\", \"+t[2]+\", t=new e=new r=new \"+t[1]+\", n=\"precisi mediump GLSLIFY 1\\n\\nunifo vec3 float vec3 vec4 f_id;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n gl_FragCol = vec4(pickI mediump GLSLIFY 1\\n\\nattri vec3 position, vec4 vec2 uv;\\n\\nuni mat4 model\\n , view\\n , vec3 eyePositio , vec3 f_normal\\n , , , vec4 vec2 f_uv;\\n\\nv main() {\\n vec4 m_position = model * vec4(posit 1.0);\\n vec4 t_position = view * m_position gl_Positio = projection * t_position f_color = color;\\n f_normal = normal;\\n f_data = position;\\ f_eyeDirec = eyePositio - position;\\ = lightPosit - position;\\ f_uv = mediump GLSLIFY 1\\n\\nfloat x, float roughness) {\\n float NdotH = max(x, 0.0001);\\n float cos2Alpha = NdotH * NdotH;\\n float tan2Alpha = (cos2Alpha - 1.0) / cos2Alpha; float roughness2 = roughness * roughness; float denom = * roughness2 * cos2Alpha * cos2Alpha; return exp(tan2Al / roughness2 / vec3 vec3 vec3 float roughness, float fresnel) {\\n\\n float VdotN = 0.0);\\n float LdotN = 0.0);\\n\\n //Half angle vector\\n vec3 H = + //Geometri term\\n float NdotH = H), 0.0);\\n float VdotH = H), 0.000001); float LdotH = H), 0.000001); float G1 = (2.0 * NdotH * VdotN) / VdotH;\\n float G2 = (2.0 * NdotH * LdotN) / LdotH;\\n float G = min(1.0, min(G1, G2));\\n \\n //Distribu term\\n float D = //Fresnel term\\n float F = pow(1.0 - VdotN, fresnel);\\ //Multiply terms and done\\n return G * F * D / max(3.1415 * VdotN, vec3 float roughness\\ , fresnel\\n , kambient\\n , kdiffuse\\n , kspecular\\ , sampler2D vec3 f_normal\\n , , , vec4 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n\\n vec3 N = vec3 L = vec3 V = \\n {\\n N = -N;\\n }\\n\\n float specular = V, N, roughness, fresnel);\\ float diffuse = min(kambie + kdiffuse * max(dot(N, L), 0.0), 1.0);\\n\\n vec4 surfaceCol = f_color * f_uv);\\n vec4 litColor = surfaceCol * vec4(diffu * + kspecular * vec3(1,1,1 * specular, 1.0);\\n\\n gl_FragCol = litColor * mediump GLSLIFY 1\\n\\nattri vec3 vec4 vec2 uv;\\n\\nuni mat4 model, view, vec4 vec3 vec2 f_uv;\\n\\nv main() {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n f_color = color;\\n f_data = position;\\ f_uv = mediump GLSLIFY 1\\n\\nunifo vec3 sampler2D float vec4 vec3 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n discard;\\n }\\n\\n gl_FragCol = f_color * f_uv) * mediump GLSLIFY 1\\n\\nattri vec3 vec4 vec2 uv;\\nattri float mat4 model, view, vec3 vec4 vec2 f_uv;\\n\\nv main() {\\n || \\n {\\n gl_Positio = } else {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n }\\n gl_PointSi = pointSize; f_color = color;\\n f_uv = mediump GLSLIFY 1\\n\\nunifo sampler2D float vec4 vec2 f_uv;\\n\\nv main() {\\n vec2 pointR = - if(dot(poi pointR) > 0.25) {\\n discard;\\n }\\n gl_FragCol = f_color * f_uv) * mediump GLSLIFY 1\\n\\nattri vec3 vec4 id;\\n\\nuni mat4 model, view, vec3 vec4 f_id;\\n\\nv main() {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n f_id = id;\\n f_position = mediump GLSLIFY 1\\n\\nattri vec3 float vec4 id;\\n\\nuni mat4 model, view, vec3 vec3 vec4 f_id;\\n\\nv main() {\\n || \\n {\\n gl_Positio = } else {\\n gl_Positio = projection * view * model * vec4(posit 1.0);\\n gl_PointSi = pointSize; }\\n f_id = id;\\n f_position = mediump GLSLIFY 1\\n\\nattri vec3 mat4 model, view, main() {\\n gl_Positio = projection * view * model * vec4(posit mediump GLSLIFY 1\\n\\nunifo vec3 main() {\\n gl_FragCol = i(t){for(v null;for(v function() E=new new s(\"\",\"Inva data type for attribute \"+h+\": new s(\"\",\"Unkn data type for attribute \"+h+\": \"+f);var new s(\"\",\"Inva data type for attribute \"+h+\": n(t){retur new i(t,e){for r=new new s(\"\",\"Inva uniform dimension type for matrix \"+name+\": new s(\"\",\"Unkn uniform data type for \"+name+\": \"+r)}var new s(\"\",\"Inva data new data type for vector \"+name+\": r=[];for(v n in e){var r}function h(e){for(v n=[\"return function new s(\"\",\"Inva data new s(\"\",\"Inva uniform dimension type for matrix \"+name+\": \"+t);retur i(r*r,0)}t new s(\"\",\"Unkn uniform data type for \"+name+\": \"+t)}}func i){var p(t){var r=0;r1){l[ u=1;u1)for l=0;l=0){v t||t}funct s(t){funct r(){for(va u=0;u 1.0) {\\n discard;\\n }\\n baseColor = color, step(radiu gl_FragCol = * baseColor. mediump GLSLIFY 1\\n\\nattri vec2 vec4 mat3 float vec4 vec4 main() {\\n vec3 hgPosition = matrix * vec3(posit 1);\\n gl_Positio = 0, gl_PointSi = pointSize; vec4 id = pickId + pickOffset id.y += floor(id.x / 256.0);\\n id.x -= floor(id.x / 256.0) * 256.0;\\n\\n id.z += floor(id.y / 256.0);\\n id.y -= floor(id.y / 256.0) * 256.0;\\n\\n id.w += floor(id.z / 256.0);\\n id.z -= floor(id.z / 256.0) * 256.0;\\n\\n fragId = mediump GLSLIFY 1\\n\\nvaryi vec4 main() {\\n float radius = length(2.0 * - 1.0);\\n if(radius > 1.0) {\\n discard;\\n }\\n gl_FragCol = fragId / i(t,e){var instanceof instanceof null;var n(t,e,r,n) highp GLSLIFY 1\\n\\n\\nvec posHi, vec2 posLo, vec2 scHi, vec2 scLo, vec2 trHi, vec2 trLo) {\\n return vec4((posH + trHi) * scHi\\n \\t\\t\\t//FI this thingy does not give noticeable precision gain, need test\\n + (posLo + trLo) * scHi\\n + (posHi + trHi) * scLo\\n + (posLo + trLo) * scLo\\n , 0, vec2 positionHi float size, vec2 char, is 64-bit form of scale and vec2 scaleHi, scaleLo, translateH float vec4 sampler2D vec4 charColor, vec2 vec2 float float main() {\\n charColor = vec2(color / 255., 0));\\n borderColo = vec2(color / 255., 0));\\n\\n gl_PointSi = size * pixelRatio pointSize = size * charId = char;\\n borderWidt = border;\\n\\ gl_Positio = positionHi positionLo scaleHi, scaleLo,\\n translateH pointCoord = viewBox.xy + (viewBox.z - viewBox.xy * * .5 + highp GLSLIFY 1\\n\\nunifo sampler2D vec2 float charsStep, pixelRatio vec4 vec4 vec2 vec2 float float main() {\\n\\tvec2 pointUV = (pointCoor - + pointSize * .5) / = 1. - texCoord = ((charId + pointUV) * charsStep) / dist = alpha\\n\\ti (dist t;){var w.push(new i(){var a(t,e){var e=void null;var number of characters is more than maximum texture size. Try reducing x=0;x 1.0) {\\n discard;\\n }\\n vec4 baseColor = color, float alpha = 1.0 - pow(1.0 - baseColor. fragWeight gl_FragCol = * alpha, highp GLSLIFY 1\\n\\nvec4 pfx_1_0(ve scaleHi, vec2 scaleLo, vec2 translateH vec2 translateL vec2 positionHi vec2 positionLo {\\n return + translateH * scaleHi\\n + (positionL + translateL * scaleHi\\n + (positionH + translateH * scaleLo\\n + (positionL + translateL * scaleLo, 0.0, vec2 positionHi vec4 vec2 scaleHi, scaleLo, translateH float vec4 vec4 main() {\\n\\n vec4 id = pickId + pickOffset id.y += floor(id.x / 256.0);\\n id.x -= floor(id.x / 256.0) * 256.0;\\n\\n id.z += floor(id.y / 256.0);\\n id.y -= floor(id.y / 256.0) * 256.0;\\n\\n id.w += floor(id.z / 256.0);\\n id.z -= floor(id.z / 256.0) * 256.0;\\n\\n gl_Positio = scaleLo, translateH translateL positionHi positionLo gl_PointSi = pointSize; fragId = mediump GLSLIFY 1\\n\\nvaryi vec4 main() {\\n float radius = length(2.0 * - 1.0);\\n if(radius > 1.0) {\\n discard;\\n }\\n gl_FragCol = fragId / i(t,e){var e(e,r){ret e in n(t,e){var in r)return r[t];for(v o=r.gl d(t){var null;var a(t,e){ret new E=new i(t,e){var r=new n(t);retur 0.0 ||\\n || {\\n discard;\\n }\\n\\n vec3 N = vec3 V = vec3 L = {\\n N = -N;\\n }\\n\\n float specular = V, N, roughness) float diffuse = min(kambie + kdiffuse * max(dot(N, L), 0.0), 1.0);\\n\\n //decide how to interpolat color \\u2014 in vertex or in fragment\\n vec4 surfaceCol = .5) * vec2(value value)) + step(.5, vertexColo * vColor;\\n\\ vec4 litColor = surfaceCol * vec4(diffu * + kspecular * vec3(1,1,1 * specular, 1.0);\\n\\n gl_FragCol = mix(litCol contourCol contourTin * mediump GLSLIFY 1\\n\\nattri vec4 uv;\\nattri float f;\\n\\nunif mat3 mat4 model, view, float height, sampler2D float value, kill;\\nvar vec3 vec2 vec3 eyeDirecti vec4 main() {\\n vec3 dataCoordi = permutatio * vec3(uv.xy height);\\n vec4 worldPosit = model * 1.0);\\n\\n vec4 clipPositi = projection * view * clipPositi = clipPositi + zOffset;\\n gl_Positio = value = f;\\n kill = -1.0;\\n = = uv.zw;\\n\\n vColor = vec2(value value));\\n //Don't do lighting for contours\\n surfaceNor = vec3(1,0,0 eyeDirecti = vec3(0,1,0 lightDirec = mediump GLSLIFY 1\\n\\nunifo vec2 vec3 float float value, kill;\\nvar vec3 vec2 vec3 v) {\\n float vh = 255.0 * v;\\n float upper = floor(vh); float lower = fract(vh); return vec2(upper / 255.0, floor(lowe * 16.0) / main() {\\n if(kill > 0.0 ||\\n || {\\n discard;\\n }\\n vec2 ux = / shape.x);\\ vec2 uy = / shape.y);\\ gl_FragCol = vec4(pickI ux.x, uy.x, ux.y + i(t){var o(t,e){var new invalid coordinate for new Invalid texture size\");ret s(t,e){ret new Invalid ndarray, must be 2d or 3d\");var new Invalid shape for new Invalid shape for pixel new Incompatib texture format for new Invalid texture new Floating point textures not supported on this platform\") s=u(t);ret s=u(t);ret f(t,e){var new Invalid texture size\");var new Invalid shape for new Invalid shape for pixel b=u(t);ret new Error(\"gl- Too many vertex n(t,e,r){v i=new n(t){for(v n(t,e){var n(t,e,r){v instanceof a=new a(t,e){ret o(t){for(v e=[\"functi orient(){v orient\");v n=new a(t,e){var o(t,e){var s(t,e){var i}}functio c(t,e){for s(this,t); s(this,t); b}for(var r}return n}return l}function i(t,e,r,n) n(t,e){var r;if(h(t)) new Error('Unk function type -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n\\n // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def float vec2 vec2 vec2 vec2 vec2 vec2 float float sampler2D vec2 vec2 float float main() {\\n // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line or when fading out\\n // float blur = u_blur * float alpha = clamp(min( - (v_linewid - blur), v_linewidt - dist) / blur, 0.0, 1.0);\\n\\n float x_a = / 1.0);\\n float x_b = / 1.0);\\n float y_a = 0.5 + (v_normal. * v_linewidt / float y_b = 0.5 + (v_normal. * v_linewidt / vec2 pos_a = vec2(x_a, y_a));\\n vec2 pos_b = vec2(x_b, y_b));\\n\\n vec4 color = pos_a), pos_b), u_fade);\\n alpha *= u_opacity; gl_FragCol = color * gl_FragCol = highp lowp\\n#def floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the vec2 vec4 mat4 mediump float mediump float mediump float mediump float mediump float mat2 mediump float vec2 vec2 float float main() {\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * // We store the texture normals in the most insignific bit\\n // transform y so that 0 => -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n v_linesofa = // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def lowp vec4 lowp float float sampler2D float float vec2 vec2 vec2 vec2 float main() {\\n // Calculate the distance of the pixel from the line in pixels.\\n float dist = * // Calculate the antialiasi fade factor. This is either when fading in\\n // the line in case of an offset line or when fading out\\n // float blur = u_blur * float alpha = clamp(min( - (v_linewid - blur), v_linewidt - dist) / blur, 0.0, 1.0);\\n\\n float sdfdist_a = v_tex_a).a float sdfdist_b = v_tex_b).a float sdfdist = mix(sdfdis sdfdist_b, u_mix);\\n alpha *= smoothstep - u_sdfgamma 0.5 + u_sdfgamma sdfdist);\\ gl_FragCol = u_color * (alpha * gl_FragCol = highp lowp\\n#def floor(127 / 2) == 63.0\\n// the maximum allowed miter limit is 2.0 at the moment. the extrude normal is\\n// stored in a byte (-128..127 we scale regular normals up to length 63, but\\n// there are also \\\"special\\ normals that have a bigger length (of up to 126 in\\n// this case).\\n// #define scale 63.0\\n#def scale We scale the distance before adding it to the buffers so that we can store\\n// long distances for long segments. Use this value to unscale the vec2 vec4 mat4 mediump float mediump float mediump float mediump float vec2 float vec2 float float mat2 mediump float vec2 vec2 vec2 vec2 float main() {\\n vec2 a_extrude = a_data.xy - 128.0;\\n float a_directio = mod(a_data 4.0) - 1.0;\\n float a_linesofa = / 4.0) + a_data.w * 64.0) * // We store the texture normals in the most insignific bit\\n // transform y so that 0 => -1 and 1 => 1\\n // In the texture normal, x is 0 if the normal points straight up/down and 1 if it's a round cap\\n // y is 1 if the normal points up, and -1 if it points down\\n mediump vec2 normal = mod(a_pos, 2.0);\\n normal.y = sign(norma - 0.5);\\n v_normal = normal;\\n\\ float inset = u_gapwidth + (u_gapwidt > 0.0 ? u_antialia : 0.0);\\n float outset = u_gapwidth + u_linewidt * (u_gapwidt > 0.0 ? 2.0 : 1.0) + // Scale the extrusion vector down to a normal and then up by the line width\\n // of this vertex.\\n mediump vec2 dist = outset * a_extrude * scale;\\n\\n // Calculate the offset when drawing a line that is to the side of the actual line.\\n // We do this by creating a vector that points towards the extrude, but rotate\\n // it when we're drawing round end points (a_directi = -1 or 1) since their\\n // extrude vector points in another direction. mediump float u = 0.5 * a_directio mediump float t = 1.0 - abs(u);\\n mediump vec2 offset = u_offset * a_extrude * scale * normal.y * mat2(t, -u, u, t);\\n\\n // Remove the texture normal bit of the position before scaling it with the\\n // model/view matrix.\\n gl_Positio = u_matrix * * 0.5) + (offset + dist) / u_ratio, 0.0, 1.0);\\n\\n v_tex_a = * normal.y * + u_tex_y_a) v_tex_b = * normal.y * + // position of y on the screen\\n float y = gl_Positio / // how much features are squished in the y direction by the tilt\\n float squish_sca = / * // how much features are squished in all directions by the float = 1.0 / (1.0 - min(y * u_extra, 0.9));\\n\\n v_linewidt = vec2(outse inset);\\n v_gamma_sc = * mediump lowp\\n#def mapbox: define lowp vec4 mapbox: define lowp float vec2 v_pos;\\n\\n main() {\\n #pragma mapbox: initialize lowp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ float dist = length(v_p - float alpha = 0.0, dist);\\n gl_FragCol = outline_co * (alpha * gl_FragCol = highp lowp\\n#def vec2 mat4 vec2 vec2 mapbox: define lowp vec4 mapbox: define lowp float main() {\\n #pragma mapbox: initialize lowp vec4 #pragma mapbox: initialize lowp float opacity\\n\\ gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n v_pos = / gl_Positio + 1.0) / 2.0 * mediump lowp\\n#def float vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 vec2 v_pos;\\n\\n main() {\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = vec4 color2 = pos2);\\n\\n // find distance to outline for alpha float dist = length(v_p - float alpha = 0.0, dist);\\n \\n\\n gl_FragCol = mix(color1 color2, u_mix) * alpha * gl_FragCol = highp lowp\\n#def vec2 vec2 vec2 vec2 float float float vec2 mat4 vec2 vec2 vec2 vec2 v_pos;\\n\\n main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n vec2 scaled_siz = u_scale_a * vec2 scaled_siz = u_scale_b * // the correct offset needs to be calculated //\\n // The offset depends on how many pixels are between the world origin and\\n // the edge of the tile:\\n // vec2 offset = size)\\n //\\n // At high zoom levels there are a ton of pixels between the world origin\\n // and the edge of the tile. The glsl spec only guarantees 16 bits of\\n // precision for highp floats. We need more than that.\\n //\\n // The pixel_coor is passed in as two 16 bit values:\\n // = / 2^16)\\n // = 2^16)\\n //\\n // The offset is calculated in a series of steps that should preserve this precision: vec2 offset_a = scaled_siz * 256.0, scaled_siz * 256.0 + vec2 offset_b = scaled_siz * 256.0, scaled_siz * 256.0 + v_pos_a = * a_pos + offset_a) / v_pos_b = * a_pos + offset_b) / v_pos = / gl_Positio + 1.0) / 2.0 * mediump lowp\\n#def float vec2 vec2 vec2 vec2 float sampler2D vec2 vec2 main() {\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos = imagecoord vec4 color1 = pos);\\n\\n vec2 imagecoord = mod(v_pos_ 1.0);\\n vec2 pos2 = vec4 color2 = pos2);\\n\\n gl_FragCol = mix(color1 color2, u_mix) * gl_FragCol = highp lowp\\n#def mat4 vec2 vec2 vec2 vec2 float float float vec2 vec2 vec2 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n vec2 scaled_siz = u_scale_a * vec2 scaled_siz = u_scale_b * // the correct offset needs to be calculated //\\n // The offset depends on how many pixels are between the world origin and\\n // the edge of the tile:\\n // vec2 offset = size)\\n //\\n // At high zoom levels there are a ton of pixels between the world origin\\n // and the edge of the tile. The glsl spec only guarantees 16 bits of\\n // precision for highp floats. We need more than that.\\n //\\n // The pixel_coor is passed in as two 16 bit values:\\n // = / 2^16)\\n // = 2^16)\\n //\\n // The offset is calculated in a series of steps that should preserve this precision: vec2 offset_a = scaled_siz * 256.0, scaled_siz * 256.0 + vec2 offset_b = scaled_siz * 256.0, scaled_siz * 256.0 + v_pos_a = * a_pos + offset_a) / v_pos_b = * a_pos + offset_b) / mediump lowp\\n#def float float sampler2D sampler2D vec2 vec2 float float float float vec3 main() {\\n\\n // read and cross-fade colors from the main and parent tiles\\n vec4 color0 = v_pos0);\\n vec4 color1 = v_pos1);\\n vec4 color = color0 * u_opacity0 + color1 * u_opacity1 vec3 rgb = color.rgb; // spin\\n rgb = vec3(\\n dot(rgb, dot(rgb, dot(rgb, // saturation float average = (color.r + color.g + color.b) / 3.0;\\n rgb += (average - rgb) * // contrast\\n rgb = (rgb - 0.5) * + 0.5;\\n\\n // brightness vec3 u_high_vec = vec3 u_low_vec = gl_FragCol = u_low_vec, rgb), gl_FragCol = highp lowp\\n#def mat4 vec2 float float vec2 vec2 vec2 vec2 main() {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1);\\n v_pos0 = / 32767.0) - 0.5) / u_buffer_s ) + 0.5;\\n v_pos1 = (v_pos0 * + mediump lowp\\n#def sampler2D sampler2D lowp float vec2 vec2 main() {\\n lowp float alpha = v_fade_tex * u_opacity; gl_FragCol = v_tex) * gl_FragCol = highp lowp\\n#def vec2 vec2 vec2 vec4 matrix is for the vertex mat4 mediump float bool vec2 vec2 vec2 vec2 main() {\\n vec2 a_tex = mediump float a_labelmin = a_data[0]; mediump vec2 a_zoom = a_data.pq; mediump float a_minzoom = a_zoom[0]; mediump float a_maxzoom = a_zoom[1]; // u_zoom is the current zoom level adjusted for the change in font size\\n mediump float z = 2.0 - u_zoom) - (1.0 - u_zoom));\\ vec2 extrude = * (a_offset / 64.0);\\n if {\\n gl_Positio = u_matrix * vec4(a_pos + extrude, 0, 1);\\n gl_Positio += z * } else {\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n }\\n\\n v_tex = a_tex / u_texsize; v_fade_tex = / 255.0, mediump lowp\\n#def sampler2D sampler2D lowp vec4 lowp float lowp float lowp float vec2 vec2 float main() {\\n lowp float dist = v_tex).a;\\ lowp float fade_alpha = lowp float gamma = u_gamma * lowp float alpha = - gamma, u_buffer + gamma, dist) * gl_FragCol = u_color * (alpha * gl_FragCol = highp lowp\\n#def float PI = vec2 vec2 vec2 vec4 matrix is for the vertex mat4 mediump float bool bool mediump float mediump float mediump float vec2 vec2 vec2 vec2 float main() {\\n vec2 a_tex = mediump float a_labelmin = a_data[0]; mediump vec2 a_zoom = a_data.pq; mediump float a_minzoom = a_zoom[0]; mediump float a_maxzoom = a_zoom[1]; // u_zoom is the current zoom level adjusted for the change in font size\\n mediump float z = 2.0 - u_zoom) - (1.0 - u_zoom));\\ // map\\n // map | viewport\\n if {\\n lowp float angle = ? (a_data[1] / 256.0 * 2.0 * PI) : u_bearing; lowp float asin = sin(angle) lowp float acos = cos(angle) mat2 RotationMa = mat2(acos, asin, -1.0 * asin, acos);\\n vec2 offset = RotationMa * a_offset;\\ vec2 extrude = * (offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos + extrude, 0, 1);\\n gl_Positio += z * // viewport\\n // map\\n } else if {\\n // foreshorte factor to apply on pitched maps\\n // as a label goes from horizontal vertical in angle\\n // it goes from 0% foreshorte to up to around 70% lowp float pitchfacto = 1.0 - cos(u_pitc * sin(u_pitc * 0.75));\\n\\ lowp float lineangle = a_data[1] / 256.0 * 2.0 * PI;\\n\\n // use the lineangle to position points a,b along the line\\n // project the points and calculate the label angle in projected space\\n // this calculatio allows labels to be rendered unskewed on pitched maps\\n vec4 a = u_matrix * vec4(a_pos 0, 1);\\n vec4 b = u_matrix * vec4(a_pos + 0, 1);\\n lowp float angle = - b[0]/b[3] - a[0]/a[3]) lowp float asin = sin(angle) lowp float acos = cos(angle) mat2 RotationMa = mat2(acos, -1.0 * asin, asin, acos);\\n\\n vec2 offset = RotationMa * 1.0) * a_offset); vec2 extrude = * (offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n gl_Positio += z * // viewport\\n // viewport\\n } else {\\n vec2 extrude = * (a_offset / 64.0);\\n gl_Positio = u_matrix * vec4(a_pos 0, 1) + vec4(extru 0, 0);\\n }\\n\\n v_gamma_sc = (gl_Positi - 0.5);\\n\\n v_tex = a_tex / u_texsize; v_fade_tex = / 255.0, mediump lowp\\n#def float float float float main() {\\n\\n float alpha = 0.5;\\n\\n gl_FragCol = vec4(0.0, 1.0, 0.0, 1.0) * alpha;\\n\\n if > u_zoom) {\\n gl_FragCol = vec4(1.0, 0.0, 0.0, 1.0) * alpha;\\n }\\n\\n if (u_zoom >= v_max_zoom {\\n gl_FragCol = vec4(0.0, 0.0, 0.0, 1.0) * alpha * 0.25;\\n }\\n\\n if >= u_maxzoom) {\\n gl_FragCol = vec4(0.0, 0.0, 1.0, 1.0) * alpha * 0.2;\\n highp lowp\\n#def vec2 vec2 vec2 mat4 float float float main() {\\n gl_Positio = u_matrix * vec4(a_pos + a_extrude / u_scale, 0.0, 1.0);\\n\\n v_max_zoom = a_data.x;\\ = vec4 values, const float t) {\\n if (t 7)return[n have been deprecated as of v8\")];if(! in \"%s\" not strict\";va a(l,e,\"arr expected, %s a(l,e,\"arr length %d expected, length %d r?[new have been deprecated as of v8\")]:[];v n(e,r,\"obj expected, %s found\",a)] o=[];for(v s in must start with \"@\"'));ret strict\";va one of [%s], %s strict\";va t(e){var n(l,s,\"arr expected, %s n(l,s,'\"$t cannot be use with operator n(l,s,'fil array for operator \"%s\" must have 3 expected, %s key cannot be a functions not functions not strict\";va url must include a \"{fontstac url must include a \"{range}\" strict\";va n(c,r,'eit \"type\" or \"ref\" is i(e,r,\"%s is greater than the maximum value strict\";va n(e,r,\"obj expected, %s f in r){var property in n(e,r,'mis required property strict\";va i(e,o,'unk property strict\";va n(r,e,'\"ty is e)for(var c in a(t){retur Sans Unicode MS new new M=new in n){for(var many symbols being rendered in a tile. See many glyphs being rendered in a tile. See exceeds allowed extent, reduce your vector tile buffer size\")}ret new new Error(\"Inv LngLat object: (\"+t+\", new new x(){return y(){return point(){re new new new new instanceof 0===s&&voi a(void new Error(\"fai to invert strict\";va n={\" strict\";va s(t){retur l(t,e,r,n) o=(new out of n(t,e){ret mapbox: ([\\w]+) ([\\w]+) ([\\w]+) a=new n?e(new Error(\"Inp data is not a valid GeoJSON t.data)ret e(new Error(\"Inp data is not a valid GeoJSON e(new Error(\"Inp data is not a valid GeoJSON e=0;ee)){v y;for(y in in p)c[y]=!0; t in new new i(t,e,i){v r(t,r){ret delete e(t);var n=new o(new new e=new in tile source layer \"'+M+'\" does not use vector tile spec v2 and therefore may have some rendering g(t,L);var F in B in n=new t.time>=(n void void t=new new i;var strict\";va new Error(\"Inv color o[e]}throw new Error(\"Inv color void n in r in Error('Sou layer does not exist on source \"'+e.id+'\" as specified by style layer t in t.id});for new Error(\"Sty is not done new Error(\"The is no source with this ID\");var delete instanceof this;var 0===e)thro new Error(\"The is no layer with this ID\");for(v r in this;var void 0===i||voi 0===a?void strict\";va i(t){retur t.value}va r,n;for(va i in t){var in for(n in in in 0===e)dele 0===e)dele o}var strict\";va new t){var this.grid= a}if(r){va _=u;for(va a}}}return r=new r(\"glyphs > 65535 not i=!t&&new l(new c(new g(e,r){var y(e,r){var i(0,0));re M in a)t[M]=new strict\";va t){var | n(){}var i(t){retur new 61:case 107:case 171:case 189:case 109:case t=0,e=0;re t=new null!==t&& new Error(\"max must be between the current minZoom and 20, t,e={};ret t instanceof e;if(t instanceof instanceof c?t:new i(this,e); void Error(\"Fai to initialize s in if(void if(void n(t){var r=new n(t){for(v e=0;e1)for delete error c(t,e,r){v f(t,e){for t in null;var delete new Error(\"An API access token is required to use Mapbox GL. See new Error(\"Use a public access token (pk.*) with Mapbox GL JS, not a secret access token (sk.*). See t}function i(t){retur a(t){retur t;var n(t){funct v[n];void in t=0;t=1)re 1;var void t={};for(v e in =0.22.0 =0.22.0 No README data run build-docs # invoked by publisher when publishing docs on the mb-pages --debug --standalo mapboxgl > && tap --no-cover build --github --format html -c --theme ./docs/_th --output --debug -t unassertif --plugin [minifyify --map --output --standalo mapboxgl > && tap --no-cover --debug -t envify > --ignore-p .gitignore js test bench diff --name-onl mb-pages HEAD -- | awk '{print | xargs build-toke watch-dev watch-benc build-toke watch-benc build-toke watch-dev run build-min && npm run build-docs && jekyll serve --no-cache --localhos --port 9966 --index index.html .\",test:\"n run lint && tap --reporter dot test/js/*/ && node && watchify bench/inde --plugin [minifyify --no-map] -t [babelify --presets react] -t unassertif -t envify -o bench/benc --debug --standalo mapboxgl -o n=new r=new r(t){var n(t,n){var i(t){retur t)return t){var 1=0)return V=1;V specify vertex creation specify cell creation specify phase strict\";va n(t){if(t in l)return l[t];for(v new Invalid boundary dst;};retu t in l){var t in u){var t in c){var return \"+s),u){va p=new p=new p()}functi for(var o=0;o1)for f(e,r){var s=\"__l\"+ i=\"__l\"+ _=[\"'use L=new L=new L(r)}funct s(t,e){var r=[\"'use [2,1,0];}e [1,0,2];}} [2,0,1];}e new new function new o=new 0===t){var 0===r){r=n o(t,e){var s(t,e){ret a(t,e){var i=new t||\"up\"in strict\";va r=void 0!==r?r+\"\" e(t,e){for t}function o)throw new to path.resol must be t)throw new to path.join must be n(t){for(v new Error(\"Giv varint doesn't fit into 10 bytes\");va o(t,e,r){v s(t,e){for new type: void n(t){var 0:return r||[];case 1:return 2:return Array(t);v r}var r(t,e){var Array(a),n n(t,e){for a(t){for(v t-e});var new t instanceof i(t){retur a(t){for(v a=1;i;){va l(t){for(v c(t){retur d(t){var u(m)}funct p(t){var 0x80 (not a basic code x});else for(_ in n(t,e){ret o;var o};var n(t,e){for n&&void e(t){var e=new Error(\"(re \"+t);throw n(t){retur t?\": i(t,r,i){t in r||e(\"unkn parameter possible values: parameter type\"+n(r) must be a typed parameter type\"+n(i) expected \"+r+\", got \"+typeof t)}functio parameter type, must be a nonnegativ shader source must be a string\",a) number \"+t+\": r=0;e(c(\"| compiling \"+s+\" shader, linking program with vertex shader, and fragment shader i(t){retur M(t,r){var n=m();e(t+ in command called from \"+n))}func A(t,e,r,i) in e||M(\"unkn parameter possible values: parameter type\"+n(r) expected \"+e+\", got \"+typeof texture format for renderbuff format for L(t,e){ret z(t,e,n){v pixel arguments to document,\" manually specify webgl context outside of DOM not supported, try upgrading your browser or graphics drivers name must be string\");v $(t){var et(t,e){va _e:r=new we:r=new Me:r=new ke:r=new Ae:r=new Te:r=new Se:r=new null}retur n=0;n0){va t[0]){var buffer data\")}els shape\");va data for buffer p=new n(a);retur d=[];retur t=0;return t&&t._buff instanceof a(t){var e||(e=new Ge:case Xe:case Ze:case type for element bit element buffers not supported, enable first\");va vertex count for buffer a}var t&&t._elem instanceof pt(t){for( At(t){retu Tt(t,e){va Or:case Fr:case Rr:case jr:var texture type, must specify a typed St(t,e){re for(var s}return o*r*n}func texture texture unpack n){var must enable the extension in order to use floating point must enable the extension in order to use 16-bit floating point must enable the extension in order to use depth/sten texture must be an extension not extension not d(e,r,i){v m(){return K.pop()||n h}function y(t,e,r){v b(t,e){var e){var e){var e){var e){var e){var i(t,e){var arguments to format for c=new T(nr);retu format for C=new z=new I(){for(va for(var P={\"don't care\":$r,\" mipmap mipmap mipmap mipmap s3tc dxt1\":Mr,\" s3tc dxt1\":kr,\" s3tc dxt3\":Ar,\" s3tc atc\":Sr,\"r atc explicit atc interpolat pvrtc pvrtc pvrtc pvrtc etc1\"]=Pr) r=B[e];ret null});ret number of texture shape for z||\"colors render targets not color buffer must enable in order to use floating point framebuffe must enable in order to use 16-bit floating point framebuffe must enable to use 16-bit render must enable in order to use 32-bit floating point color color format for color format for extension not u=d=1;var for(D=new color attachment \"+a+\" is color attachment much have the same number of bits per depth attachment for framebuffe stencil attachment for framebuffe depth-sten attachment for framebuffe not resize a framebuffe which is currently in use\");var i;for(var shape for framebuffe must be be d||\"colors render targets not color buffer color color format for l=1;var a(t){var t=0;return vertex fragment shader\",n) a=i[t];ret a||(a=new o(o){var must create a webgl context with in order to read pixels from the drawing cannot read from a from a framebuffe is only allowed for the types 'uint8' and from a framebuffe is only allowed for the type 'uint8'\")) arguments to buffer for regl.read( too s(t){var r;return l(t){retur l}function jt(t){retu Nt(t){retu Bt(){funct t(t){for(v r(){functi n(){var e=a();retu n(){var new new m(t){retur v(t,e,r){v g(t,e,r){v y(){var ei:var ri:return ni:return ii:return ai:return c={};retur n=e.id(t); in c)return c[n];var b(t){var in r){var if(Di in n){var e}function x(t,e){var in r){var i=r[Pi];re framebuffe in n){var a=n[Pi];re framebuffe null}funct n(t){if(t in i){var in a){var \"+t)});var in in e?new s=o;o=new w(t){funct r(t){if(t in i){var r});return n.id=r,n}i in a){var o=a[t];ret null}var r(t,r){if( in n){var in i){var s=i[t];ret in n){var in i){var o=i[Ri];re in n){var t=n[ji];re Be[t]})}if in i){var r=i[ji];re in \"+n,\"inval primitive, must be one of Aa}):new in n){var vertex t})}if(Ni in i){var r=i[Ni];re vertex s?new vertex offset/ele buffer too l=new k(t,e){var o(e,n){if( in r){var o})}else if(t in i){var vi:case si:case oi:case Ai:case hi:case Ci:case xi:case wi:case Mi:case pi:return flag fi:return in \"+i,\"inval \"+t+\", must be one of di:return color attachment for framebuffe sent to uniform data for uniform a[r],\"inva uniform or missing data for uniform T(t,r){var a&&a,\"inva data for attribute offset for attribute divisor for attribute parameter \"'+r+'\" for attribute pointer \"'+t+'\" (valid parameters are in r)return r[s];var in '+a+\"&&(ty dynamic attribute if(\"consta in \"+a+'.cons === in S(t){var a(t){var parameter L(t,e,r){v C(t,e,r,n) z(t,e,r){v n=m(e);if( in r.state)){ c,h;if(n in in I(t,e,r,n) if(mt(u)){ l(t){var ua:case da:case ga:return 2;case ca:case pa:case ya:return 3;case ha:case ma:case ba:return 1}}functio attribute i(i){var a=c[i];ret a(){functi o(){functi vertex vertex vertex i(t){retur n(e){var n=r.draw[e s(t){funct e(t){var args to args to e(t){if(t in r){var e=r[t];del delete l(t,e){var regl.clear with no buffer takes an object as cancel a frame callback must be a h(){var callback must be a function\") event, must be one of Kt={\"[obje renderbuff renderbuff arguments to renderbuff r(){return i(t){var s(){return p.pop()||n o}function u(t,e,r){v c(){var t(){var new requires at least one argument; got none.\");va e.href;var \",e);var s=new o;n=-(i+a) null;var n(t){retur n(t){for(v R;};return i(t){var e=s[t];ret strict\";\"u n(t){for(v i}function h(t,e){for r=new r}function r=new l(e)}funct u(t){for(v e=s(t);;){ t=k[0];ret f(t,e){var r=k[t];ret n(t,e){var l}else if(u)retur l}else if(u)retur u;return i(t,e){ret t.y-e}func a(t,e){for r=null;t;) t;var r}function l(t){for(v n=d.index; n(t,e){var i(t,e,r,n) o(t,e){for r}function s(t,e){for m}function s[t];for(v new unexpected new failed to parse named argument new failed to parse named argument new mixing positional and named placeholde is not (yet) s[t]=n}var n(t){for(v Array(e),n Array(e),i Array(e),a Array(e),o Array(e),s x=new u(t){retur c(t){var h(t){retur f(t){var d(t,e){for r in t}function p(t){retur t.x}functi m(t){retur t.y}var time\");var r=\"prepare \"+t.length %d clusters in c)|0 p=new Array(r),m Array(r),v Array(r),g p=new o}function s}function T(t){retur n=z(t);ret t){var r={};for(v i in e={};for(v r in n(t,e){var i(t,e){var s/6}return 1}var n&&void e(t,e){var for(a=0,n= n})}}var s;var in new Error(\"n must be new Error(\"alr s(t){retur new l(t){retur new u(t){retur new c(t){retur new h(t){retur new f(t){retur new d(t){retur new p(t){retur new m(t){retur x?new v(t){retur new n(t)}var null}retur t=0;tn)ret instanceof n)return t;var i=new n;return a(t){retur instanceof o(t,e){ret s(t,e){ret new 'url' must be a string, not \"+typeof t);var i(t,e){var a(t,e){var o(t,e){ret t}function s(t){var e={};retur a;var v=e.name?\" c(e)}var o+\": \"+s}functi d(t,e,r){v n=0;return \")+\" \"+t.join(\" \")+\" \"+t.join(\" \")+\" p(t){retur t}function v(t){retur g(t){retur t}function t}function t}function _(t){retur void 0===t}func w(t){retur M(t)&&\"[ob k(t){retur M(t)&&\"[ob A(t){retur instanceof t}function S(t){retur t||void 0===t}func E(t){retur L(t){retur t=a)return new Error(\"unk command if(7!==r)t new Error(\"unk command i(t){for(v e}var new Error(\"fea index out of new new String too long (sorry, this will get fixed later)\");v l(t){for(v e(t){var e=n(t);ret e?u in r(t,e){var o(t){var i?u in i&&delete t){var r?r[0]:\"\"} n?!r&&en)t al-ahad\",\" {0} not {0} {0} {0} mix {0} and {1} a(t,e){ret ;var format a date from another number at position name at position literal at position text found at dd M MM d, d M d M d M d M yyyy\",RSS: d M a=this;ret var _inline_1_ = - var _inline_1_ = - >= 0) !== (_inline_1 >= 0)) {\\n + 0.5 + 0.5 * (_inline_1 + _inline_1_ / (_inline_1 - }\\n n(t,e){var r=[];retur strict\";va u(r,i){ret i(t,e){var void E.remove() void null;var strict\";va void c();var t}function i(t){var e=x[t];ret a(t){retur the calendar system to use with `\"+t+\"` date data.\"}var i={};retur t}var i?\"rgba(\"+ n=i(t);ret t){var A(e,r){var T(){var void strict\";va strict\";va strict\";va strict\";va strict\";va strict\";va n(){var e(e){retur r;try{r=ne strict\";va i(t,e,r,n) a(t){var void n.remove() void \")}).split \")}).split scale(\"+e+ n,i,a;retu strict\";va 0 1,1 0 0,1 \"+a+\",\"+a+ 0 0 1 \"+a+\",\"+a+ 0 0 1 \"+r+\",\"+r+ 0 0 1 \"+r+\",\"+r+ 0 0 1 0 1,1 0 0,1 0 1,1 0 0,1 n(t,e,r,n) t.id});var strict\";va strict\";va i(t,e,r){v r(t){var void r.remove() r(e,r,o){v if(i[r]){v o;if(void strict\";va n(t){var n(r){retur strict\";va n(t){for(v \");var i(t,e){var click on legend to isolate individual l(t){var u(t){var strict\";va r[1]}retur i}function i(t){retur t[0]}var h(t){var f(t){var d(t){var n(t,e){var i(t){for(v n(t){for(v 0}}var o(t,e){var 0 1,1 0 0,1 extra params in segment t(e).repla strict\";va strict\";va u(r,i){ret r(t,e){ret l(t,e,r){v u(t,e,r){v c(t,e){var n(){return p(t,e){var g(t,e){ret y(t,e){ret b(t,e,r){v x(t,e){var _(t){for(v r(t,e){ret strict\";va strict\";va strict\";va t){var void t)return void void n}function l(t){retur u(t){retur c(t){retur d\")}functi h(t){retur d, yyyy\")}var t.getTime} r={};retur n=new a(t){retur o(t){for(v r={};retur n(){return strict\";va for(var c(t){retur void property r(t,e){var instanceof RegExp){va void o(t,e){ret t>=e}var binary r=e%1;retu n(t){var e=i(t);ret n(t,e){ret i(t){retur \")}functio a(t,e,r){v was an error in the tex null;var r=0;r1)for i=1;i doesnt match end tag . Pretending it did s}function c(t,e,r){v o(),void e();var 0,\":\"],k=n t(t,e){ret void n(t){var i(){var 1px new strict\";va strict\";va n(t,e){for r=new new Error(\"No DOM element with id '\"+t+\"' exists on the page.\");re 0===t)thro new Error(\"DOM element provided is null or previous rejected promises from t.yaxis1); array edits are incompatib with other edits\",h); full array edit out of if(void & removal are incompatib with edits to the same full object edit new Error(\"eac index in \"+r+\" must be new Error(\"gd. must be an 0===e)thro new is a required new Error(\"cur and new indices must be of equal u(t,e,r){v new Error(\"gd. must be an 0===e)thro new Error(\"tra must be in in i(t){retur a(t,e){var r=0;return new Error(\"Thi element is not a Plotly plot: \"+t+\". It's likely that you've failed to create a plot before animating it. For more details, see void c()}functi d(t){retur overwritin frame with a frame whose name of type \"number\" also equates to \"'+f+'\". This is valid but may potentiall lead to unexpected behavior since all plotly.js frame names are stored internally as This API call has yielded too many warnings. For the rest of this call, further warnings about numeric frame names will be addFrames accepts frames with numeric names, but the numbers areimplici cast to n(t){var i}function i(){var t={};retur a(t){var o(){var s(t){retur l(t){funct u(t){funct c(t){retur h(t,e,r){v f(t,e,r){v e={};retur t&&void n(t){retur Error(\"Hei and width should be pixel values.\")) l(t,e,r){v u(t,e,r,n) \"+o:s=o+(s dtick p(t,e){var c=new t.dtick){v error: t+i*e;var dtick a(t){for(v strict\";va v(r,n){ret to enter axis\")+\" e;var n(t,r){for n(t,e,r,n) u(t,e){ret y(t){var b(t,e,r){v back X(e,r){var K()}functi W(e){funct n(e){retur void k.log(\"Did not find wheel motion attributes \",e);var strict\";va n(t){retur t._id}func went wrong with axis Error(\"axi in in in o){var t(t){var e(t){retur strict\";va r(r,n){var e/2}}funct v(t,e){var g(t,e){var b(t,e){var x(t,e){var new Error(\"not yet r(t,r){for i(){for(va a(t,e){for n(t,e){var n(t){retur i(t,e,r,n) a(t,e){ret i(t,e){var r(t){retur n(t){var l(t,e){var u(t){var c(t,e){var f(t,e,r){v strict\";va i(t){var e=new n;return a(t){var o(t){var i=new n(t,e);ret strict\";va Sans Regular, Arial Unicode MS r(t,e){ret - delete t)return e,n,i={};f in i}return r=a(t);ret e&&delete P=(new + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + '' + 0px\",\"1px -1px\",\"-1p 1px\",\"1px \"+t+\" 0 \"+n+\" \"+n+\" \"+n+\" void c=\"t: \"+u.t+\", r: l;var r in t)r in r in r=e||6;ret 0===t)retu null;var void t(){var t={};retur n.mode,del strict\";va e(e,i){ret s;return t(t,e){ret i=r[n];ret strict\";va t(t,e,r){v e(t,e){ret r(t,e){ret a(t,i){var tozoom back f(t,e){var i(t){retur y,b;return o,s;return ii))return e}return void h(t){retur f(t,e){ret void strict\";va strict\";va 0, 0, strict\";va s;return r in void strict\";va null;for(v strict\";va o(e){var s(e){var strict\";va n(t,e,r){v i(t,e,r){v strict\";va converged strict\";va strict\";va strict\";va strict\";va s(r,i){ret n(t,e,r,n) strict\";va n(t,e){for o(t){retur strict\";va void strict\";va strict\";va c(r,i){ret loop in contour?\") s(t,e,r){v 15===r?0:r many contours, clipping at i}function a(t,e,r){v o(t,e,r){v s(t,e,r,n) e=l(t,r) r(t){retur to newendpt is not vert. or perimeter scale is not scale is not void data invalid for the specified inequality many contours, clipping at strict\";va strict\";va h(t){retur to newendpt is not vert. or perimeter o(t,e,r){v s(t,e,r){v scale is not scale is not strict\";va iterated with no new in strict\";va g}var didn't converge strict\";va s=0;sa){va in strict\";va l(r,n){ret u(t){var e=l(t);ret strict\";va strict\";va e(e){var strict\";va strict\";va void r(t,e){ret traces support up to \"+u+\" dimensions at the c}var l(r,n){ret strict\";va l(n){var i}function c(t,e,r){v l(t,e,r){v n=o(r);ret u(t,e){ret c(t){retur h(t){var e=o(t);ret f(t){var d(t){retur t[0]}funct p(t,e,r){v m(t){var v(t){retur l(t){var u(t){retur c(t,e){for e.t+\"px \"+e.r+\"px \"+e.b+\"px 255, 255, 0)\");var 1px 1px #fff, -1px -1px 1px #fff, 1px -1px 1px #fff, -1px 1px 1px strict\";va i(t,e,r){v strict\";va n(t,e){for m};var strict\";va o(r,a){ret strict\";va strict\";va strict\";va n(t,e,r){v u;var 1;var a(t,e){var r(t,e){ret n(t,e){ret s(t,e){var 1;var t+\" void strict\";va strict\";va strict\";va 0, i(t,e){var r=new for(r=new is present in the Sankey data. Removing all nodes and strict\";va u(r,a){ret n(t){retur t.key}func a(t){retur t[0]}funct o(t){var 0 0 1 0 0)\":\"matri 1 1 0 0 0)\")}funct M(t){retur k(t){retur 0 0 1 0 0)\":\"matri 1 1 0 0 0)\"}functi A(t){retur 1)\":\"scale 1)\"}functi T(t){retur S(t){retur L(t,e,r){v var C(t,e,r){v i(){for(va e={};retur 1px 1px #fff, 1px 1px 1px #fff, 1px -1px 1px #fff, -1px -1px 1px strict\";va _=new strict\";va void strict\";va strict\";va m(r,a){ret strict\";va strict\";va strict\";va r(e){var i(t){var strict\";va n(t,e){var + m(t){retur v(t){retur g(t){retur t.id}funct g}function x(e){var scatter strict\";va s(t,e){ret l(t){retur M[t]}funct o=0;o=0){v n(t,e,r,n) strict\";va d(r,i){ret s=o[0];if( 0;var v.push(\"y: strict\";va strict\";va e(t){retur r(t){var 1/0;var strict\";va n(t,e){var n}function s(t,e,r,n) n=new s(t){var 1/0;var strict\";va strict\";va strict\";va strict\";va d(r,i){ret strict\";va strict\";va e=f(t);ret e=f(t);ret e=f(t);ret e=f(t);ret Unconfirme transactio In\u00a0[4]: # import mempool data downloaded from mempool = pd.read_cs header=Non ) # split the datetime to date and time temp = = temp.date = temp.time del # reorder the columns cols = mempool = mempool[co inplace=Tr d2 = mempool = In\u00a0[5]: # there are 3 values per day. get average mempool size for each day mempool = The number of transactio waiting to be confirmed on the Bitcoin blockchain increased to an all time maximum on May 18th of 175,978. For comparison the average value in 2016 was less than\u00a010,00 Once the the number of unconfirme transactio had peaked, it fell about as quickly as it rose and by mid July was generally below 10,000\u00a0aga The current state of the unconfirme transactio pool along with the fee rates currently offered can be seen here. In\u00a0[6]: series1 = go.Scatter name='Dail average', line = dict( color = (color2), width = 2,)) series2 = go.Scatter name='Week average', line = dict( color = (color1), width = 3,)) data = [series1, series2] layout = go.Layout( transactio of yanchor='t y=1, x=0.5) ) fig = layout=lay py.iplot(f Out[6]: Median transactio confirmati time (minutes)\u00b6 would expect that the average time taken to confirm a transactio will increase with the size of the unconfirme transactio pool. The figure below shows the median time in minutes for a new transactio to be\u00a0confirm In\u00a0[7]: # The Daily Median time taken for transactio to be accepted into a block, presumably in minutes ATRCT = ATRCT = In\u00a0[8]: series1 = go.Scatter name='Dail median', line = dict( color = (color2), width = 2)) series2 = go.Scatter name='7 day average', line = dict( color = (color1), width = 3)) data = [series1, series2] layout = go.Layout( title='Med time taken for transactio to be accepted into a block', (minutes)' yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[8]: The median transactio confirmati time does not increase noticeably when the pool of unconfirme transactio increases, in fact the two features have only a weak Pearson correlatio of 0.37 (details below). This is surprising because I expected that the time taken to confirm a transactio would increases when the pool of transactio waiting to be Perhaps this is because only valid transactio can be confirmed and included in the median average calculatio but invalid transactio are included in the pool of transactio awaiting confirmati One way to test this would be to query the transactio awaiting confirmati and quantify if they are valid and what fee rate they are\u00a0offeri Average block size (daily, MB)\u00b6Each block in the Bitcoin network had a maximum size of 1MB before 1 August 2017. As the Bitcoin network has grown and transactio volume has increased the blocksize limit began to limit Was the increase in unconfirme transactio correlated to the blocks getting \u201cfilled up\u201d to their maximum 1MB\u00a0size? In\u00a0[9]: # The Average block size in MB AVBLS = In\u00a0[10]: av_bs = del av_bs['Val In\u00a0[11]: series1 = go.Scatter line = dict( color = (color2), width = 2)) series2 = go.Scatter name='7 day average', line = dict( color = (color1), width = 3)) data = [series1, series2] layout = go.Layout( title='Blo size', size (MB)'), yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[11]: From March through June the blocksizes seem to have frequently hit their maximum possible size, suggesting that the Bitcoin network was processing the maximum amount of data possible. The increase in unconfirme transactio occurred from mid-April to end of\u00a0June. The average block size began a sharp decrease on July 2nd, and at the same time the median transactio confirmati time also began a quick reduction. By July 2nd the number of unconfirme transactio had already fallen back to (Not all transactio are the same size, as a transactio can have any number of outputs and inputs, and a transactio with many inputs and/or outputs would be a larger amount of data than a transactio with only 1 input and 1 or 2\u00a0outputs. Lets confirm if the number of transactio increased over the same\u00a0perio Average number of transactio per (1MB) block\u00b6 In\u00a0[12]: # The average number of transactio per block. each day? NTRBL = NTRBL = In\u00a0[13]: series1 = go.Scatter name='Aver transactio per block', line = dict( color = (color2), width = 2)) series2 = go.Scatter name='7 day average', line = dict( color = (color1), width = 3)) data = [series1, series2] layout = go.Layout( title='Ave number of transactio per block', of yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[13]: The average number of transactio per block hit a peak at the end of May 2017 and then saw two sharp declines. It fell quickly at the beginning of June and then again at the beginning of\u00a0July. In June the blocksizes remained more or less as large as possible which suggests the blocks were full of a few large transactio At this time the size of the mempool was At the beginning of July the number of transactio per block reduced and the average blocksize was also rapidly reducing. This suggests that the volume of smaller transactio had\u00a0reduce The difference in average blocksizes in early June and early July suggests that in early June the number of transactio reduced because the average size of transactio had increased, but in July the number of transactio per block reduced because fewer transactio were being\u00a0crea Perhaps Bitcoin exchanges and other organisati with high transactio volumes had changed their behaviour and begun posting larger transactio with many inputs and/or outputs, rather than posting many smaller transactio with fewer inputs Bitcoin is often held by speculator who expect the value of a Bitcoin to increase. Perhaps increases in transactio volume are correlated to increases in Transactio fees earned by miners each fees are charged to users sending Bitcoin. Node operators (miners) collect unconfirme transactio confirm their validity and perform the proof-of-w requiremen to submit these transactio as a new block of In order to provide an incentive for node operators to process and confirm new transactio and to compensate for the equipment and energy costs required to do so, a fee is charged to confirm each transactio The size of the fee is proportion to the size (in bytes) of the transactio and is quantified as the fee rate otherwise miners would prefer smaller sized transactio as they could fit more into each\u00a0block The pool of unconfirme transactio is automatica sorted by transactio fee rate, so that miners confirm transactio with a higher fee rate before those with a lower fee\u00a0rate. Because of this, it is expected that as the number of unconfirme transactio increases, the fees paid to ensure a transactio gets processed will also increase. This is shown in the figure\u00a0bel Perhaps one reason the number of unconfirme transactio grew was because the fee rate offered for many of these transactio was below some threshold where it wasn\u2019t worth the miners efforts to confirm\u00a0th The total value of confirmati fees earned per day and the size of the unconfirme transactio pool are plotted\u00a0be In\u00a0[14]: # transactio fees - the total BTC value of transactio fees miners earn per day. TRFEE = In\u00a0[15]: tn_fee = del In\u00a0[16]: trace1 = go.Scatter transactio ) # used later trace2 = go.Scatter fee', yaxis='y2' ) data2 = [trace1, trace2] layout = go.Layout( title='Tot value (BTC) of transactio confirmati fees earned each day', yanchor='t y=1.1, x=0.5), xaxis=dict ticklen=7, ), yaxis=dict title='Num of unconfirme transactio zeroline=T autotick=T ticks='', ), yaxis2=dic title='Dai sum of confirmati fees (BTC)', side='righ ) ) fig = layout=lay py.iplot(f Out[16]: It looks as if confirmati fees correlate positively to the number of unconfirme transactio This is expected as users would need to pay higher fees when there are a lot of unconfirme transactio in order to have their transactio moved towards the front of the queue and processed However it looks as if changes to a miners fee rate lags behind changes in the size of the unconfirme transactio pool by about 2 weeks. The variation in the transactio fee is also a lot smaller than variation in the size of the unconfirme This suggests that the method for calculatin transactio the fee rate could be improved so that fee rate responds faster to changes in the number of transactio awaiting confirmati This would make mining less profitable and more competitiv and would make the Bitcoin network cheaper for\u00a0users. Lets look at how expensive it is to use the Bitcoin network by analysing the transactio fee rate relative to Ratio of transactio fees to transactio volume\u00b6 In\u00a0[17]: # The Average transactio confirmati fee rate (%) CPTRV = CPTRV = In\u00a0[18]: series1 = go.Scatter name='Fee rate', line = dict( color = (color2), width = 2)) series2 = go.Scatter name='7 day average', line = dict( color = (color1), width = 3)) data = [series1, series2] layout = go.Layout( title='Min revenue as as percentage of the transactio volume', rate (%)'), yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[18]: The results show that a fee rate (Miners Volume) of 0.5-1% is typical on the Bitcoin network. This is a bit cheaper than ecommerce payment methods. Surprinsin there is a correlatio of -0.25 with the number of unconfirme transactio This means the fee rate decreases when the number of unconfirme transactio increases. The correlatio is weak. One possible explanatio for this may be that activity on the network increases when the price of Bitcoin increases. When the price of Bitcoin increases, more resources are allocated to mining because it is increasing profitable Also, more people decide to buy Bitcoin because it\u2019s becoming so valuable. This leads to more transactio but even more miners competing to confirm transactio and claim the rewards. This increase in supply drives down the transactio confirmati fee\u00a0rate. Lets see how the number of transactio per day has changed in 2017 so\u00a0far. Number of transactio per day\u00b6 In\u00a0[19]: # Number of Transactio from Popular Addresses NTREP = # excluding popular addresses NTRAN = # from all addresses NTREP = #excl. popular NTRAN = #all addresses NTRFP = NTRAN - NTREP # Popular only NTRFP['all = NTRAN['Val NTRFP['unp = NTREP['Val NTRFP['pop = NTRFP['all - NTRFP['unp #NTRFP.hea In\u00a0[20]: series1 = go.Scatter name='From all addresses' line = dict( color = (color2), width = 2)) series2 = go.Scatter name='From all addresses - 7 day average', line = dict( color = (color1), width = 3)) series3 = go.Scatter excluding 100 most popular addresses' yaxis='y1' line = dict( color = ('#CEB7DF' width = 2)) series4 = go.Scatter excluding 100 most popular addresses - 7 day average', yaxis='y1' line = dict( color = ('#830DD4' width = 3)) series5 = go.Scatter - 7 day average', yaxis='y2' line = dict( color = ('#4FA6D4' width = 3)) data = [series1, series2, series3, series4, series5] layout = go.Layout( title='Bit transactio per day', xaxis=dict ticklen=5, ), yaxis=dict per day', zeroline=T autotick=T ticks='', ticklen=7, ), yaxis2=dic side='righ ), legend=dic y=-0.45, x=0, ) ) fig = layout=lay py.iplot(f Out[20]: The figure above shows the number of transactio posted each day from all addresses, and the number of transactio each day from addresses excluding the 100 most popular addresses. The difference between the two (the number of transactio from the 100 most popular addresses) is shown in blue using the axis on the\u00a0right. There is a positive correlatio with the size of the unconfirme transactio pool. Interestin there is a stronger correlatio with transactio created by the 100 most popular addresses (0.54) than for unpopular addresses (0.46). Possible reasons for this are Finally, lets consider the influence of the price of Bitcoin on the size of the unconfirme Bitcoin price\u00b6 In\u00a0[21]: # The USD value of BTC MKPRU = MKPRU = In\u00a0[22]: series1 = go.Scatter name='Dail line = dict( color = (color2), width = 2)) series2 = go.Scatter name='7 day average', line = dict( color = (color1), width = 3)) data = [series1, series2] layout = go.Layout( title='Bit price', (USD)'), yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[22]: Apart from showing a notable increase of around 500% in 13 months, the price has a correlatio of just 0.42 with the size of the unconfirme The number of transactio coming from popular addresses is positively correlated (0.41) to Bitcoin price, suggesting that when the Bitcoin price increases trading activity on exchanges also increases. Transactio from less popular addresses is inversely correlated to Bitcoin price (-0.31) and this could be because when the Bitcoin price surges, individual holding Bitcoin do not want to spend Bitcoin to make purchases, and will need to use an exchange to convert fiat currencies into\u00a0Bitco Note that ordinarily a new address is used for each Ratio of unique addresses to I want to compare the number of unique Bitcoin addresses used to the total number of transactio created. I initially expected the ratio of addresses to transactio to be close to 1, not realising that each transactio will contain at least 2 addresses (1 input and 1 output, and probably a 2nd output address which is equal to the input address for the change). If each transactio on average has 2 outputs, then the idea ratio of Bitcoin transactio to addresses will be\u00a00.5. In\u00a0[23]: # unique addresses used each day NADDU = NADDU = #number of transactio is NTRAN RATIO = NTRAN / NADDU d1 = d2 = RATIO = In\u00a0[24]: series1 = go.Scatter name='Dail line = dict( color = (color2), width = 2)) series2 = go.Scatter name='7 day average', line = dict( color = (color1), width = 3)) data = [series1, series2] layout = go.Layout( title='Rat of transactio to unique addresses' yanchor='t y=1.1, x=0.5) ) fig = layout=lay py.iplot(f Out[24]: The figure above shows that the ratio of unique Bitcoin transactio to unique addresses approaches 0.5. If users reuse an address for multiple transactio (which is bad) then the ratio will rise above 0.5, and if users create transactio with more than the usual minimum of 2 unique addresses then the ratio will dip below\u00a00.5. Correlatio between each time series\u00b6The table below shows the Pearson correlatio coefficien between each time In\u00a0[25]: '''' # using daily averages tseries = [ av_bs['Siz tn_fee['Fe RATIO['Val ] cols = ['Unconf trnsx', 'Conf time', 'Block size', 'Trnsx/blo 'Conf fees', 'Fee rate', 'USD/BTC', 'Trnsx - pop addrs', 'Trnsx - unpop addrs', 'Tnsx : Addrs ratio'] tbl = len(tserie for i in for j in tbl[i,j] = # values index=cols # 1st column as index columns=co # 1st row as the column names '''; In\u00a0[26]: # Using 7 day moving average tseries = [ cols = ['Unconf trnsx', 'Conf time', 'Block size', 'Trnsx/blo 'Conf fees', 'Fee rate', 'USD/BTC', 'Trnsx - pop addrs', 'Trnsx - unpop addrs', 'Tnsx : Addrs ratio'] tbl = len(tserie for i in for j in tbl[i,j] = # values index=cols # 1st column as index columns=co # 1st row as the column names Out[26]: .dataframe thead tr:only-ch th { text-align right; } .dataframe thead th { text-align left; } .dataframe tbody tr th { top; } Unconf trnsx Conf time Block size Trnsx/bloc Conf fees Fee rate USD/BTC Trnsx - pop addrs Trnsx - unpop addrs Tnsx : Addrs ratio Unconf trnsx 1.000000 0.512825 0.572234 0.662592 0.751183 -0.318481 0.476519 0.684877 0.732961 0.042633 Conf time 0.512825 1.000000 0.875708 0.821096 0.652057 -0.347289 0.519717 -0.016592 0.466974 0.234847 Block size 0.572234 0.875708 1.000000 0.856833 0.748027 -0.435521 0.624602 0.387743 0.522778 -0.082361 Trnsx/bloc 0.662592 0.821096 0.856833 1.000000 0.619106 -0.513678 0.332789 0.263169 0.915094 0.402304 Conf fees 0.751183 0.652057 0.748027 0.619106 1.000000 -0.326240 0.824995 0.661089 0.378880 -0.363753 Fee rate -0.318481 -0.347289 -0.435521 -0.513678 -0.326240 1.000000 -0.217555 -0.591717 -0.393656 0.126348 USD/BTC 0.476519 0.519717 0.624602 0.332789 0.824995 -0.217555 1.000000 0.523433 -0.354745 -0.716468 Trnsx - pop addrs 0.684877 -0.016592 0.387743 0.263169 0.661089 -0.591717 0.523433 1.000000 0.244647 -0.409759 Trnsx - unpop addrs 0.732961 0.466974 0.522778 0.915094 0.378880 -0.393656 -0.354745 0.244647 1.000000 0.471671 Tnsx : Addrs ratio 0.042633 0.234847 -0.082361 0.402304 -0.363753 0.126348 -0.716468 -0.409759 0.471671 1.000000 if { var mathjaxscr = = = = ? \"innerHTML : \"text\")] = + \" config: + \" TeX: { extensions { autoNumber 'AMS' } },\" + \" jax: + \" extensions + \" displayAli 'center',\" + \" displayInd '0em',\" + \" showMathMe true,\" + \" tex2jax: { \" + \" inlineMath [ ['$','$'] ], \" + \" displayMat [ ['$$','$$' ],\" + \" true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS' { \" + \" linebreaks { automatic: true, width: '95% container' }, \" + \" styles: { .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important' }\" + \" } \" + \"}); \"; (document. || }"},{"title":"Corporate\u00a0London","url":"corporate.html","body":"2014 -\u00a02017 Having made arrangemen to leave London, it seems like a reasonable time to reflect on my time\u00a0here. London is a tough city to live in \u2014 it\u2019s big enough that a sub 1-hour commute is considered good, and it\u2019s super expensive. My stay in London has been defined by my quest to complete my graduate scheme and qualify as an\u00a0account When I first arrived I had no idea what London or working in a corporate would be like. I was coming from academia, and my motivation for moving to finance could be summed up in two\u00a0points Understand more about the 2008 Get paid more for using mostly the same skills (math) as in\u00a0enginee I don\u2019t think there\u2019s anything wrong with these motivation but I should have been making a long term career plan\u00a0inste When I started my job I was surprised at the 6 weeks of induction and hot-deskin Life as an auditor felt nomadic, as everyone would spend large amounts of time at client\u2019s offices, and no-one has their own desk in our office. The only thing you really own is your knowledge and your network In 2014, I arrived with a high opinion of my employer and the view that I would be staying for several years. Having escaped the financial insecurity of short-term research grants, I\u2019d moved back to my home country to contribute to the system that educated me. I was happy to have a regular job with a decent, A phrase that kept coming to mind back then was \u201caccountan factory\u201d. Our training materials were scripted and everything was a standardis process. We were being processed. Graduates in, corporate The company is huge and so are the efficienci and barriers to competitio that come with this. When I arrived I was impressed there were free biscuits, and it felt presumptuo to put a meeting in someones diary. Now I ignore any communicat addressed to a mailing\u00a0li Accountant The toughest experience were related to the accountanc qualificat Na\u00efvely I had believed that the qualificat wouldn\u2019t be a big deal and I didn\u2019t give any thought to it when I applied \u2014 I just wanted to find out how banks worked. If there were some exams to be done then it would be fine, I\u2019d already achieved a PhD and I would handle\u00a0it. That turned out to be a mistake. A huge mistake. The alarm bells should have rung louder when I realised most of my graduate colleagues had only applied so that they could get the qualificat (and were already intending to leave asap afterwards Doing the pre-course work before college wasn\u2019t trivial. College - I was being sent to a classroom again. We were being taken out of the office, away from clients, and put in a classroom to prepare for these\u00a0exam Starting a job that required becoming a chartered accountant without considerin the effort and time involved to qualify was dumb. It\u2019s an oversight I find hard to\u00a0believe The disappoint and sadness at having to study again was profound. The teaching and assessment style was several steps behind what I\u2019d become used to. Compared to the depth, autonomy and research-f of a doctorate, writing cookie-cut essays in time-press exams felt stupid. The exams were hard and I failed several of them, probably because of my low morale whilst revising. Retaking them required more weekends and annual leave being spent away from my family, camped-out in libraries and\u00a0office This required much patience and generosity from Ritsya, who having not seen me for long periods during my doctorate expected us to have a more normal lifestyle when we came to London. I found it miserable to pause my social life and other interests whilst studying, and then I would try to rush back to them when I had the time, knowing that soon I\u2019d have to pause them\u00a0again If I hadn\u2019t already studied in Vienna, this process of working and studying in London would have been exciting and felt valuable or special. I\u2019d passed enough exams though to know that they\u2019re never as important as they\u2019re made out to be, and whatever it was that was missing from my life wasn\u2019t going to be found in On the upside though, the ACA is the most practical qualificat I\u2019ve gained and has taught me many useful aspects of business and finance. I\u2019m glad to finally see business not as some mysterious system but something attainable Neighbourh Living in London has also required a lot of time on trains \u2014 my door to desk commute is about an hour each way, and I\u2019ve never lived somewhere in London that hasn\u2019t felt transient. We\u2019ve needed to live near a station and didn\u2019t want to commute more than an hour and that\u2019s put us in neighbourh with other young profession who also don\u2019t have long term plans (or financial ability) to stay in the city. We\u2019re all looking to move on and move up as fast as possible. I want to leave London partly because there are so many people with the same attitudes and priorities as my\u00a0own\u2026 Back in the office I spent the first few months figuring out how people were organised, how teams operated and how decisions were made. I think it took me about a year to feel like I understood how things really worked, and 1.5 years to feel like I could do all aspects of my job with certainty. There is a difference between how things are spoken about, and how things My experience has been that performanc is all that matters and behaviour is defined by self-inter I don\u2019t think this is different from previous environmen I\u2019ve worked in \u2014 academia isn\u2019t any different1 and the constructi industry certainly isn\u2019t. I wonder if the only way people can endure it is if they\u2019ve not experience anything else, or think it normal, or necessary. I find myself wishing we could be kinder to each other, and that we could create structures that incentivis Corporate finance usually appears clean and well presented. Its people appear dependable and capable. The culture is sanitised and there\u2019s a lot of pressure to conform - you can see it in the clothes and accessorie we wear and the jokes we tell each other in the\u00a0cantee I came to see my office as a glass and steel cathedral. I read about the Middle East, and migrants arriving in Europe, and tower blocks burning, and felt Despite the high regard with which we hold ourselves, we don\u2019t ask each other how we can help. Maybe we are too busy, or feel powerless to help. Apparently we do not know how to help, despite our wealth, talents, education I remember watching a man leave the office one day and thought that if he gave his whole career and left his firm after decades of service, there would be nothing left to identify him after a few weeks. The work will always get done. Market forces will dictate how the business adapts and grows. The City rolls\u00a0on. Publicatio trump almost all other metrics, and the incentives to publish quantity over quality are such that risky or slow research is inexcusabl The metrics used to standardis success and allocate resources can be subverted just as well in academia as in any other industry. What management measures, the team prioritise"},{"title":"Blockchains from the ground up: Part\u00a02","url":"blockchain-networks.html","body":"Maintain an accurate list of transactio across a large group of users, without a This is part 2 of an introducti to the key features of a generalise blockchain Part 1 introduced key features of immutable record creation between 2 parties using public key cryptograp Part 2 explores how a network of users can maintain the same (true) list of transactio and protect each other against\u00a0fr Broadcasti transactio to the\u00a0networ In Part 1 we saw Lizzie, John and Chris exchanging coins. Lizzie also paid John with coins that were owed to her by Chris. These transactio were authentica using PKI\u00a0which: Ensured Prevented participan claiming that they didn\u2019t make a Prevents anyone creating a transactio on someone else\u2019s behalf without their\u00a0cons As the number of people in the network grows, the transfer of coins from one user to another becomes harder to track. If every users ledger is not identical then the opportunit arises to use coins that have already been spent to pay someone who doesn\u2019t know they\u2019ve already been\u00a0used. This is double spending, and is possible because the ledger that is shared amongst all members of the group only has weak consistenc - it is not necessaril correct all the time in all\u00a0locati Weak consistenc could be solved by requiring that everyone votes to accept a transactio before it is accepted into the ledger (Unanimous consensus) or to save time we could reduce the requiremen so that only 50% of all users validate a transactio before it is accepted into the ledger (Quorum consensus) Either of these solutions is possible for a small local group with a list of all\u00a0users. However Unanimous or Quorum Consensus doesn\u2019t solve the weak consistenc problem\u00a0if The group is\u00a0large The group is small but spread across different locations or\u00a0timezon It is not possible to know how many members there are and therefore what proportion of users The real identity of a user is\u00a0unknown In these cases a peer-to-pe network is required where transactio between users require approval by other users before being confirmed. This has not been trivial to solve, as some users would be incentivis to be dishonest, and some may make mistakes. This is the distribute consensus problem, which on wikipedia is defined\u00a0as The consensus problem requires agreement among a number of agents for a single data value. Some of the processes (agents) may fail or be unreliable in other ways, so consensus protocols must be fault tolerant or resilient. The processes must somehow put forth their candidate values, communicat with one another, and agree on a single When the number and identity of participan is known, distribute consensus is possible. Two types of protocol which allows all users in a distribute system to agree on a transactio are the Paxos family of protocols and the Two-phase commit protocol. Both of these would require that at least 50% of all users reach agreement in order to add a However in a public peer-to-pe network the total number of active users is not known - its fast and cheap to create new user profiles, and existing user profiles may become dormant. This makes it impossible to know how many users 50% would be. Additional because its possible to cheaply create new user profiles (just generate a new public-pri key pair), a single actor could generate and control many user accounts in order to have many votes and force incorrect transactio onto the ledger. An attack where one user subverts a network by creating many profiles is known as a Sybil attack. Proof of\u00a0Work The solution to the Sybil attack is to increase the cost of verifying a transactio such that the cost exceeds the reward. This is achieved through proof-of-w (PoW) algorithms which are expensive for a sender claiming to have verified a transactio and simple for the receiver to verify that the sender has validated One possible Proof of Work approach is to require that the hash of a verificati message begins with a certain set of characters The chosen set of characters is called a nonce and the only way to create a verificati message with an acceptable hash is to try many slightly different messages. For example, a nonce may be 3 zeros. It\u2019s arbitrary, but the longer the nonce is the more difficult it becomes to find a hash that fits This is because a hash is a random list of characters and altering even a single part of the data being hashed will result in a completely different hash value. Therefore there is no way to predict a hash value. The only way to generate a hash with the required none is to repeatedly alter the data being hashed (even by just one character) until a hash with the required features is randomly achieved. This is expensive to achieve, but simple to\u00a0verify. Using the method, a user who seeks to verify a transactio and broadcast the result must (once they\u2019ve verified the transactio repeatedly try different messages until they randomly find a message that meets the nonce requiremen It is simple for a user to check if a transactio verificati message meets the nonce requiremen because it is simple to inspect a hash and compare it to the\u00a0nonce. The effect of this requiremen is a process that makes it expensive to claim that a transactio has been verified and cheap to check that verificati claim. This removes the threat of a Sybil attack, but does not remove the distribute consensus problems created by not\u00a0knowin The true identity of users in the\u00a0networ How many users\u00a0exis This problem cannot be completely solved, and the practical solution is to relax the requiremen such that the probabilit of accepting a fraudulent transactio is lower than some user defined threshold. This is acceptable because a user would require a higher degree of confirmati for a high-value transactio than they would for a low-value transactio and would therefore be willing to incur more time and cost to verify a high value transactio and reduce the probabilit of accepting an incorrect transactio below a\u00a0threshol If a user wishes to make fast or low-value transactio or trusts the party they\u2019re transactin with, then they may accept a transactio without any other users on the network verifying that the sender has the required However when the senders is not assured, verificati is required. The more risky or valuable the transactio the more users the receiver of the funds will ask to verify that the sender has access to the required funds. The higher the number of users, the higher the probabilit that a dishonest transactio will be identified before An appropriat level of verificati will depend on the amount being transferre and how well the receiver of the funds knows the\u00a0sender Asking peers on the network to verify transactio introduces a new problem. Verifying a transactio requires time and effort, and incurs a cost. This cost requires that network participan be rewarded for correctly verifying transactio between An attacker would only attack if the cost is less than the reward. Therefore the number and cost of verificati required should be just enough to make the cost of an attack more than the value of This introduces the problem that it costs more to verify a transactio than the value of the transactio itself. It is also create the recursive problem where the users who verified the first transactio would need to verify that the payment they received was then also valid. Furthermor a high proportion of the original transactio value is spent as a transactio fee (for verificati which is not\u00a0effici These problems are avoided by combining multiple transactio and verifying them at the same time, broadcasti the successful verificati of multiple transactio simultaneo by grouping the transactio together into a block By confirming multiple transactio at once (and proving it using transactio fees can be aggregated (allowing each individual fee to be much lower). Each block includes a list of verified transactio a reference to the previous block, and a block ID. Incentivis The transactio verificati process outlined above is remarkable because it creates a demand for new participan to the network by creating a financial incentive to verify transactio This makes the network more secure as increasing the number of participan makes a sybil attack Summary Users generate new transactio and broadcast them on a peer-to-pe network An idle user listens for new transactio and collects them until the sum of all transactio verificati fees is greater than the cost the user will incur to verify them and meet the The idle user adds an extra transactio to their list of transactio that transfers the sum of the transactio fees to their own\u00a0addres The idle user generates the block of newly verified transactio referencin the previously verified block so that transactio can be ordered and completing the proof-of-w challenge. This new block is then broadcast to the\u00a0networ Other users are listening for new block announceme These users verify that the block is valid according to the proof-of-w requiremen and the order of the\u00a0blocks Users with unverified transactio look inside the verified block to see if their pending transactio have been\u00a0accep Competing to validate blocks Each user can choose which transactio they verify, and how many to verify before beginning the proof-of-w requiremen and hopefully collecting the transactio fees. This lack of order around transactio verificati is fine because the only way to increase the probabilit of being the first to claim the transactio fees associated with a collection of transactio (a block) is to spend more CPU power searching for the required partial If two users complete a block at approximat the same time then the blockchain will look different in different parts of the network, as each completed block begins to propagate and other users accept the new block and add it to their ledger. This is ok if a rule is enforced that requires a user to always accept the longest chain of\u00a0blocks. This works because if multiple blocks are created at the same time, the time it takes to create subsequent blocks will vary due to the random behaviour of the proof-of-w algorithm. Therefore chains of different length will always exist and one version of the block chain will be longer than the others, providing a clear candidate for which branch of the blockchain to use. If there are transactio in the discarded branch which are not present in the new (longest) blockchain then they are added back into the pool of transactio A block of transactio in never The above procedure for verifying transactio and adding new blocks onto the chain means that even if a user inspects a new block and sees that their transactio has been verified, its possible that in the future a longer chain will be discovered (which must be accepted) which doesn\u2019t include Therefore any block could potentiall be removed, which means a transactio is never completely verified. However the probabilit of a block being removed decreases as the number of blocks after it increases. This means verificati can be thought of in terms of the number of blocks that have been added to the chain after the block containing If you are willing to accept a high level of risk, or you trust the party you are transactin with you could opt for a small number of blocks to be added after the block containing your transactio This has the benefit of increasing the speed of the transactio verificati If the transactio is risky or high-value you might require a larger number of blocks to be added to the chain before accepting the transactio This will increase the time required to verify the transactio but reduce the probabilit that a longer chain will undo the block containing the transactio in\u00a0questio"},{"title":"Move","url":"move.html","body":"Ambition Since I was 15, one of my ambitions has been to be an entreprene I used to joke with my dad about buying him a nice boat one\u00a0day. Life so far has been predominan about education, and that stage is over now. I\u2019m walking away from what I\u2019ve come to see as a lifestyle and career that has too many it doesn\u2019t make sense to live like this. Ultimately I want to develop multiple sources of passive income. I want to\u00a0create. Summer\u00a0201 The last few months have been intense, with some weeks leaving me feeling ambitious and energetic, and others feeling anxious and overwhelme I need to get better at defining a goal, taking the quickest path there, and ignoring Ritsya is brave enough and imaginativ enough to force me to think big and consider how to live a better life. My biggest fear is that I screw it up, shooting myself in the foot and Ritsya and my daughter also. We were headed safely to an unremarkab existence and it would be terrible to swap that for something worse. That won\u2019t\u00a0happ I know that I make better decisions and produce my best work under pressure, I thrive when I\u2019m perceived as an underdog. I need to accept this without using it as a reason to be foolish. Whilst I don\u2019t have a clear plan, or direction, or goal, (I have many of them) I\u2019ve got skills and I want to see what I can make. I\u2019ll never do my best work, using my most productive combinatio of skills and experience if I\u2019m a cog in someone It\u2019s safer being an employee than self-emplo I guess the price of removing risks is the difference between the value you generate for your employer and what you\u2019re paid. I think those risks are overpriced and most people are more capable than they realise. Sometimes you have to walk into a situation to find out how to make the most of it. And sometimes you have to leave a place in order to Autumn\u00a0201 Last year my boss gave me some advice that was supposed to be encouragin I\u2019d requested to reduce my involvemen with some stuff that was unrelated to my job so that I could contribute more to my team and still maintain some semblance of a My boss was pretty clear that reducing my involvemen in the extra stuff would not be possible. During our conversati he advised me not to worry about how much I contribute to the team because \u201cthe work will get done anyway\u201d. This was meant to be encouragin but instead removed any conviction that the work I did was\u00a0import"},{"title":"Flee","url":"flee.html","body":"Summer children squeal and shriek, Watch wars. Excitement mounts. How exotic. Unexpected Death arrives. Grow-up Grow old Face death Evade evil. Don\u2019t tire. World is fragile. Towers burn, Bridges bludgeon, Markets stab. Got to get\u00a0out."},{"title":"Understanding VC\u00a0Investment","url":"investment.html","body":"I attended the Lisbon Investment Summit in June and wrote about my experience here. One of the best sessions was with Boris Golden called \u201cUnderstan Investment These are my\u00a0notes: VC\u2019s are seeking to identify high-poten startups, and then support and fund\u00a0them. They are looking for something that is innovative and\u00a0unprov A startup is not a company but an organisati searching for a Whilst executing and discoverin a scalable way to\u00a0grow. Startups need money for ambitious but credible growth\u00a0pla A typical stake for a VC could be roughly\u00a020 VC\u2019s want an exit price of at least 100m, otherwise their business models don\u2019t work\u00a0out. $10m can seem a lot for a founder with a 30% stake, but it\u2019s not enough to attract VCs, so aim\u00a0higher How to\u00a0pitch Identify specific people with real\u00a0needs Size of\u00a0market Why\u00a0now? What is your clear competitiv advantage - why can no-one else do\u00a0this? Market Find a large and Management Build a smart, skilled and cohesive\u00a0t With a strong ability to deliver quickly and to learn\u00a0quic That is ready to go big whatever it\u00a0takes With a unique vision Model Valuable and Efficient go-to-mark and Profitable Scalabilit Momentum Show traction and that you\u2019ve cracked the Show ambitous and credible growth\u00a0pla With a growth\u00a0mod And a clear strategy to scale and\u00a0win"},{"title":"How to be an ambitious founder in\u00a0Europe","url":"ambitious.html","body":"Whilst at the Lisbon Investment Summit I went to a session called \u201cWhat it means to be ambitious for founders in Europe\u201d by Oussama Ammar. I\u2019ve written generally about the summit here. Oussama spoke with passion and humour. It\u2019s clear that he cares about encouragin would-be founders and confrontin some of the cultural hurdles that exist in Europe. These are my notes from the\u00a0sessio If you are serious about building something that matters then you have a great and exciting future\u00a0ahe You can never predict who will be successful and who will fail. There are a lot of dumb successful people and clever founders can\u00a0fail. It\u2019s really important to learn how to leverage your time effectivel You can lose anything else and get it back but you can never recover the time you\u2019ve already\u00a0sp The only way to succeed is to try\u00a0hard. It\u2019s hard to do something that\u00a0matte In Europe you need to be more ambitious than average (measured against other countries) because the environmen makes it harder to be an entreprene (Attitudes to risk, comfort, security, expectatio of failure.. are all unhelpful for\u00a0founde It\u2019s not impossible just harder. So be Everyone is replaceabl no-one is\u00a0unique. You can always lose money and replace it. There is so much money in the\u00a0world. Take a look at Crunchbase and see the failure rate for companies that raised $1m - $10m, it is the same rate for those that raised\u00a0$10 If you lose money on a project you will learn things from that experience and you can leverage that\u00a0learn \u2026 But you will never get back the time you\u00a0spent. Zombies are companies that make enough money to survive but not enough to provide joy to the people in the company - aim high\u00a0and\u2026 Starting a company is a big deal, like starting a marriage. Think hard about what you will be doing, what problem you are trying to solve, who you will be working\u00a0wi In Europe there is not enough money to go around and this makes pitching hard. Silicon Valley doesn\u2019t have this\u00a0probl On average a startup in Europe will raise less than a startup in the USA. Europe is irrational concerned about risk. USA is less concerned with risk and this is very\u00a0helpf So be pragmatic about\u00a0risk Incorporat in London where the laws are good, even if you don\u2019t want to operate in the UK. It has a better designed system of corporate law. Estonia is also\u00a0good. Best European city for\u00a0founde London, There isn\u2019t a best city because none are\u00a0holist London has the\u00a0money Paris has engineerin and product developmen (and no-one outside Paris knows the Berlin has automation and execution, execution and\u00a0scalin \u2026You need to draw on all 3 of these cities, which when taken together can surpass Anecdotes The founder of Slack lives in Vancouver with his wife and kids. He spends 1 day each week in California and everyone thinks Slack is California due to good marketing. Forget national pride, be pragmatic. Leverage the\u00a0intern The founders of Airbnb would fly from New York to California for 1 day each week. They were originally in California where AirBnB started, but decided that if they could make it work in New York then AirBnB would be a success, so they moved to New York and flew back for meetings , etc. If you ask Europeans to come to a meeting in London (which is not as far as California is from New York) people start complainin and deciding they can\u2019t be\u00a0bothere"},{"title":"The Lisbon Investment\u00a0Summit","url":"lis17.html","body":"I attended the Lisbon Investment Summit on June 6 and 7 as part of the Oula.la InsurTech startup team with John Sullivan. It was the first startup conference I\u2019d been to and I arrived hoping to have some useful conversati and understand more about the Fintech and startup spaces in Europe. As usual I wanted to focus on tech, finance Who was\u00a0there As well as some outstandin sessions there was a high ratio of investors to startups. This made talking to VCs, bankers and M&A lawyers really easy. I was happy to meet a London-bas and VC when I sat down to have lunch, and thanks to interrupti a session to ask blockchain related questions on Day 1, a banker struck up a conversati with me over breakfast the next day. I met a lot of people across many relevant roles, and once I\u2019ve worked through my notes and my new collection of business cards I hope to have some great The most useful sessions for me\u00a0were: \u201cWhat it means to be ambitious for founders in Europe\u201d by Oussama Ammar from TheFamily \u201cUnderstan Investment by Boris Golden from \u201cBuilding successful businesses on blockchain technologi a panel discussion moderated by Kevin Loaec, founder of the Chainsmith blockchain consultanc and including Mir Serena Liponi from Blockchain The blockchain session was valuable because it\u2019s unusual to meet people who have been working in the blockchain space for several years. I\u2019m always wary of being distracted by the hype and noise around developmen in the blockchain space so I appreciate hearing some informed and The opening session included a speech from the Mayor of Lisbon and the next day opened with a speech from the Secretary of Industry. Both speeches conveyed an open and progressiv attitude to internatio cooperatio aimed at promoting and supporting founders and startups in Portugal. The warm words were supported by practical measures including tax incentives and a state-back scheme to match amounts contribute by private investors. It was really refreshing to hear a politician extol the virtues of multinatio cooperatio and bringing different cultures into Portugal. I wish Britain could do this\u00a0too. On the topic of Brexit, which inevitably arose due to London\u2019s present role as a centre of finance and innovation it seems Europe is still expecting the UK to come to its senses, and cannot understand why it\u2019s destroying its goodwill and reputation so\u00a0thoroug Overall The two days in Lisbon were full of useful and energetic conversati and it was a great experience pitching Oula.la multiple times and talking about what we\u2019re trying to do and how blockchain are a part of that. The opportunit to meet founders and investors are invaluable for making informed decisions. Lisbon is a beautiful city which I found easy to get around and always felt safe in. It\u2019s also refreshing affordable"},{"title":"Blogging with Pelican: Design, Plugins,\u00a0Sharing","url":"pelican2.html","body":"Design My approach to building my blog is to keep it as simple as possible, only adding features when they make a significan improvemen to how the content is understood and used. Therefore I\u2019ve done away with several features that would normally come baked into a WordPress theme. For example a footer full of links that would never be used, and a sidebar full I opted for a single column design that hopefully presents text-heavy articles clearly and intuitivel (please leave a comment and tell me what you\u00a0think) Plugins My use of plugins to extend Pelican\u2019s functional reflects this, there is the Neighbors plugin so that the next or previous post can be accessed from the bottom of a post without going back to the index, and the Tag Cloud plugin to reflect which subjects are written about the most (and provide a link to all Speed The speed of the site is important because a faster site is more enjoyable to use. Therefore I\u2019ve minified the CSS and the JavaScript using the Assets plugin. I\u2019ve also set the CSS and JavaScript to load Images are optimised using the Optimize Images plugin so that their file size is as small as possible and they download quickly. The site uses CloudFlare free CDN features so hopefully no matter when you view the site from you get a decent page\u00a0speed I\u2019ve also arranged the homepage so that posts are shown by their category and then by posting date. This may not work very well with a larger number of posts, but I\u2019ll only consider that problem once it presents itself. Designing for hypothetic or conditions that don\u2019t yet exist is a waste of\u00a0time. There are examples of how I\u2019ve used Jinja templates below in the context of sharing my articles on Twitter and\u00a0Facebo Plugin: Share\u00a0Post I noticed that my posts were beginning to get tweeted about, so I thought it would be useful to have some sharing buttons at the bottom of each post for Twitter, Facebook and Email. Looking at the Pelican Plugins repo on Github showed there was (as usual) a plugin for this (called Share Post), though I noted it hadn\u2019t been updated for a couple of\u00a0years. Installing and initial set-up was simple thanks to the readme on the git repo. You need to copy the plugin folder to the plugins directory, and add the name of the plugin to the list in Then you need to copy paste some Jinja/HTML into the article.ht template. That\u2019s enough to make it\u00a0work. I noted though that when I shared to Twitter the text to be tweeted was encapsulat in quotes and there was a \u2018b\u2019 at the front. I realised this was due to using Python 3.x when the plugin (which hadn\u2019t been updated for 2 years) was likely written for Python 2.x. A quick google and the obligatory trip to SO showed me how to convert a bytes string to a normal text\u00a0strin # Python 2 tweet = ('%s%s' % (title, # Python 3 tweet_as_b = ('%s%s' % (title, tweet = I also found that an article couldn\u2019t be shared to twitter from a mobile device and this was due to the URL being incorrectl formatted. The new URL format required separate arguments for the URL, additional text and the # Incorrect twitter_li = % tweet # Correnct twitter_li = % (url, tweet, t_handle) Using meta-data to specify tweet\u00a0text I thought it would be cool to add some default text to a tweet, as I\u2019ve enjoyed this feature on other blogs when I\u2019ve found a post I wanted to share on Twitter. - A user may know they want to share an article but if they\u2019re in a hurry it might be hard to find the right words, so why not provide a ready-made tweet. The text is editable so it\u2019s only a\u00a0suggesti The text would be different for each post so it makes sense to specify it when writing the article. The article \u2018summary\u2019 would be too long, and I know that Pelican supports arbitrary meta-data tags. I assumed that Jinja would pick up the data the same way it picks up the \u2018standard\u2019 meta-data and added a function def tweet = '' if 'tweet'): tweet = return quote(('%s % else: return ' ' Once this function was working it was simply a case of calling the function and assigning the output to a variable I called \u201cTweet\u201d, and then adding \u201cTweet\u201d to the text string to be included in a tweets\u00a0tex tweet = tweet_as_b = ('%s' % tweet = t_handle = There was a bit of fiddling around to make sure that the number of spaces between each part of the tweet was correct, but nothing as complicate as when making Time Until. Specifying the text and image in a Facebook\u00a0s Sharing to Facebook worked without any formatting problems, but it bugged me that the opening text of the article was being used in the preview that was shared to Facebook when I had a summary already prepared and that would be much more useful to potential readers. For some articles I also had an article image that I wanted to see being\u00a0used Googling revealed that I needed to use particular meta tags in the webpage\u2019s header if I wanted to control what Facebook would pickup. Facebook uses the \u201copen graph\u201d standards so I would need the headers in my article pages to include the\u00a0follow `<!-- Open Graph data -->` `<meta content=\"T Here\" />` `<meta />` `<meta />` `<meta />` `<meta Here\" /> I could see that I already had some meta tags being generated using the Jinja templates so I set about copy-pasti and modifying them to build the new tags. I had some issues with trailing white space or line breaks being included within the content string. This was solved like\u00a0so: {# Adding '-' after and before the %'s strips white space and line breaks #} {% block descriptio %} {%- if -%} {{ }} {%- else -%} {{ }} {%- endif -%} {% endblock descriptio %} I also needed to use some blocks more than once, because a descriptio tag was already included but Facebook wants an and Twitter wants a too. All three of these tags will include the same text (generated in the Jinja2 snippet above). If a block only needs to be used once then it\u2019s generated like\u00a0this; <meta content=\"{ block descriptio %}{% endblock descriptio %}\"> But If you call \u201c{% block descriptio %}{% endblock descriptio %}\u201d again Jinja will throw you an error. The documentat (and SO) reveal that the solution is to\u00a0use: <meta content=\"{ }}\"> This allows you to reuse blocks multiple times and keep your Finally, when I was testing Facebook to see if the correct text or image was being picked up I was initially frustrated to see that the new tags were not having any effect. This is because Facebook crawls your site and saves what it finds. If you want it to take a fresh look at your page with its new meta tags, you need to tell Facebook to crawl the page again, using the Facebook You can see the new sharing buttons below, please click them and see what\u00a0happe Note: My first article describing how I began to use Pelican is here"},{"title":"Blockchains from the ground up: Part\u00a01","url":"blockchain-introduction.html","body":"How to maintain a reliable list across a small network without a This is part 1 of an introducti to the key features of a generalise blockchain I haven\u2019t included references to Bitcoin or any particular digital currencies or blockchain This is because a digital currency is just one applicatio of Create a financial document that cannot be forged or\u00a0dispute Let\u2019s imagine there is a village somewhere where people still trade by bartering. John has some apples whilst Lizzie has some oranges. John would like an orange, and offers Lizzie an apple in exchange. She accepts, and writes John a\u00a0receipt. Date: 1234 From: Lizzie To: John What: 1 Orange Price: 1 Apple So far, so good. The receipt is evidence of the transactio The next day John wants an orange but doesn\u2019t have anything to exchange. He offers to write Lizzie a note saying he owes Lizzie 1 orange (an IOU). They think about this and agree that John should sign the note so that Lizzie can prove that John owes her 1\u00a0orange. Date: 1234 From: John To: Lizzie What: 1 Orange Signed: John's signature, Lizzie's signature This IOU is a nice gesture, but it\u2019s simple to forge. Lizzie has the only copy of the IOU and once Lizzie has seen Johns signature, she could easily copy it and create more IOU\u2019s. She could also change this IOU from 1 orange to 11 oranges (for example) and John couldn\u2019t prove what the original amount was. If Lizzie and John disagreed over what was owed it would be impossible to know who was telling the truth. It\u2019s one person\u2019s word against the\u00a0other. Lizzie realises this and suggests an improvemen - they will find a witness and make 3 copies of the IOU. Each copy will be signed by Lizzie, John and the Witness. Lets call this Date: 1234 From: John To: Lizzie What: 1 Orange Witness: Walter Signed: \"John's signature\" \"Lizzie's signature\" \"Walter's signature\" This is a much stronger document and is more difficult to forge. If Lizzie changes the \u201cWhat:\u201d to \u201c11 Oranges\u201d, both John and Walter will have copies of the original with her signature on it. It will be 2 pieces of evidence against Lizzie\u2019s 1. Lizzie will be laughed out of court.\u00a0Hah 3 Party transactio work pretty well, and this is how most transactio are recorded today. But there is a weakness: If Lizzie can bribe Walter then the transactio can be falsified! John would rely on Walter to verify his version of the transactio but would be let down by Walters lack of integrity. Lizzie and Walter could change 1 orange to 11 oranges and if Lizzie offered Walter some of the extra oranges this would give them both an incentive to forge the documentat If Walter liked oranges enough, he might not care that his career as a witness will be\u00a0ruined. This is a problem for modern financial systems and a great deal of time, money and regulation is devoted to trying to ensure that third parties are trustworth E.G. If I buy a car and my bank is in cahoots with the car dealership I could be defrauded. Reducing this risk to an acceptably low level makes financial services slower and more expensive than they would otherwise need to\u00a0be. The solution is public-key infrastruc (which is introduced in my previous post). In this system, each individual generates their own public-pri key pair. They keep their private key private and make their public key freely available. A detailed descriptio of public-key cryptograp is out of scope for this post, but\u00a0briefl A public key is derived from a private key, and this pair together have a set of unique mathematic properties Either key can be used to encrypt a message but only the other key can be used to decrypt it. You cannot use the same key to encrypt and decrypt a message. If the private key is used to encrypt then anybody can decrypt (because the public key is publicly available) and whilst this is clearly a terrible way to keep a secret it\u2019s a great way to verify who encrypted the message, because only one person has the private key. Because of this, using a private key to encrypt a message is effectivel creating a digital signature which cannot be forged. (If the public key is used to encrypt a message then only the private key can be used to decrypt it, and this approach is used to transfer secret Back to the fruit. If Lizzie wants to accept John\u2019s IOU she can use public-key cryptograp and no-one needs to worry about Walter. There are 3 steps to 1] Create the IOU stating that John owes Lizzie 1\u00a0orange. Date: 1234 From: John To: Lizzie What: 1 Orange 2] John creates a public private key pair and encrypts the IOU using his private key. He adds an unencrypte \u201cFrom\u201d\u00a0lin From: John Date: 1234 To: Lizzie, What: 1 Orange <- Signed and encrypted by John using his private key 3] John makes his public key freely available to anyone who wants\u00a0it. This will work because anybody (not just Lizzie) can check that John signed the IOU. The transactio can be verified by looking at the \u201cFrom\u201d part of that transactio noticing that this transactio is supposedly from John and then using John\u2019s public key to decrypt the encoded The signature can only be decrypted using John\u2019s public key if his private key was used to encrypt it. Because John is the only person with his private key, that proves the transactio is valid, and Lizzie isn\u2019t dishonestl creating a debt for John to\u00a0pay. Clearly if John discloses his private key (or its stolen) then he will make the system insecure, but this is a problem with John and his security protocols, not with Create and maintain an accurate list So far we\u2019ve seen how 1 IOU (for an orange) can be securely created, signed and verified. This process can extended to be used by more people to exchange more fruit. For\u00a0exampl The original\u00a0n From: John Date: 1234, To: Lizzie, What: 1 Orange <- Signed and encrypted by John using his private key Then some From: Lizzie // Date: 1235, To: John, What: 2 Apples <- Signed and encrypted by Lizzie using her private key From: John // Date: 1236, To: Chris, What: 1 Banana <- Signed and encrypted by John using his private key From: Chris // Date: 1237, To: Lizzie, What: 2 Bananas <- Signed and encrypted by Chris using his private key After these 4 transactio between John, Chris and\u00a0Lizzie John owes 1 orange to Lizzie and 1 banana to\u00a0Chris Lizzie owes 2 apples to\u00a0John Chris owes 2 bananas to\u00a0Lizzie. This is confusing, (and ridiculous It is not possible to know who is the most in debt or who is the most wealthy. Lizzie owes 2 apples, but is owed 2 bananas and 1 apple. Does that mean her fruit business is losing money or making money? We cannot say. To be able to know we need to use the same unit of value for all the fruits. Lets say that an orange is worth 2 apples, and a banana is also worth 2 apples (therefore 1 banana = 1 orange.), also lets invent a currency called \u201ccoins\u201d and say 1 apple is worth 1 coin. The 4 transactio can now be rewritten\u00a0 From: John // Date: 1234, To: Lizzie, What: 2 coins <- Signed and encrypted by John using his private key From: Lizzie // Date: 1235, To: John, What: 2 coins <- Signed and encrypted by Lizzie using her private key From: John // Date: 1236, To: Chris, What: 2 coins <- Signed and encrypted by John using his private key From: Chris // Date: 1237, To: Lizzie, What: 4 coins <- Signed and encrypted by Chris using his private key By going through the list of transactio we can see\u00a0that: John owes Lizzie and Chris 2 coins each, and is owed 2 coins from Lizzie. His net amount is\u00a0-2 Lizzie owes John 2 coins but is owed 4 coins from Chris. Her net amount is\u00a0+2 Chris owes Lizzie 4 coins but is owed 2 coins from John. His net amount is\u00a0-2 So far Lizzie is the only person who appears to have any What if Lizzie wanted to use the 4 coins that she is owed by Chris to buy something from John? Could she use this system to transfer Chris\u2019 promise to pay her 4 coins so that Chris would pay John instead? The fact that everyone can be sure that the record of the transactio is accurate and authentic allows a debt to be used as payment. Lizzie\u2019s transactio would look like\u00a0this: From: Lizzie // Date: 1235, To: John, What: ba781... <- Signed and encrypted by Lizzie using her private key The \u201cWhat\u201d section contains a hash of the original transactio (with Chris) that she wants to transfer to John. A hash is the signature for a file or some text and in this case it is the signature for Lizzie\u2019s transactio with Chris. The signature is unique to each transactio and identifies which transactio is being used as payment. Because both transactio are signed using Lizzie\u2019s private key, it is simple to verify that Lizzie has the right to use this previous transactio where she is owed (or paid) some coins as payment to This shows how public-pri key infrastruc can be used to securely record transactio and enable trade between a group of people, - under certain conditions Blockchain can be used to transfer units of value like in this example, but we could just as easily put selfies or certificat of ownership (for houses, financial instrument diamonds, etc) inside the \u201cWhat\u201d part of the transactio If we make two other changes - removing the \u201cTo\u201d part of the transactio and including a hash of the transactio as part of the text which is signed using a private key. If we do this, then a record would\u00a0be: From: Chris // Date: 2345, What: \"A photo of me\" <- Signed and encrypted by Chris using his private key This would create a reliable record of what Chris claims he looks like. He can confidentl send anyone this record and if they have his public key then they can verify that it is Chris himself who signed it and is asserting that the photo is him. If somebody changed the photo then the data in the transactio would change and the transactio will have a new hash value. The new hash value will not match the hash value contained within the signature, and the text in the signature cannot be changed because it can only be encrypted using Chris\u2019 private key, which only Chris has. Therefore it will be simple to show that someone other than Chris is trying to change the\u00a0photo. Another use for public-key cryptograp arises if Chris were an employee in a bank, and the \u201cWhat\u201d contained documents about a customer the bank is providing financial services for. In this scenario, Chris (represent the bank) is effectivel confirming the customer\u2019s true identity and documentin the evidence that\u2019s been collected to show that the bank knows who their customer really is. If the transactio included a new section called \u201cCustomer ID\u201d (for example) then a database of all customers whose identity checks have been successful completed can be made. This can be shared with other department (or banks) easily and immutably. This is the concept behind KYC on a\u00a0blockcha Back to our fruit traders: At the moment a participan is allowed to carry a net negative balance. For this system to work in reality, the creation of \u201ccoins\u201d will need to be controlled in order to maintain their value. In the example above, people can freely create \u201ccoins\u201d and can also carry negative amounts of \u201ccoins\u201d. This would result in the value of a \u201ccoin\u201d plummeting Therefore their creation (and conversion from fruit) must be controlled in a Our examples so far only include 3 people. If there are a lot of people in the network it wouldn\u2019t be feasible to insist that everyone is present or online each time a new transactio is added to the list (the chain) of transactio However if we allow transactio to be added whilst some people are offline we create an opportunit for fraud. The reasons why, and the solution to this and other problems will be described in part 2."},{"title":"London Rent vs. London\u00a0Salaries","url":"london.html","body":"Living in London is expensive, everyone in the UK knows this. Everyone knows that this is mostly due to the price of property, which we all enjoy talking\u00a0ab I\u2019ve lived here for 3 years now, and when my wife and I were both working we lived comfortabl Two incomes under one roof is just fine. Not as fine as in many other cities, but reasonable Just over a year ago my first child was born, and now that my wife\u2019s maternity pay has ended we are a single income household. This is not\u00a0fine. Even though I earn almost \u00a350k/year, and any salary increase will be taxed at 40%, I cannot cover essential day-to-day costs. I\u2019m in the 40% tax bracket, and my basic monthly outgoings exceed my monthly take home pay by \u00a3400. If my employer wanted to pay me enough to cover essential costs, it would cost them approximat double the shortfall due to tax. None of this makes\u00a0sens Salaries and living expenses have become For most people the most obvious way to increase wealth is to buy property. Mortgage repayments are cheaper than rent, and the value of the property increases over time. Double win. But there is a big hurdle to overcome before this is possible - saving money for a deposit on a house is often impossible without external help (i.e. the \u201cBank of Mum and Dad\u201d), because so much of the money earnt must be spent on rent\u00a0first It\u2019s a trap. Repayments on a mortgage are cheaper than paying rent, but you cannot save enough for a so much of your salary goes to paying rent1,\u00a0and the average cost of a flat in London is 500k2, which means a 5% deposit is \u00a325k \u2014 which is a higher deposit than you would need anywhere else in the UK, despite being the place you are least able to accumulate What if my partner went back to work and we put our baby in childcare? Childcare is not cheap, and working would need to bring in more than the cost of childcare. This requires a higher than average salary from a graduate job, (which is the life stage when people might reasonably start having children), so unless both parents are unusually high earners that option isnt viable\u00a0eit Living in London is only financiall possible if you are either single or household income is more You\u2019d be lucky to find a 1 bedroom flat for less than \u00a31000/mont According to Rightmove, an estate agent with some useful price statistics"},{"title":"Introduction to the \u00c6ternity blockchain\u00a0project","url":"aeternity.html","body":"These are my notes on the \u00e6ternity blockchain project, I\u2019m not affiliated with the \u00e6ternity\u00a0t \u00c6ternity is a new blockchain project that is pre-launch The headline goal is to securely facilitate large volumes of which interface with external data sources. This is made possible via a decentrali oracle based on prediction markets. These terms are explained below. The \u00e6ternity project has proposed several notable Smart Contracts Oracles and native Governance by Written in\u00a0Erlang Different types of\u00a0node Sharding Smart Contracts A smart contract is a way to execute a contract without an intermedia (middle-ma and The smart contract is a protocol which is stored and executed on a blockchain executing transactio (outputs) based on specific inputs and programmab logic automatica The logic often mirrors that contained in clauses of a State channels are payment networks that exchange funds off-chain and periodical settle up accounts with the main blockchain (The Bitcoin Lightning Network is creating a system for routing Bitcoin payments through State channels increase scalabilit by making groups of transactio independen of each other. This allows them to be processed in\u00a0paralle \u00e6ternity proposes executing in state channels (Turing complete means, colloquial real-world and general purpose), which should allow greater volumes of transactio and make the smart contracts more secure and easier to\u00a0analyse This is because executing the off-chain makes them private and the code used to execute the smart contract won\u2019t need to be broadcast to the primary blockchain This should increase processing capacity by allowing contracts to execute in\u00a0paralle Disadvanta to the state-chan approach include reduced transparen as running smart contracts in state channels requires more trust in both the contract creator and the node running\u00a0it Oracles and The Oracle functional allows to interact with data outside the \u00e6ternity blockchain This is possible by checking on-chain prediction market results and rewarding users who made the correct prediction Users are rewarded through automated payments and the immediate recording of transactio in the blockchain This creates incentives to participat in prediction markets, which have been shown to be\u00a0effecti On-chain, rather than off-chain allows greater efficiency The prediction market is expected to run using a native (on-chain) consensus procedure. The oracle mechanism is designed to use the same Governance by Oracle functional compliment prediction Prediction markets are proposed to implement governance of the \u00e6ternity blockchain This is a new\u00a0approa The \u00e6ternity protocol would be governed by user input. A prediction market will exist where changes to features and protocols would result in a higher token\u00a0valu The incentive to increase the value of a token (\u00c6on) would allow the \u00e6ternity community to decide efficientl which changes to\u00a0impleme Low level protocol changes to variables like block times and block capacity could be\u00a0possibl The consensus developed by the prediction market will initially provide input to the developmen Later, a fully autonomous prediction market for governance is expected (a DAO) Written in\u00a0Erlang Erlang is normally used for large-scal systems that manage the allocation of scarce network resources (telecoms, banking, Could make it easier to run a lightning network and process many state-chan in\u00a0paralle As far as I know, \u00c6Ternity is the first blockchain project to be written in\u00a0Erlang Different types of\u00a0node The \u00e6ternity network will contain nodes with different functions. Each type of node will contribute towards the efficient functionin of particular aspects of the\u00a0networ Node types will\u00a0inclu Liquidity - Lots of channels and lots of users. Open a channel to issue a contract, for a\u00a0fee. Exchanges - Trustless exchanges of assets offered by market makers. Profitable to market makers because of Presumably features such as consensus algorithms and prediction markets will also require their own dedicated node types. Users of the node will incur transactio fees to participat providing an incentive to run a\u00a0node. Sharding Allows a greater transactio volume, removing scalabilit problems that Bitcoin and Ethereum Sharding works by splitting the space of possible accounts into subspaces (for example based on the first digit of a Each shard gets a set of validators Each validator validates 1 shard\u00a0only Contracts and transactio within the same shard work as\u00a0normal Contracts and transactio across shards require alternativ techniques based on"},{"title":"The \u00c6ternity ICO: My\u00a0experience","url":"aeternity-ico.html","body":"On April 3rd, I happened to be Googling digital currencies and blockchain innovation when I came across the \u00c6ternity website and skimmed their white paper. The project is ambitious, like many crypto projects, but seems well organised. The team is well known in the space. There is a clear plan to develop the project and create a blockchain technology that, if successful could bring. A step change in the use of digital currencies for high volume low value transactio and the viable implementa of The ICO To my surprise, I realised that phase 1 of the Initial Coin Offering (ICO) was about to begin, and if I wanted I could acquire the rights to \u00c6ons (the \u00c6ternity token). During phase 1, 1 ETH would purchase 1100 \u00c6ons. In early April 2017, 1 ETH was worth about\u00a0\u00a338. I was willing to make a small and risky investment but in order to do that I would need to work out how to convert my convention Sterling into Bitcoin or Ether, in order to then purchase \u00c6ons. The \u00c6ternity website made it super easy to set up an Etherium wallet, and to use that wallet to invest in the \u00c6ternity project, but buying Ether immediatel and putting it in my new wallet proved to Helpfully, the \u00c6ternity project had partnered with the Swiss firm \u2018Bitcoin Suisse AG\u2019 who would directly convert to \u00c6ons from fiat currencies cutting out the need to purchase an intermedia digital currency. However once I\u2019d completed the identity checks and signed, scanned and sent the multiple forms, I realised I\u2019d need to pay a \u2018signing on\u2019 fee of about \u00a370. To Bitcoin Suisse\u2019 credit though, they did manually approve my identifica and contract within an\u00a0hour. At the time, I thought the project would still be a good investment even with this extra cost, but I determined to exhaust all other possibilit first. I\u2019m familiar with cryptocurr wallets and public/pri keys due to some previous research, so I was able to immediatel begin trying to set up an account with an\u00a0exchang Exchanges What followed was a fairly chaotic few hours where I would sign-up to several exchanges and see how close I could get to purchasing either Bitcoin or Ether immediatel before realising I either had to wait 48 hours for security clearances or provide additional details, or wait for manual verificati of my scanned By the end of the evening I had a rough idea of which usernames, passwords and (small) sterling amounts had been submitted to each\u00a0excha After a couple of false starts, I used a combinatio of the Coinbase desktop website and their iOS app to purchase ETH up to their weekly limit, and then used multiple cards to increase my holding of ETH. The Coinbase app would bug out when processing a debit card payment and verifying the card details with the bank. This had initially led me to try other exchanges, where I would hit other roadblocks and delays. CEX.IO, for example, doesn\u2019t allow you to make trades in the first 48 hours (IIRC) after registerin which is reasonable enough unless you\u2019re in a\u00a0hurry. Conclusion Once I had a few Ether to my name, the rest of the process was simple. I was delighted to visit Etherscan. and view the details of my Etherium - \u00c6ternity transactio almost immediatel This gave me a lot of confidence that I hadn\u2019t sent money into a void, and was a nice contrast to my (successfu experience buying bitcoin in early\u00a02014 Finally, the simple tool to check your \u00c6 balance at the bottom of Eternity\u2019s contributi page assured me that I\u2019d made my investment successful To me, the \u00c6ternity project stands as an exciting endeavor seeking to solve some widespread and highly valuable technical challenges I hope they\u2019re successful and wish them\u00a0well."},{"title":"Blogging with\u00a0Pelican","url":"pelican.html","body":"When I began blogging in 2016, I became more aware of how blogs are designed. Many of my favorting blogs had simple designs which made it easier to focus on the content, and they loaded really fast. (E.g. and CuriousGnu I wanted this for my blog, too. I\u2019d used Wordpress to build and publish my blog which was a great way to begin, but I felt I was compromisi on its design and functional I wanted to have control over my This led me to static sites which contain only fixed content and are faster to load and easier to design than one built using a dynamic blogging platform such as Wordpress. Because I was already familiar with Python I chose Pelican rather than another static site generator such as\u00a0Jekyll. There are plenty of sites to tell you how to start blogging in Pelican, so here I will focus on my experience after the initial set-up. When I was learning how to begin, I found Amy Hanlons blog particular useful and\u00a0clear. The learning\u00a0c \u2026 was longer than I expected. Since setting out to switch from Wordpress to Pelican, I\u2019ve taught myself enough of the following tools to hack this site together. I\u2019m really happy about this because these tools could be used in future projects\u00a0t HTML I find HTML quirky but intuitive. Tags make sense, comments are laborious and learning by google is relatively quick. Whatever it is I\u2019m trying to do (like add a link to jump back to the top of the page) someone will have posted the CSS Writing CSS feels a lot more concise that HTML but it also felt impossible to learn without taking a step back and reading an introducto course. Usually I learn by hacking new phrases together from existing examples so it was frustratin to go backwards before progressin There was a lightbulb moment when I realised CSS Selectors were a thing, and realising CSS files get called in the header (usually) of an HTML\u00a0file\u2026 I ended up using a trial subscripti to Thinkfuls Front-end developer course, which is pretty good at explaining how CSS is structured and how to arrange content on a page. If I still had access, I\u2019d be completing the second half of the course\u00a0:) Jinja is a tool written in Python to create HTML pages. It doesn\u2019t look intuitive to me, but I\u2019ve been able to get enough done by copy-pasti similar snippets from other parts of the theme I started with (Thanks Molivier!) to make the changes I wanted. I\u2019d like to learn more as it seems Pelican To build a website using Pelican, you need to run commands from the terminal. There are various commands but I found myself using only a few regularly. will generate a project skeleton to get you started. \u201cmake devserver\u201d will initialise a local server and generate output files so that I can view changes locally before uploading (its opposite is \u201cmake stopserver Finally \u201cpelican content -s generates the html and css for remote hosting. Some of the plugins I use such as \u201cAssets\u201d which minifies the CSS only work when publshconf is called, which confused me initially as I didn\u2019t think it was working when I was only using Git This really challenged me, and I still don\u2019t feel like I know what I\u2019m doing. Git is far more powerful than I need it to be, when all I want to do is undo some erroneous edits and upload a new version of the site to\u00a0Github. I can stage and commit files, I can create local and remote repo\u2019s from the command line. I can change a remote\u2019s URL, reset a repo and force a push or a pull. That\u2019s all. I haven\u2019t tried to merge or to create a test branch, and if some part of the process goes wrong it usually takes hours to make it right\u00a0agai This is one tool for which the awesome SO and Google cannot magic up the exact right answer For example, there is still an output folder in the source repo that is\u2026 mysterious to me. Its not the real output, its a version frozen in time from a few weeks ago, and it has an \u201c@\u201d in its name. I don\u2019t know how it got there. It was created one afternoon in a blur of frustrated google queries and I find git\u2019s commands the least intuitive of all the tools I use, with its preceeding single dashes and double dashes, and random words thrown into the middle of otherwise But Git is ubiquitous and Github is awesome, so I will learn\u00a0it. Github pages with an external URL You\u2019ll need to add a file called CNAME to the repo containing the output. CNAME should contain the address of your site in You\u2019ll also need to update the DNS records of your domain name to point the name at Github\u2019s servers. For Github, you need two \u201cA Records\u201d with host type \u201c@\u201d and values and respective You also need a CNAME record with host type \u201cwww\u201d and the value equal to It took about 12 hours for the changes to propagate, and during that time I had variable success loading the\u00a0site. Plugins One thing I didn\u2019t want when moving away from Wordpress was a site bloated with features that didn\u2019t make the content easier to read. However I found I still needed a few plugins to optimise my site and provide some basic functional that doesn\u2019t come with the Super useful, as all I need to do to publish a notebook as a webpage is copy the .ipynb file into the context directory and add a sidecar .ipynb-met file with standard meta data. This functional is one of the main reasons why Pelican is popular with data bloggers. (Though Nikola is Neighbors At the end of a post there should be a link to the previous and next blog posts - I was surprised this wasn\u2019t included as standard. After putting the plugin in the plugins folder and updating you need to copy a couple of jinja snippets into a template, and maybe add some css to make the links look\u00a0nice. Make those images as small as possible to help make the site as fast as possible. Add the plugin, update and thats\u00a0all. Assets Before I started working with Pelican, minifying css and JavaScript would have been too advanced. But once Pingdom and Google Pagespeed started criticisin me for my multiple .css files, I accepted the\u00a0challe Conclusion I\u2019m super happy wth the websites design and speed. It\u2019s designed the way I want it, and I\u2019ve learnt a ton of useful stuff along the\u00a0way. Update: My second post about blogging in Pelican is here."},{"title":"Analysing a personal\u00a0library","url":"books.html","body":"A friend of mine has collected books for many years and has recently begun to catalogue them. In this post, I show some simple analysis of the catalogue and then query an ISBN database to fill in some missing\u00a0da In\u00a0[1]: from import HTML function code_toggl { if (code_show } else { } code_show = !code_show } $( document </script> <font toggle the visibility of the code blocks, click <a <script> window.onl = function() //time is set in millisecon 10000) }; </script> ''') Out[1]: function code_toggl { if (code_show } else { } code_show = !code_show } $( document To toggle the visibility of the code blocks, click here. Set-up and data some settings and import some\u00a0packa In\u00a0[2]: # Display plot results inline, not in a separate window %matplotli inline %pylab inline # Set the size of all figures = (14, 5) import pandas as pd import re import bibtexpars import numpy as np import as plt Populating the interactiv namespace from numpy and matplotlib Load the In\u00a0[3]: table = table = table[0:91 df = table orig_rows = (df.shape[ print(\"The are %d rows in the catalogue\" % (df.shape[ ) There are 9187 rows in the catalogue Data formatting and tidying:\u00b6V the top 5 rows to see how the data is arranged and how many cells are\u00a0comple In\u00a0[4]: df.head() Out[4]: Location Subject Title Author Publisher ISBN? Shelf Pages Price Value Date 0 HR Islam The Islamic Invasion R Morey Harvest HP 1960 0 89081 983 1 17cm 221 3 8 2008-04-01 00:00:00 1 HR Word lists New Testament Word Lists Morrison & Barnes Erdmans 1975 0 8028 1141 8 NaN 125 3 NaN 2008-04-01 00:00:00 2 HR Theology: Salvation The Triumph of the crucified E Sauer Paternoste 1952 NaN NaN 207 3 10 2008-04-01 00:00:00 3 HR Early Fathers Ante-Nicen Christian Library Ed Menzies T&T Clark 1897 NaN NaN 533 9 NaN 2010-03-01 00:00:00 4 HR Apologetic Earth\u2019s earliest ages G.H. Pember H & S 1895 NaN NaN 494 3 NaN 2008-04-01 00:00:00 Set float format to two decimal places (currency) Not all rows can become a\u00a0float: In\u00a0[5]: = def to_number( try: s1 = return s1 except ValueError return s df.Price = f : to_number( df.Value = f : to_number( Find and remove blank\u00a0rows In\u00a0[6]: # How many rows are all NaN values df = # drop a row only if ALL columns are NaN print('%d row removed ' % (orig_rows - df.shape[0 ) # 1 row contained all NaN and has been removed 1 row removed List the number of rows in each column which are\u00a0empty: In\u00a0[7]: # How many rows in each column are NaN Out[7]: Location 29 Title 34 Publisher 175 Shelf 336 Pages 540 Author 915 Price 3611 ISBN? 4770 Date 5712 Subject 6208 Value 9179 dtype: int64 Based on these results, title and publisher are the most Split a column containing two types of\u00a0data: The \u201cPublisher column contains both the publisher and the year it was published. This should be split into two\u00a0column In\u00a0[8]: = None # default='w df['PubYea = expand=Tru # regex is confusing = expand=Tru Improve the format of the \u2018Date\u2019\u00a0col In\u00a0[9]: df['Date'] = The data frame is now in the columns I want it to be in, and the top 5 rows\u00a0are: In\u00a0[10]: df.head() Out[10]: Location Subject Title Author Publisher ISBN? Shelf Pages Price Value Date PubYear 0 HR Islam The Islamic Invasion R Morey Harvest HP 0 89081 983 1 17cm 221 3.00 8.00 2008-04-01 1960 1 HR Word lists New Testament Word Lists Morrison & Barnes Erdmans 0 8028 1141 8 NaN 125 3.00 NaN 2008-04-01 1975 2 HR Theology: Salvation The Triumph of the crucified E Sauer Paternoste NaN NaN 207 3.00 10.00 2008-04-01 1952 3 HR Early Fathers Ante-Nicen Christian Library Ed Menzies T&T Clark NaN NaN 533 9.00 NaN 2010-03-01 1897 4 HR Apologetic Earth\u2019s earliest ages G.H. Pember H & S NaN NaN 494 3.00 NaN 2008-04-01 1895 of books by The bar chart below shows how many books in the library were published in a given decade. The list below shows the 5 oldest\u00a0boo In\u00a0[11]: = '{:,}'.for df.PubYear = fig = // 10 * logy = False) of Publicatio of Books\") fig Out[11]: at 0x10501f2b View the 5 oldest\u00a0tit In\u00a0[12]: df['PubYea = df2 = != 0.0] Out[12]: Location Subject Title Author Publisher ISBN? Shelf Pages Price Value Date PubYear 4753 Lib NaN In Christ\u2019s own country Dom Ernest Graf Burns Oates NaN Sh.4.5 302 Gift NaN 1999-07-30 1037 3043 StM NaN The First Epistle of Peter C.E.B. Cranfield SCM Press interestin Sh.5.5 128 NaN NaN NaT 1050 4574 Lib Music Score Easy-Play Speed Music; waltz clas NaN Sight & Sound NaN Sh.4.4 47 99P NaN NaT 1076 7184 25A NaN The Noble Qur\u2019an transl Al-Hilali & Khan Madinah NaN Sh.3.6 956 3.0 NaN 2010-12-10 1417 3296 Lib NaN Chained Bible NaN Chris Barker Very incomplete Sh.1.1 NaN NaN NaN NaT 1585 List the number of books in each\u00a0locat In\u00a0[13]: df3 = df = Out[13]: Location 25A 3411 LIB 2457 CH 1088 ST9 1000 STM 886 HR 305 NAN 29 ST.M 3 HR 2 ST M 1 SA 1 LB 1 CH 1 :LIB 1 dtype: int64 Create a list of the differnet subjects, order the list by the most In\u00a0[14]: df4 = df df4[\"Subje = Out[14]: Subject NAN 6208 COMMENTARY 61 LOCAL HISTORY 58 SERMONS 41 THE CENTURY BIBLE 37 THEOLOGY 36 CHRISTIAN BIOGRAPHY 34 BIOGRAPHY 31 NT COMMENTARY 31 HEBREW GRAMMAR 30 SACRED BOOKS OF THE EAST 30 POETRY 29 CLARK'S FOREIGN THEOL LIB 28 GENERAL EDIT ANTONIA FRAZER 25 NOVEL 24 WRITERS AND THEIR WORK 24 CATALOGUE 22 AUTOBIOGRA 20 OT COMMENTARY 19 GREEK 19 THE EXPOSITOR' BIBLE 18 SRIMAD BHAGAVATAM 18 THE BABYLONIAN TALMUD 18 PHOTOGRAPH 17 GREAT MUSEUMS OF T WORLD 15 CHURCH HISTORY 14 DAILY READINGS 14 CHRISTIAN LECTURES 14 NOTES ON THE CATHEDRALS 14 INTERNATIO CRITICAL COMM 13 ARAMAIC 13 THE CLARENDON BIBLE 13 FICTION - CADFAEL 13 CLARK'S FOREIGN THEOL LIB. 13 APOLOGETIC 12 PSALMS 12 THE CAMBRIDGE BIBLE 12 PRAYER 12 LIFE LIBRARY OF PHOTOGRAPH 11 COMMENTARY ON HOLY SCRIPT 11 THE MASTERPIEC LIBRARY 11 COMMENTARY ON THE O.T. 10 HYMNS 10 POEMS 10 MYSTICISM 10 DICTIONARY OF THE BIBLE 10 EXHIBITION CATALOGUE 10 POETICAL WORKS OF TENNYSON 10 DICTIONARY 10 HISTORY 10 dtype: int64 Create a list of authors in the library. Order the list by number of\u00a0books: In\u00a0[15]: df5 = df df5[\"Autho = Out[15]: Author NAN 915 VARIOUS 31 BHAKTIVEDA S PRAB 19 C.H. SPURGEON 19 ELLIS PETERS 17 ED. RABBI I. EPSTEIN 17 LESLIE WEATHERHEA 15 ED CARLO RAGGHIANTI 15 ALBERT BARNES 14 JAMES HASTINGS 13 GEORGE ADAM SMITH 13 WILLIAM TEMPLE 12 JAMES MOFFATT 11 ED J A HAMMERTON 11 KEIL & DELITZSCH 11 WILLIAM BARCLAY 10 SHAKESPEAR 10 PETER ACKROYD 10 IAN WILSON 9 CHARLES DICKENS 9 BERNHARD WEISS 9 J.B. PHILLIPS 9 ED QUENNELL & HODGE 8 CHARLES GORE 8 H.V. MORTON 8 M.F. SADLER 8 VARIOUS AUTHORS 8 ED ANDREW LANG 8 GEZA VERMES 8 S.R. DRIVER 8 EVELYN UNDERHILL 8 \" 8 HENRY ALFORD 7 ALDOUS HUXLEY 7 ED JAMES HASTINGS 7 ED ARTHUR MEE 7 ROY STRONG 7 ALEXANDER MACLAREN 7 MARCUS DODS 7 JOACHIM JEREMIAS 6 THOMAS WRIGHT 6 ED R. CROMARTY 6 SUSAN GLYN 6 DAVID FOUNTAIN 6 WILLIAM WHISTON 6 C.S. LEWIS 6 BARRIE TRINDER 6 DAVID TRUMPER 6 G. CAMPBELL MORGAN 6 ALISTER MCGRATH 6 dtype: int64 Distributi of book length by number of\u00a0pages: In\u00a0[16]: def try: s1 = return s1 except ValueError return '' df.Pages = f : df6 = != ''] 2000, 100.0)) fig = range=[0, 2000]) of Pages\") of Books\") Out[16]: at 0x10585b3c Query an ISBN database to find missing\u00a0da Lastly, I thought it would be a fun challenge to fill in gaps in the data. The table below shows rows with ISBN number but missing either Author, Title or\u00a0Publish It turns out that there are only 10 rows that meet this criteria, and in all cases it is the publisher that is\u00a0missing In\u00a0[17]: df7 = & ((df['Auth == '') | (df['Title == '') | == ''))] df7 Out[17]: Location Subject Title Author Publisher ISBN? Shelf Pages Price Value Date PubYear 1430 ST9 BIOGRAPHY OF SCIENTIST Longitude (John Harrison) DAVA SOBEL 1 85702 571 7 Sh.1.2 184.0 5.99 NaN NaT 1998 2707 STM DEVOTIONAL Romans: Momentous News DAVID COOK 978 1 906173241 Sh.3.4 55.0 1.0 NaN 2011-07-28 2011 3874 LIB NAN Annie\u2019s Box - Darwin\u2019s daughter RANDAL KEYNES 1 84115 060 6 Sh.2.4 331.0 3.99 NaN 2002-07-20 2001 4705 LIB HISTORICAL NOVEL Galileo\u2019s Daughter DAVA SOBEL 1 85702 861 9 Sh.4.5 429.0 NaN NaN 2002-09-27 1999 5949 25A NAN Short Life Long Times of Mrs Beeton KATHRYN HUGHES 1 84115 373 7 Sh.1.4 525.0 \u00a32.50P NaN 2012-02-09 2005 6008 25A NAN Signs in the Sky (Birth of a New Age ADRIAN GILBERT 0 609 80793 5 Sh.1.5 329.0 4.0 NaN 2012-03-07 2001 6097 25A NAN Isaac Newton, the last Sorcerer MICHAEL WHITE 1 85702 706 X Sh.1.6 403.0 \u00a31.50P NaN 2012-02-09 1997 6663 25A NAN Live Wires - powerful stories of cha D. J. CARSWELL 978 1 906173 13 5 Sh.2.7 124.0 1.0 NaN 2011-06-29 2010 8640 25A KING ARTHUR QUINCENTEN One in Specyal ED SIDNEY HART 0 948485 00 0 Sh.5.9 145.0 \u00a32.49P NaN 2003-06-28 1985 8845 25A NAN The Order of St John - a short history E L EDMONDS 0 947718 07 9 Sh.5.7b 35.0 \u00a33.75P NaN NaT 1986 In\u00a0[18]: from isbnlib import * from isbnlib.co import * from import * import bibtexpars def SERVICE = 'isbndb' APIKEY = 'IZXL3ESD' # YOUR key APIKEY) # register your key bibtex = isbn = clean(isbn try: a = SERVICE)) return a except: return 'isbn is invalid' In\u00a0[19]: def get_pub(is bibtex_str = has_isbn(i try: bib_db = dic = return except: return In\u00a0[20]: df7['ISBN? = df7.Publis = f : get_pub(f) The table below shows the results of the isbnlib query. I thought it odd that all the \u2018missing\u2019 publishers names began with a number. It turns out that the regex method I used to split publisher name and year of publicatio into separate columns doesnt work when there are numbers in the publishers name. Rather than go back and correct this, I\u2019ll leave the script as it is to show how to use the In\u00a0[21]: df7 Out[21]: Location Subject Title Author Publisher ISBN? Shelf Pages Price Value Date PubYear 1430 ST9 BIOGRAPHY OF SCIENTIST Longitude (John Harrison) DAVA SOBEL Fourth Estate 1 85702 571 7 Sh.1.2 184.0 5.99 NaN NaT 1998 2707 STM DEVOTIONAL Romans: Momentous News DAVID COOK 10Publishi 978 1 906173241 Sh.3.4 55.0 1.0 NaN 2011-07-28 2011 3874 LIB NAN Annie\u2019s Box - Darwin\u2019s daughter RANDAL KEYNES 4th Estate 1 84115 060 6 Sh.2.4 331.0 3.99 NaN 2002-07-20 2001 4705 LIB HISTORICAL NOVEL Galileo\u2019s Daughter DAVA SOBEL Fourth Estate 1 85702 861 9 Sh.4.5 429.0 NaN NaN 2002-09-27 1999 5949 25A NAN Short Life Long Times of Mrs Beeton KATHRYN HUGHES None 1 84115 373 7 Sh.1.4 525.0 \u00a32.50P NaN 2012-02-09 2005 6008 25A NAN Signs in the Sky (Birth of a New Age ADRIAN GILBERT Three Rivers Press 0 609 80793 5 Sh.1.5 329.0 4.0 NaN 2012-03-07 2001 6097 25A NAN Isaac Newton, the last Sorcerer MICHAEL WHITE Fourth Estate 1 85702 706 X Sh.1.6 403.0 \u00a31.50P NaN 2012-02-09 1997 6663 25A NAN Live Wires - powerful stories of cha D. J. CARSWELL None 978 1 906173 13 5 Sh.2.7 124.0 1.0 NaN 2011-06-29 2010 8640 25A KING ARTHUR QUINCENTEN One in Specyal ED SIDNEY HART Three Golden Crowns 0 948485 00 0 Sh.5.9 145.0 \u00a32.49P NaN 2003-06-28 1985 8845 25A NAN The Order of St John - a short history E L EDMONDS s.n 0 947718 07 9 Sh.5.7b 35.0 \u00a33.75P NaN NaT 1986 if { var mathjaxscr = = = = ? \"innerHTML : \"text\")] = + \" config: + \" TeX: { extensions { autoNumber 'AMS' } },\" + \" jax: + \" extensions + \" displayAli 'center',\" + \" displayInd '0em',\" + \" showMathMe true,\" + \" tex2jax: { \" + \" inlineMath [ ['$','$'] ], \" + \" displayMat [ ['$$','$$' ],\" + \" true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS' { \" + \" linebreaks { automatic: true, width: '95% container' }, \" + \" styles: { .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important' }\" + \" } \" + \"}); \"; (document. || }"},{"title":"FakeGL: A Synthetic General Ledger and Trial\u00a0Balance","url":"fakegl.html","body":"My work involves processing a lot of General Ledgers and I wanted to build and test various automation and analytical techniques to see how my workflow could be improved. In order to do that in a free and fun way, I would need fake data, so I set out to build a process to generate a fake General Ledger (GL) and a correspond Trial Balance (TB). Motivation and I didn\u2019t know how comprehens I needed the GL to be - modern systems are complex and store data for a wide variety of uses. I resolved to start with something simple and iterate for as long as I\u00a0wanted. The journals produced below satisfy the following general Each journal contains equal debits and\u00a0credit Opening and closing Trial Balances net to\u00a00 Profit and Loss (P&L) accounts start the year with 0 balance, Balance Sheet (BS) accounts do\u00a0not. Each transactio hits both the P&L and the BS (i.e. If an account on the P&L is credited, then the other side of the transactio is a debit to the BS) Distinguis between manual and The GL: Contains journals posted evenly throughout the year (this isnt realistic, but is a simple way to generate date\u00a0data) Receives journals Identifies if a journal is manual depending on which subledger it Records which user posted the journal if the journal is\u00a0manual The script below allows the user to\u00a0specify The number of accounts on the GL/TB The number of journals in the GL A mean and variance for the number of lines in each\u00a0journ A mean and variance for the functional amounts posted to\u00a0account How many different users post The beginning of the financial\u00a0 The criteria for a manual journal, based on\u00a0subledg The proportion of manuals which are\u00a0manual The proportion of accounts which hit the P&L or BS An arbitrary list of\u00a0subledg The Jupyter Notebook below shows the annotated Python 3 code I\u00a0wrote: Notebook set-up\u00b6Loa the various libraries used to easily add the required features. Two libraries to\u00a0note: Pandas is pythons ubiquitous data handling\u00a0t Faker is a useful tool to generate fake data, and is an easy way to bootstrap a\u00a0database In\u00a0[1]: from random import gauss from faker import Factory import random import numpy as np import time from datetime import timedelta import datetime from natsort import natsorted, ns import pandas as pd Choose parameters and values for the GL and TB\u00b6 In\u00a0[2]: # ****** Number of different accounts in the GL ********* x = 111 # ****** Number of journals in the GL j = 15713 # Setup posting date d0 = '20160101' # first day, data generated over 1 year. d1 = \"%Y%m%d\") # ****** Distributi of lines per journals ********** jl_1 = 21 # mean jl_2 = 10 # variance j_l = lambda x, y: # ****** Number of different users posting journals ***** fake = U = 10 ul = [] for _ in range(0,U) # ****** Functional amount values q1 = 700 # mean q2 = 104 # variance def q(q1,q2): p = < 0.5 # True implies if p: i = -1 else: i = 1 out = i * return out # ****** Proportion of journals which are manual ******** Mp = 0.23 # ****** Proportion of accounts that are P&L accounts *** Pp = 0.3 # ****** Subledger names ********* source_fee = if an account feeds into the P&L or BS: In\u00a0[3]: def if len(elemen > 0: return element[2] == 'P' return False def if len(elemen > 0: return element[2] == 'B' return False Generate account\u00a0co In\u00a0[4]: def b_names = [] p_names = [] a_names = [] p = 'ACP' b = 'ACB' for i in range(x): A = < Pp if A: y = else: y = if len(b_name % 2 != 0: del b_names[-1 if len(p_name % 2 != 0: del p_names[-1 a_names = b_names + p_names Generate journal names and\u00a0length In\u00a0[5]: def d0 = '20160101' # first day, data generated over 1 year. d1 = \"%Y%m%d\") a_n = [] for i in range(j): n = y = 'J_' + n + d1 = d1 + a_n.append j_names = dict((el, int( j_l(jl_1,j / 2 )) for el in a_n) # determine how many lines are in each journal. return j_names Create the list of journal names and account codes\u00b6 In\u00a0[6]: j_names = a_names = Create the fake General Ledger and save it to a text file\u00b6 In\u00a0[7]: # Output format glf = f = open('gl.t 'w') f.write(gl + '\\n') for key in key=lambda y: y.lower()) line_no = -1 i = 0 # Assign each journal a source feed source_id = # Assign each journal a posting date posting_da = d1 = d1 + # Make journal either M or A, if M assign user t = < Mp # True implies U, 3*U/4) if t: man_ind = 'M' u_name = ul[int(p)] else: man_ind = 'A' u_name = '' # Assign functional amount to each line while i < j_names[ke i = i + 2 line_no = line_no + 2 line_no2 = line_no + 1 dr = q(q1,q2) cr = -1 * dr a_names_p = a_names)) a_names_b = a_names)) an1 = an2 = l_1 = key + '|' + str(line_n + '|' + man_ind + '|' + posting_da + '|' + u_name + '|' + an1 + '|' + source_id + '|' + 'GBP' + '|' + str(dr) l_2 = key + '|' + str(line_n + '|' +man_ind + '|' + posting_da + '|' + u_name + '|' + an2 + '|' + source_id + '|' + 'GBP' + '|' + str(cr) f.write(l_ + '\\n') f.write(l_ + '\\n') f.close() Create the Trial Balance and save it to a text file\u00b6 In\u00a0[8]: # Use gl to calc movement on each account gl = sep = '|') tb = # Calc net movement on each account tb = inplace=Tr tb.columns = = 'Account') # Assign account type # Set b/f balances to 0 for P&L accounts == 'P', 'Balance b/f'] = 0 == 'P', 'Type'] = 'P&L' == 'B', 'Type'] = 'BS' tb['Balanc b/f'] # if b/f balance is != 0, generate a balance for that account i = 0 for index, row in tb.iterrow if row['Balan dummy'] != 0: row['Balan b/f'] = bal = b/f'] = bal b/f'] = -1 * bal i += 2 del tb['Balanc dummy'] # create c/f field tb['Balanc c/f'] = ( tb['Balanc b/f'] + tb['Moveme ).round(2) # create 'date of balance' column tb['Balanc date'] = # Arrange columns tb = tb[['Accou 'Type', 'Balance b/f' , 'Balance c/f', 'Balance date']] # print TB to file sep='|', header=Tru index=Fals Load the text files back in and display their top 10 rows\u00b6Verif that the files have been produced correctly and that the TB balances as\u00a0expecte In\u00a0[9]: gl = sep = '|') tb = sep = '|') In\u00a0[10]: tb.head(10 Out[10]: Account Type Balance b/f Balance c/f Balance date 0 ACB00003 BS 673.10 72348.68 20161231 1 ACB00007 BS -673.10 20045.09 20161231 2 ACB00010 BS 748.16 -30340.79 20161231 3 ACB00012 BS -748.16 188.39 20161231 4 ACB00015 BS 814.96 48294.11 20161231 5 ACB00017 BS -814.96 10659.80 20161231 6 ACB00021 BS 835.56 18357.33 20161231 7 ACB00032 BS -835.56 3406.80 20161231 8 ACB00034 BS 759.60 26505.40 20161231 9 ACB00036 BS -759.60 -39128.80 20161231 In\u00a0[11]: gl.head(10 Out[11]: Journal_ID Line Type Date User Account Source 0 J_20160101 1 M 20160101 Iain Gardiner ACP00054 sl2 GBP -587.49 1 J_20160101 2 M 20160101 Iain Gardiner ACB00064 sl2 GBP 587.49 2 J_20160101 3 M 20160101 Iain Gardiner ACP00022 sl2 GBP 816.17 3 J_20160101 4 M 20160101 Iain Gardiner ACB00017 sl2 GBP -816.17 4 J_20160101 5 M 20160101 Iain Gardiner ACP00088 sl2 GBP 628.60 5 J_20160101 6 M 20160101 Iain Gardiner ACB00062 sl2 GBP -628.60 6 J_20160101 7 M 20160101 Iain Gardiner ACP00079 sl2 GBP -672.34 7 J_20160101 8 M 20160101 Iain Gardiner ACB00017 sl2 GBP 672.34 8 J_20160101 1 A 20160101 NaN ACP00005 sl1 GBP -683.52 9 J_20160101 2 A 20160101 NaN ACB00036 sl1 GBP 683.52 In\u00a0[12]: print('Net Opening TB:',\"%.2f % tb['Balanc b/f'].sum( print('Net Closing TB:',\"%.2f % tb['Balanc c/f'].sum( Net Opening TB: 0.00 Net Closing TB: 0.00 if { var mathjaxscr = = = = ? \"innerHTML : \"text\")] = + \" config: + \" TeX: { extensions { autoNumber 'AMS' } },\" + \" jax: + \" extensions + \" displayAli 'center',\" + \" displayInd '0em',\" + \" showMathMe true,\" + \" tex2jax: { \" + \" inlineMath [ ['$','$'] ], \" + \" displayMat [ ['$$','$$' ],\" + \" true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS' { \" + \" linebreaks { automatic: true, width: '95% container' }, \" + \" styles: { .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important' }\" + \" } \" + \"}); \"; (document. || }"},{"title":"Reconciliation of a trial balance to a general\u00a0ledger","url":"reconciliation.html","body":"I\u2019ve been working with financial ledgers a lot recently. The python code below shows an automated workflow to import, process and report on the reconcilia of a Trial Balance (TB) to a General Ledger (GL). I\u2019m using fake data, but the script would work fine with real data if the fields were renamed appropriat A real data set would have additional fields that needed to be considered but these vary depending on the size and type of business being analysed. Therefore the fake GL and TB used here are simple and generic. Additional fields, such as entity code, transactio status, approver, time stamps, etc can be added quickly and\u00a0simply Set-up the notebook and import\u00a0dat In\u00a0[1]: # Import libraries import pandas as pd import random In\u00a0[2]: # Import (possibly incomplete and/or inaccurate data gl = sep = '|') tb = sep = '|') Create Calculate net movement for each account in the ledger\u00a0dat In\u00a0[3]: gl_move = gl_move = inplace=Tr = = 'Account') inplace = True) Calculate the movement for each account in the trial\u00a0bala In\u00a0[4]: = ( tb['Balanc c/f'] - tb['Balanc b/f'] ).round(2) inplace = True) Compare each accounts movement in the ledger data and the trial balance and write the result to a report containing the reconcilia results for all\u00a0accoun In\u00a0[5]: Rec_report = how = 'outer', left_index = True, right_inde = True) = - = date'] = sep='|', header=Tru index=True Put the accounts which do not reconcile into a In\u00a0[6]: Unreconcil = != 0] Unreconcil = = False).ind sep='|', header=Tru index=True Unreconcil Out[6]: Type Balance b/f Balance c/f Balance date TB_Movemen GL_Movemen difference Account ACP00081 P&L 0.00 -16279.96 20161231 -16279.96 -17017.89 737.93 ACB00082 BS 760.09 -4041.02 20161231 -4801.11 -4063.18 -737.93 ACP00071 P&L 0.00 -28547.84 20161231 -28547.84 -29260.31 712.47 ACB00091 BS 628.24 3054.74 20161231 2426.50 3138.97 -712.47 ACP00017 P&L 654.01 24449.74 20161231 23795.73 23123.06 672.67 ACB00076 BS 768.49 -48456.11 20161231 -49224.60 -48551.93 -672.67 ACB00037 NaN NaN NaN nan NaN 19808.54 NaN ACP00001 NaN NaN NaN nan NaN -3817.92 NaN ACP00041 NaN NaN NaN nan NaN -14365.77 NaN ACP00046 NaN NaN NaN nan NaN 1825.79 NaN ACP00086 NaN NaN NaN nan NaN -7263.37 NaN Create a report containing In\u00a0[7]: # Accounts not in the TB but in the GL = == True] In\u00a0[8]: # Accounts in the TB where TB_Movemen isn't matched in the GL = == ==False)] In\u00a0[9]: Total_Acco = = Total_Acco - = Rec_Fracti = / Total_Acco Unrec_Frac = / Total_Acco In\u00a0[10]: with \"w\") as text_file: print('The data 'accounts' 'accounts reconcile (', '%)\\n', 'accounts do not reconcile (', '%)', In\u00a0[11]: with \"a\") as text_file: print('The are', 'accounts in the GL and not in the TB. (', '% of unreconcil accounts)' print('The are', 'accounts with journals missing (', '% of unreconcil accounts)' In\u00a0[12]: net_diff = == with \"a\") as text_file: print('The net of all the difference is', net_diff, In\u00a0[13]: # Does the TB balance? with \"a\") as text_file: print('\\nT opening balance is unbalanced by', \"%.2f\" % tb['Balanc b/f'].sum( print('TB closing balance is unbalanced by',\"%.2f\" % tb['Balanc c/f'].sum( print('*** these are not 0 then the TB is certainly wrong and receiving a \\nbalanced TB is the first step to reconcilin all accounts** In\u00a0[14]: diffs = frequency = {} for w in diffs: frequency[ = 0) + 1 pairs = {x for x in frequency if x > 1} # dict comprehens to filter for pairs with \"a\") as text_file: print('\\nT are', len(pairs) 'unreconci accounts with equal and opposite difference In\u00a0[24]: with 'r') as fin: The data contains 110 accounts 99 accounts reconcile ( 90.0 %) 11 accounts do not reconcile ( 10.0 %) There are 5 accounts in the GL and not in the TB. ( 45.45 % of unreconcil accounts) There are 6 accounts with journals missing ( 54.55 % of unreconcil accounts) The net of all the difference is 0.0 TB opening balance is unbalanced by -2486.28 TB closing balance is unbalanced by 1326.45 ***If these are not 0 then the TB is certainly wrong and receiving a balanced TB is the first step to reconcilin all accounts** There are 3 unreconcil accounts with equal and opposite difference if { var mathjaxscr = = = = ? \"innerHTML : \"text\")] = + \" config: + \" TeX: { extensions { autoNumber 'AMS' } },\" + \" jax: + \" extensions + \" displayAli 'center',\" + \" displayInd '0em',\" + \" showMathMe true,\" + \" tex2jax: { \" + \" inlineMath [ ['$','$'] ], \" + \" displayMat [ ['$$','$$' ],\" + \" true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS' { \" + \" linebreaks { automatic: true, width: '95% container' }, \" + \" styles: { .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important' }\" + \" } \" + \"}); \"; (document. || }"},{"title":"Spotify song\u00a0history","url":"spotify.html","body":"Spotify recently sent me their \u20182016 Wrapped\u2019 email containing a few statistics about my listening habits in 2016, and a playlist of my 101 most played\u00a0son I decided to compare their statistics with those I\u2019d gathered myself. There is an option in the Preference menu to log (\u2018scrobble your Spotify listening history to last.fm. from where you can then download your data. I thought it would be interestin to verify my Spotify stats, and see if I could discover any other 1. Notebook set-up\u00b6 In\u00a0[1]: # Display plot results inline, not in a separate window %matplotli inline %pylab inline # Set the size of all figures = (14, 5) # Import pandas and a nicer plotting style import pandas as pd import as plt plt.style. = 'default' Populating the interactiv namespace from numpy and matplotlib In\u00a0[2]: # Make the dataframes look a bit prettier from import HTML css = + # Print output to 2 decimal places only = # Just works for this cell only # This sets behaviour for entire notebook %precision %.2f Out[2]: '%.2f' 2. Load and explore the data\u00b6 In\u00a0[3]: # Load in the data from the .csv file I downloaded column_nam = ['Artist', 'Album', 'Name', 'd_t'] # Choose the column names df = names = column_nam header=Non # Convert the date time column (d_t) from a string to date time, so that I can search by date df['d_t'] = Some initial questions to answer to get a feel for the\u00a0data: How much data is\u00a0there? What type of data is\u00a0present What are the maximum and \u2026etc. In\u00a0[4]: # View the top 10 rows df.head(10 Out[4]: Artist Album Name d_t 0 Bonobo Animal Magic Kota NaT 1 Samaris Samaris G\u00f3\u00f0a tungl 2016-12-15 09:44:00 2 The Staves Dead & Born & Grown Mexico 2016-12-15 09:40:00 3 Bonobo Animal Magic Kota 2016-12-15 09:34:00 4 The Gloaming The Gloaming The Sailor\u2019s Bonnet 2016-12-15 09:30:00 5 St. Germain St Germain Forget Me Not 2016-12-15 09:24:00 6 Yosi Horikawa Wandering Bubbles 2016-12-15 09:19:00 7 South Music from The O.C. Mix 1 Paint the Silence 2016-12-15 09:13:00 8 The Staves Sleeping In A Car Sleeping In A Car 2016-12-15 09:09:00 9 Bonobo Days to Come Recurring 2016-12-15 09:04:00 In\u00a0[5]: #How many rows are there? len(df) Out[5]: 36454 In\u00a0[6]: # How many 2016 rows == 2016]) Out[6]: 1983 In\u00a0[7]: # Are there any NaN values in 2016? Ans = == 2016) & print('The are', Ans, 'rows with NaN values in 2016') There are 0 rows with NaN values in 2016 In\u00a0[8]: # Maximum and minimum dates print(\"Old record print(\"Mos recent record Oldest record is: 1970-01-01 00:00:00 Most recent record is: 2016-12-15 09:44:00 Scrobbling to last.fm was turned off for the first half of\u00a02016. In\u00a0[9]: #print(\"Ol record in 2016 == in 2016 began == Scrobbling in 2016 began at: 2016-06-14 20:24:00 Therefore it will be important to remember that the data being analysed only represents about half of\u00a02016. 3. Verify Spotify\u2019s statistics Total Minutes\u00b6 According to Spotify, I listened to 2357 unique artists this year, and 3309 unique tracks. Total streaming time was 45202 minutes. They dont tell me how many non-unique plays I\u00a0accumula Googling \u2018average song length\u2019 reveals that most pop songs last 3.5 minutes, but I think the songs I listen to are longer than\u00a0that. Spotify\u2019s UI is understand light on details, so initially I wondered how to calculate my average song length. First I just eyeballed the data (\u2018scanning analytics. Then I realised I could sort the playlist by song length and then I could pick the median, although I\u2019d need to know how many songs were in the\u00a0playli Whilst searching for playlist properties like \u2018number of songs\u2019 I simply spotted what I needed below the descriptio 101 songs, total play time of 8hrs, 29\u00a0minutes Spotify tell me I\u2019ve spent 45202 minutes streaming music and listened to 3309 unique tracks in\u00a02016. Therefore: In\u00a0[10]: # Average length of my 101 most listened to songs Average = round((8*6 + 29)*60/101 print(\"Ave song length:\", Average, \"seconds\") print(\"or 5 minutes, 2 seconds \\n\") # Time spent listening to Spotify in days print(\"Day listening to Spotify in 2016:\", A whole month?!? \\n\") # Time spent listening to Spotify in seconds seconds_us = 45202 * 60 # Number of songs played n_tracks = / Average,2) print(\"Num of songs played in # Average number of plays per songs unique_tra = 3309 # From summary email average_pl = round(n_tr / print(\"On average, I listen to each track\", average_pl \"times\") Average song length: 302.38 seconds or 5 minutes, 2 seconds Days listening to Spotify in 2016: 31.46 ...Really? A whole month?!? Number of songs played in 2016: 8969.24 On average, I listen to each track 2.71 times This is a surprising large amount of time spent listening to music. I cannot believe I spent 1/12 of 2016 streaming music from Spotify. I also thought I was quite a repetitive listener, so the average of 2.7 plays per song is surprising low. (For example, I have a short playlist with \u2018Recurring by Bonobo in it 5\u00a0times.) 3b. Unique to Spotify, I listened to 2357 unique artists in\u00a02016. In\u00a0[11]: Ans = == print('Num of unique artists scrobbled in 2016:', Ans) Number of unique artists scrobbled in 2016: 510 If all songs were being scrobbled to last.fm and my listening behaviour remained the same thoughout the year, I would have expected the result to be about half of Spotify\u2019s result (because I only began scrobbling in June). The summary email reported 2357 unique artists in 2016. 510 is clearly much less than half of\u00a02357. 3c. Unique tracks\u00b6 In\u00a0[12]: Ans = == print('Num of unique tracks scrobbled in 2016:',Ans Number of unique tracks scrobbled in 2016: 1060 Spotify reported 3309 unique tracks played in 2016. My result of 1060 unique tracks in only the second half of 2016 is less than I would have expected, though the difference between expectatio and result is not as great as for 3d. My top to Spotify, my 3 most played tracks in 2016\u00a0are: Lights Out Words\u00a0Gone Shuffle Luna \u2026all by Bombay Bicyle\u00a0Clu In\u00a0[13]: # Which tracks did I listen to the most in 2016? == Out[13]: Miserere 14 Shuffle 14 Lights Out Words Gone 12 The Pilgrim\u2019s Song 12 Always Like This 12 Luna 11 The Hare 11 You Already Know 11 Koop Island Blues 10 The Sailor's Bonnet 10 Name: Name, dtype: int64 This shows that whilst the \u2018top tracks\u2019 are some of my most listened to tracks, Spotify either uses a different method to calculate plays, or \u2018top\u2019 is not synonymous to When using data from June onwards, my \u2018top tracks\u2019 are only my joint 1st, 3rd and 6th most 3e. Top from\u00a0Spoti Bonobo Bombay Bicyle\u00a0Clu The\u00a0Staves Jack\u00a0Johns In\u00a0[14]: == Out[14]: The Staves 170 Bonobo 161 The Gloaming 132 Koop 107 Bombay Bicycle Club 105 Giovanni Pierluigi da Palestrina 70 Jimi Hendrix 48 Jack Johnson 44 Gregorio Allegri 43 Broken Social Scene 33 Name: Artist, dtype: int64 My results are different to Spotify\u2019s, but not by\u00a0much. Artist Spotify\u2019s results My result Bonobo 1 2 Bombay Bicycle Club 2 5 The Staves 3 1 Thievery Corporatio 4 - Jack Johnson 5 8 Apart from Thievery Corporatio being conspicuou absent from my own scrobbled data, all other difference could reasonably be explained by listening trends being different from Jan - June than July -\u00a0Dec. The results show that I prefer listening to Bonobo over Bombay Bicycle Club, but that my 3 most played songs are all by Bombay Bicyle Club. This shows that there are more songs by Bonobo that I enjoy listening to, and that I have more polarised opinions about songs by Bombay Bicyle\u00a0Clu 3f. Favourite day to to Spotify, I stream more music on Saturday than any other\u00a0day. In\u00a0[15]: # Plot how many songs wer played on each day of the week in the second half of 2016 == Out[15]: at 0x111243c8 Monday=0 and\u00a0Sunday This result shows that I stream more music on a Sunday than any other day. This is a different result to The small difference could be explained (again) by my data set only relating to the second half of 2016. From January to June I listened to more music on a Saturday than a Sunday - probably when I was playing Starcraft. This hobby unfortunat was put on ice and had to come second to revision for exams in July and\u00a0Novemb 4. Time of day\u00b6I thought it would be interestin to also see how much music I listen to at different times. I expect my listening habits to be different during the week compared to weekends, so I split the data by day and then summed the songs played whilst grouping them by\u00a0hour. The figures below show an unexpected dip around 3pm at weekends, and consistent streaming through the day whilst at the office during the week. No matter which day of the week, it appears I always listen to music in the\u00a0evenin In\u00a0[16]: # Count number of songs played during each hour at the weekends == 2016) & > Out[16]: at 0x110493dd In\u00a0[17]: # Count number of songs played during each hour during the week == 2016) & < Out[17]: at 0x11048fa5 5. of the attempts to verify Spotfiy\u2019s statistics fall short because my data only covers the second half of 2016. This shows the requiremen of good quality inputs in order to achieve good Having said that, it is clear that I listen to Spotify far more than I thought I did. The observatio that I like many of Bonobo\u2019s tracks but have more polarised views about Bombay Bicycle Club\u2019s songs is novel. Most of all, I think I\u2019m getting great\u00a0valu The chart below is number of songs played each day from 14 June 2016 to 15 December\u00a02 My top 101 songs of 2016 can be played at: In\u00a0[18]: import warnings figure = == foo = n = 3 ticks = ticklabels = [l.get_tex for l in if { var mathjaxscr = = = = ? \"innerHTML : \"text\")] = + \" config: + \" TeX: { extensions { autoNumber 'AMS' } },\" + \" jax: + \" extensions + \" displayAli 'center',\" + \" displayInd '0em',\" + \" showMathMe true,\" + \" tex2jax: { \" + \" inlineMath [ ['$','$'] ], \" + \" displayMat [ ['$$','$$' ],\" + \" true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS' { \" + \" linebreaks { automatic: true, width: '95% container' }, \" + \" styles: { .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important' }\" + \" } \" + \"}); \"; (document. || }"},{"title":"Vim!","url":"vim.html","body":"Vim is a text editor renowned for its efficiency and its use of keyboard shortcuts. It\u2019s based on the Vi text editor from the 1970\u2019s. It was first released in 1991 and is still being developed today. It comes pre-instal on Unix systems (including MacOS) and can be run from the\u00a0termin Vim is famous in another way too - for being difficult to learn. I found some good and remarkably creative tools to begin learning its concepts and controls. This was necessary because there is no GUI. There is a game here, and there is this interactiv tutorial. There\u2019s also a built-in vim tutorial - just type \u2018vimtutor\u2019 into\u00a0Termi Vim is designed so that you don\u2019t need to take your hands off your keyboard and use a mouse. It has the \u2018insert\u2019 mode where you enter text as usual, and the \u2018command\u2019 mode where you can make use of a comprehens and flexible shortcut language to move around, edit and search the text. With no GUI or toolbar, it\u2019s a very different approach to text editing than I\u2019m used\u00a0to. You can run Vim from the terminal, but there are also versions that run as apps. MacVim on MacOS has the option to show a tool bar of simple commands like a normal program, and lets the arrow keys move the cursor in addition to VIM\u2019s \u2018hjkl\u2019 functional This makes getting started a There are also a lot of plugins to extend Vim\u2019s functional and turn it from a text editor into an IDE. This post walks you through setting up Vim as a Python IDE and explains how to manage I recommend Daniel Mieslers blog post for a quick overview of how to use\u00a0Vim."},{"title":"Autumn, BIN and $PATH","url":"autumn-bin-and-path.html","body":"Two small things have been learnt recently: the importance of PATH and the contents of various BIN\u00a0folder Autumn 2016 has not gone as planned. Whilst studying for a couple of exams plans were put on hold and hobbies were ceased. Now that life is returning to normal, I have opportunit to post\u00a0again PATH $PATH is a variable (string) which contains a series of folder locations separated by \u201c:\u201d. Each of these folders contains programmes When you type the name of a programme into terminal without specifying its location, the OS looks sequential in each of the folder locations listed in $PATH to see if the programme is there, and then executes\u00a0i BIN Bin as in Binary, not Bin as in\u00a0Trash. The bin folders contain binary files, which are programmes ready to be\u00a0run. If I run \u201cecho $PATH\u201d from the Terminal, I see 9 folders called bin, and its only by convention that they contain binaries. They are just normal folders, which the OS is set to look in when asked to run"},{"title":"Introduction to my doctorate research -\u00a0Silos","url":"silos.html","body":"Background From Spring 2010 until the Autumn of 2013, I was a PhD candidate living in Vienna, Austria and working at the University of Natural Resources and Life Sciences. Before working in Vienna, I completed my Masters degree in Civil and Environmen Engineerin at the University of\u00a0Edinbur My research quantified the effects of changing the amount of gravity acting on granular materials as they poured out of a\u00a0silo. My PhD thesis and the short presentati I used when I defended\u00a0i Granular materials are a broard class of materials that are encountere everyday - salt, pills, breakfast cereal, sand, rice, soil, landslides salt are all granular materials. They are ubiquitous and occur in many different sizes and\u00a0variet Silos are a common type of container for storing granular materials. You pour the granular material in from the top, store it for a while, and then dispense the material in controlled quantities from the\u00a0bottom Research\u00a0f My research focussed on quantifyin how changes in gravity affected the material contained inside a silo, particular whilst the silo was being emptied. This is pertinent because engineers and scientists do not yet have a scientific understand of how granular materials behave. Whilst gravity clearly affects a granular materials, we cannot say exactly\u00a0ho This means we can\u2019t use analytical methods to quantify the physics that occur in a real system. Instead we use empirical methods guesses, and knowledge of what worked in previous similar situations This isn\u2019t necessaril bad, but it is less efficient and less reliable than an I built a small model silo (30cm tall) and put it into quite a large centrifuge (3 metre diameter). By rotating the model silo around the centre of the centrifuge at a constant speed I could simulate a higher gravity. I added high-speed cameras, pressure sensors and weighing scales so that I could measure how the material was moving once I opened the silo outlet and the silo began to empty. Photos of the experiment model I built can be seen\u00a0below I also programmed a computer model (using the commercial PFC 3D software and working in the FISH scripting language) to simulate and investigat if the same behaviours could be observed numericall as\u00a0physica The class of computer model I used is known as DEM (Discrete Element Modelling) These models work by considerin every grain of material individual usually as a sphere. If one sphere overlaps with another (i.e. the distance between the two particle centres is less than the sum of their radiuses) then a force proportion to the overlap size repels the two spheres away from each other. This simple approach is repeated over every grain or particle in the model, and produces life like behaviour in many situations It has many advantages over \u201ccontinuum based techniques that model groups of grains as if they were all just one big particle with unusual properties DEM has one massive limitation though. It requires huge amounts of computatio resources - and this limits its use in industrial scenarios outside. Until computers become much, much more powerful, DEM will only be used for theoretica research and Results When gravity increases by a factor of \\(x\\), both the discharge rate and local velocities within the silo increase by \\(\\sqrt(x) That\u2019s good to know if you\u2019re planning on storing stuff on the moon, but it\u2019s also a useful step towards explaining exactly why bulk granular materials behave the way they\u00a0do. An overview of my research, my doctorate thesis and published papers can be downloaded below. PDFs containing 3D models and movies require flash to render, and Adobe Reader Desktop must be used in order to view\u00a0them. PhD Thesis\u00a0(20 Modeling silo discharge in a centrifuge Experiment investigat of flow and segregatio behaviour of bulk solids in silos under high gravity conditions (Particles 2013 Centrifuga modelling of granular flows (Eurofuge Overview of research (PhD defence,\u00a02 if { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.wi"},{"title":"Encryption","url":"encryption.html","body":"Blockchain use Elliptical Curve Cryptograp (ECC) to authentica users and authorise transactio These notes introduce the field of cryptograp and explains how modern cryptograp methods work. I wrote them to teach myself about encryption 1 To begin with the absolute basics, encryption generally works by taking a message and representi it as a series of numbers 2 which are then turned into a series of random looking numbers. Decryption works by turning the random looking numbers back into the Background The history of Cryptograp can be split into classical and modern eras. Modern cryptograp began in 1977 with the introducti of the RSA and Diffie-Hel algorithms Until then, Cryptograp required using a single key (the secret code) to encrypt and decrypt the message. This was transferre from sender to receiver secretly, but not In classical cryptograp the code is a shared\u00a0sec The modern era removed the requiremen for a shared secret and instead used number theory as a basis for quantifyin the strength of an encryption method. The strength of a modern cryptograp technique is quantifiab and provable by reference to number theory, rather than a users ability to transport or transfer a secret\u00a0cod Modern cryptograp is defined by Public Key Cryptograp systems. They use one key (code) for encryption and another for decryption The encryption key can be made public without any risk of the message being decrypted, and is therefore known as the public key. The key used to decrypt data is the private key, and must not be revealed. If a message is encrypted with the public key it can only be decrypted with the private\u00a0ke Public Key Cryptograp (PKC) systems use algorithms that are easy to process in one direction but difficult to reverse, which are known as mathematic trap-door functions. A simple example of a trap-door function is the product of two prime numbers. If two random prime numbers are chosen, it is trivial to find their product. However if only the product of the two numbers is known, it is relatively difficult to find either of the factors used to create the number (this is This was first noticed in 1874, when \u201cCan the reader say what two numbers multiplied together will produce the number 8,616,460, I think it unlikely that anyone but myself will ever\u00a0know. This simple problem shows that finding the product of two (secret) prime numbers is simple, but factorisin the result is not. This type of problem is a key feature of modern cryptograp Factoring prime numbers is a super famous mathematic problem, it was studied by Eratosthen 3 in the 3rd century BC and more recently the RSA Factoring Challenge has intended to track factorisat techniques by issues cash prizes for the factorisat of products of large\u00a0prim Generally, the bigger the difference in difficulty between executing the function and reversing it, the better the The RSA algorithm below uses factorisat as the foundation of its security, but factorisat is not the hardest problem to solve relative to the size of the keys required. Algorithms have been developed to factor the products of large prime numbers, and are much more efficient than randomly guessing possible factors. The greater the size of the primes being factored, the more efficient these algorithms become, and therefore the difference in difficulty between executing the function (multiplyi two large primes) and reversing it becomes smaller as the size of the cryptograp key length increases. This is a problem because as public key cryptograp becomes more commonly used the resources available to factor products of primes increases, and consequent larger keys are\u00a0requir Ultimately encryption techniques based on the difficulty of factorisat will become redundant as the difficulty gap between creating and solving them shrinks. A better trap door function is\u00a0require Overview of the RSA\u00a0algori Named after its founders (Ron Rivest, Adi Shamir, and Leonard Adleman), RSA was one of the first public-key encryption algorithms and is still widely\u00a0use RSA (as well as other cryptograp techniques makes use of a number line which loops back to zero after reaching a maximum value, rather than increasing indefinite This means that once a maximum number \\(n\\) has been defined, if a number greater than \\(n\\) is created the result simply loops around to 0 and begins counting from 0 again. i.e. if \\(n = 10\\), then \\(7 + 5 = 12 - 10 = 2\\). The result of a calculatio on a looping number line may easily be found by doing long division and using the remainder as the final answer, i.e. \\(12 / 10 = 1\\) with Generation of a pair of RSA\u00a0keys: 1. Generate the RSA\u00a0module Select two large random prime numbers, \\(p\\) and \\(q\\). They need to be random because anyone who knows or guesses them will be able to decrypt Calculate \\(n =\u00a0pq\\) 2. Find derived number\u00a0(e) e must be greater than 1 and less than \\(( p - 1)( q - 1)\\). There must be no common factor for e and \\(( p - 1)( q - 1)\\) except for 1. 4 3. Form the public\u00a0key The pair of numbers \\((n, e)\\) form the public key and can be made\u00a0publi Even though \\(n\\) is public, it is so difficult to factor the product of 2 large prime numbers that an attacker would not be able to find its component primes in the time available. The strength of RSA rests entirely on the difficulty of factoring \\(n\\) into its two component prime\u00a0numb 4. Generate the private key\u00a0(d) The private key is generated from using \\(p\\), \\(q\\) and e as inputs to the Extended Euclidean Algorithm. For a given set of values, there is a unique answer \\(d\\). \\(d\\) is the inverse of \\(e\\) modulo \\(( p - 1)( q - 1 )\\). This means that \\(d\\) is the number less than \\(( p - 1 ) ( q - 1 )\\) such that when it is multiplied by e, it is equal to \\(1\\) modulo \\(( p - 1 ) ( q - 1 )\\). RSA\u00a0exampl RSA does not directly operate on strings as bits, it operates on numbers modulo (less than) \\(n\\). and it is necessary to represent plain text as a series of numbers less than \\(n\\). The dominant encoding on the internet is UTF-8, which represents each upper case Latin letter as a number between 65 and 90. Using this encoding, a message \u201cHELLO\u201d would become \u201c\\(72, 69, 76, 76, 79\\)\u201d. The maximum number \\(n\\) needs to be the product of the two prime numbers \\(p\\) and \\(q\\). For this example choose \\(p = 7\\) and \\(q = 13\\), so \\(n = 91\\) 5 The public key component e can be any number we choose, as long as there is no number other than 1 which is a common factor of e and \\(( p - 1 ) ( q - 1 )\\). In our example, this requires that there be no common factor between 72 and e other than 1, so let e \\(= 5\\). Therefore our public key is (91, 5). This can be made available to anyone without messages being decrypted because of the difficulty of factoring the product of (very large) prime\u00a0numb Using the fact that we know 5 and 11 are the prime factors of 55 and e is 5, we can use the Extended Euclidean Algorithm to compute our private key \\(d\\), which is\u00a029. Therefore when the prime factors 7 and 13 are used, the public key is (91, 5) and the private key is (91, 29). These parameters fully define a functional RSA\u00a0system Encoding To encode a letter H in a message (\u2018H\u2019 is \\(72\\) in UTF-8), we need to multiply it by itself \\(e\\) times (\\(e = 5\\)), rememberin to wrap around each time we pass our maximum value of \\(n = 91\\). \\(72 \\times 72 = 5184, 5184 / 91 = 56\\) with \\(88\\) remaining, (i.e. \\(5184 = 91 \\times 56 + 88\\)). Therefore: \\(72 \\times 72 = 5184 = 88\\) \\(88 \\times 72 = 6336 = 57\\) \\(57 \\times 72 = 4104 = 9\\) \\(9 \\times 72 = 648 =\u00a011\\) Therefore the encrypted value of \u201cH\u201d is \u201c\\(11\\)\u201c Using the method for each character in the message \u201cHELLO\u201d results in the encoded message To decrypt the message, we take each number and multiply it by itself \\(d\\) times, (\\(d=29\\)) wrapping around each time we pass \\(91\\). \\(11 \\times 11 = 121 = 30\\) \\(30 \\times 11 = 330 = 57\\) \u2026 \\(57 \\times 11 = 627 = 81\\) \\(81 \\times 11 = 891 =\u00a072\\) And we\u2019re back to our Files The spreadshee I used to calculate the encrypted and decrypted values can be downloaded here. A simple python script to encrypt and decrypt a message is here. It uses the AES Footnotes I used the explanatio here and here a lot.\u00a0\u21a9 A simple example is \\(A=1, B=2\\) etc\u00a0\u21a9 Eratosthen invented his famous sieving algorithm which finds all the primes up to a given limit.\u00a0\u21a9 If this is the case then e and ( p - 1) ( q - 1 ) are called \u201ccoprime\u201d\u00a0 Whilst the Extended Euclidean Algorithm is apparently simple to compute, its descriptio is not. Therefore I\u2019ve used the same numbers in the following example as in the tutorials here and here.\u00a0\u21a9 if { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.wi"},{"title":"Digital currencies: the\u00a0basics","url":"digital-currencies-the-basics.html","body":"Digital currencies are often discussed in the context of finance, technology and economics. The Blockchain - the technology which applicatio like Bitcoin are built on - is significan because it removes the need for trust or an intermedia between unrelated parties transactin with each other. So far, the most influentia and famous digital currency is\u00a0Bitcoin This post is intended to introduce the basic concepts of digital currencies and the problems a distribute ledger system needs to\u00a0overcom What is a A digital currency is an internet based medium of exchange. Units of digital currency are not printed, are not physical, and represent nothing. A unit of currency is produced by running algorithms to solve complex mathematic problems. When a solution is found, a unit of currency (for example, 1 Bitcoin) is\u00a0generat If the currency represents nothing, why is it\u00a0valuabl Because people believe that in future, other people will believe it does, and because people are willing to trade real goods and services in exchange for\u00a0it. This is the same as for dollars, sterling and euros (fiat currencies which also don\u2019t represent anything physical. (Although these examples are supported by laws In the past, creating a new currency without the support of government hasn\u2019t been A central bank was required to control the physical creation of new currency (otherwise people would create counterfei currency, decreasing scarcity and moving its value towards\u00a0ze An intermedia (a bank) was required for all large or remote transactio to make sure that the amount of money each party owns is correctly recorded and updated in a ledger (preventin double spending of\u00a0funds) The technologi breakthrou was preventing double spending without requiring an intermedia This is made possible by using cryptograp techniques developed over the last few decades, and cheap, powerful computers which have only recently Central and With convention currencies everyone\u2019s balance and transactio are recorded in one central ledger (a list showing how much money each account has) and each account holder only has access to their own balance and transactio With digital currencies a copy of the entire ledger (every transactio ever made by everyone) is held on each computer (or node), and anyone can see if two parties wish to make a remote transactio then they need a bank to be the intermedia The bank mediates by updating the central ledger to record the change in each parties funds as a result of the transactio This is how one party knows if the counter-pa is able to pay, and how payment is confirmed. If there is only one copy of the ledger, maintained by the bank, then the bank must be involved in every transactio between its account holders. This need for an intermedia increases the complexity and cost of Sending\u00a0mo To send money, a message is broadcast to the network that the amount in your account should decrease and the amount in another account should increase. Each computer in the network (a node) which receives this message will check its authentici make the changes, and pass the message along to other\u00a0node What problems does For a transactio to be accepted and entered into the distribute ledger, its authentici needs to be verified. Because the ledger is distribute everyone can see everyone else\u2019s transactio Therefore user authentica and transactio authorisat needs to be possible without compromisi a user\u2019s ability to send secure payments in\u00a0future. There is also the problem of double spending - because the currency is neither physical, printed or representa of anything, how do you prevent a user from spending their currency more than once, or simply creating as much new currency as they\u00a0want? Another problem is the addition of new transactio to the ledger from many unrelated users. If each party has their own copy of the ledger, updating (or changing) it as they want, how would the ledgers completene and accuracy be assured? How would you update your ledger to take account of transactio between third parties, and how would you know the order in which they occurred?1 The blockchain is remarkable because it is the first technology to solve all of these problems. Future posts will consider each of these problems are\u00a0overco This is the Byzantine Generals problem, which is nicely described in the introducti of this paper\u00a0\u21a9"},{"title":"Spare\u00a0time","url":"spare-time.html","body":"This is a list of interests I want to consider pursuing. I wrote it when I began to study for my last set of exams and my mind filled with things I\u2019d rather be doing instead. Some of these interests are just me reacting to having no spare time for a few months, but others are decent goals and projects. Temporaril losing my spare time made me value it\u00a0more. I wrote this post so that I could compare what I thought was important when I was busy to when I wasn\u2019t. Writing the list allowed me to move on without forgetting Here\u2019s a break down of each\u00a0item: \u201cRun 10k in 50 minutes\u201d - This is easier than I imagined. When I wrote the list I could barely run 2k without stopping, but intended to go for a weekly run whilst studying. I\u2019ve struggled to stay energetic and healthy during previous exam phases, so now I have a rule that I must do some mild exercise even if I think I can\u2019t spare the time. I find running really lowers my stress levels, and increasing my heart rate and working up a sweat lets me sleep better and concentrat for longer. After a few weeks of minimal running, I could run 5k easily. If I kept at it, 10k in 50 mins would be easy. But point\u00a02\u2026 \u201cBe Insanity strong\u201d - As in, do the Insanity workout programme. Again. I thought about it and decided to do P90x instead. Week 1 is going\u00a0grea \u201cRead the bible habitually - When I was 17 I found out about Jesus and became a Christian. In the months and years after that I\u2019d regularly read my bible almost every day and pray a lot. I wanted to understand so I set a goal of 5 chapters each\u00a0day. I read through the new testament repeatedly and read most of the old testament during this time. I knew the scriptures well enough that a lot of sermons became boring and obvious - I\u2019d already studied the bible passage being used. During this time I remember being aware that the way I thought was different to how it would have been otherwise. My perspectiv were long-term and less me focused. I thought about what I was reading instead of the days\u2019 headlines or social chatter. I remember enjoying the benefits and thinking that I should keep this habit. The reasons why I think Christiani is so wonderful are well summarised in this sermon. It\u2019s now 10 years later, and whilst my conviction are strong, my knowledge of the bible is sadly pretty fuzzy. My thinking is clouded by the perspectiv contained in the media I consume and the conversati I\u2019m part of. I strongly suspect I would be acting and thinking differentl if I read my bible more, but I don\u2019t know what those difference would be. Trouble is, it\u2019s often not obvious what the immediate benefits of reading the bible are, you have to work for it a bit. Praying for help is effective. The books in the bible were written to last through the ages and across all cultures, so it\u2019s not surprising that they\u2019re not as easy to read as something written for an English speaker in a hurry. I should stop being in a hurry, and stop prioritisi only \u201cThink more\u201d - On a similar note to 3, but less supernatur If I spent less time consuming content and a little more looking around me or walking, I reckon I\u2019d be more self-aware and make better decisions. This would probably lead to a happier, more \u201cPray more\u201d - I don\u2019t know why, but the creator of the universe wants me to talk and share my life with him. He cares about me. This makes no sense to me, at all. If I was God, I would not go out of my way to consider the views and concerns of a very flawed human. But when I pray, my prayers are very often answered. I should ask him about\u00a0this \u201cAmazon seller?\u201d - Amazon have this \u201cFulfilled by Amazon\u201d service, which means you don\u2019t even need to hold the stock you want to sell. If I choose the right products, import them cheaply from China and reselling them on Amazon at a profit, I end up quids in for minimal effort and manageable risk. \u201cBlog about interestin data\u201d - Here I am, blogging. I should stick to the main topic and get technical. I was inspired by this blog in\u00a0particu \u201cFinish Coursera\u201d - The data science specializa is great! It\u2019s in R, and I want to focus on Python, but I\u2019d still like to do it. I need to consider the opportunit cost of the time \u201cRead for fun, history, fiction\u201d - When I started my job, in April 2014, I was half way through Savage Continent, which is an eye-openin and eye-wateri history of Europe in the years after World War 2. I see Europe through different eyes because of it. However I only got half way though, and since April \u201814 I never felt I had the free time or energy to pick it back up. This should change. When I was a researcher and when I was a student, I had so much more opportunit to develop my own pursuits. Since entering the corporate world, I find myself fighting a war of attrition to exert my personalit onto my\u00a0lifesty Read for fiction\u2026 I\u2019m unconvince What do you have at the end of it? What can you do with it? Maybe I\u2019ve just been reading the wrong authors, but I\u2019m going to leave this for now. Sure, you could gain an appreciati of a different time or place, but that appreciati comes via the fictional characters and events, its secondary. What about abstract constructs perspectiv morals\u2026 things that history books are ill-suited for? Great fiction would be essential for exploring these. But for now, I\u2019ll prioritise point\u00a03. \u201cDo a photograph project\u201d - This I would love to do, but probably won\u2019t. It would be a luxury, and the opportunit cost would be too great right now. I\u2019d like to shoot a series of black and white portraits, and turn them into large prints. I think good portrait photograph is uniquely impactful and moving, choosing B&W removes distractio and leaves a subjects humanity more\u00a0expos \u201cHave a list of ideas\u201d - There\u2019s no excuse for this one, anyone can have several good ideas. It\u2019s turning them into reality that takes skill. Need to have the ideas first,\u00a0tho \u201cDo a law MOOC\u201d - i.e Study particular areas of law, in my own time and at my own pace. I studied a tiny bit of law during the ACA, and realised employment law or contract law could be really useful. (Same for the tax system - another surprise). We only had a brief introducti though, so if I could find the time I\u2019d like to know\u00a0more. \u201cDo an InfoSec or Network Security MOOC\u201d - Its super interestin but not likely to be a good use of\u00a0time. \u201cLearn to fight - Krav Maga / MMA\u201d - Ever since watching The Bourne Identity I\u2019ve wanted to learn Krav Maga, and Georges St-Pierre made me want to train for MMA. For now though, I\u2019ll do P90x. I can reconsider in 90\u00a0days. \u201cGet out of London\u201d - My contract ends in April 2017, next summer will be a crossroads I hate the commute, I hate being constantly rushed. Living in other cities has been a lot more\u00a0pleas"},{"title":"\u00dcbersicht widget: Time\u00a0Until","url":"time-until.html","body":"In a previous post I described how I was introduced to CoffeeScri via \u00dcbersicht, the desktop widget app for OS X, and eventually published the \u201cTime Since\u201d\u00a0wid Seeing a few people download the widget was really satisfying and I was soon wondering what else I could publish. As a pragmatic engineer, it seemed clear to me what the next widget should do: If my first widget calculated the time since an event, the next should calculate the time until an event. I set out to create the companion to \u201cTime Since\u201d and improve upon the My first code design choice was to cut out the use of an Apple Script and calculated everything in one CoffeeScri file. It would be more efficient and easier to\u00a0read. Unfortunat I soon began to realise why the original widget I\u2019d based \u201cTime Since\u201d on had used AppleScrip to calculate the time elapsed. Date-Times are fiddly to work with in many languages, and this is true in My code began to look increasing like spaghetti as I tried to achieve 6 key\u00a0featur Calculate the number of months and days between two dates (complicat by the varying number of days in Add the option to specify the level of detail in the output text (to the minute, to the hour, to the day,\u00a0etc.) Remove any 0 amounts from the output (\u201c2 Months and 5 Hours\u201d not \u201c2 Months, 0 Days and 5\u00a0Hours\u201d) Add the option to abbreviate times (\u2018years\u2019 \u2192 \u2018y\u2019, \u2018hours\u2019 \u2192 \u2018h\u2019, \u2018and\u2019 \u2192 \u2018&\u2019,\u00a0etc.) Output a grammatica correct sentence (correct pluralisat and comma separated terms, with an \u201cand\u201d between the last two\u00a0terms) Prepend and append users After a few frustratin hours, it was clear that it would be a lot easier to modify the existing AppleScrip rather than reinvent the wheel in JavaScript I decided to use it instead of using only JavaScript as I knew the AppleScrip file could correctly consider the number of days in the months between the 2 dates (feature 1), and it contains a function to pluralise the correct parts of the string (feature\u00a05 The remaining features were added by using a combinatio of if statements and arrays. The if statements simply ask if an amount is equal to zero. If not, then it\u2019s appended to an array of terms to include in the output. At the end of this code chunk it\u2019s possible to ask how many non-zero terms to include in the output. An array with length one less than the number of output terms is created and filled with commas with an \u2018and\u2019 in the The two arrays are combined in the correct order by looping through the index of the longer array and appending each term to a final array. The final array is the\u00a0output \u2018Time Until\u2018 can be downloaded from the \u00dcbersicht widgets gallery. I think another useful feature would be an option to specify the output only in terms of a chosen amount, such as weeks or days. Maybe I\u2019ll do that in the\u00a0future"},{"title":"How to wake up\u00a0early","url":"how-to-wake-up-early.html","body":"For years I\u2019ve wished that I could wake up early and use the quiet pre-breakf hours for productivi And for years I have spectacula failed at this. I love the quiet, hours at the end of the day too, as well as waking up slowly.1 Many famous leaders and politician are known to start their days early, and I would like to be able to do this too. After years of trying and failing, I have had The secret of waking up early is\u00a0this: Become a\u00a0parent. After the first couple of months with a\u00a0new-born You will be well-pract at quickly waking up at previously You will be used to operating on less sleep than you ever Your beautiful child will become a reliable alarm clock, waking you up at the crack of dawn with cute smiles and increasing insistent demands that you get up, play and If you have the freedom to begin sleeping and working when you want, I still believe this is a great option. I bashed out a PhD in 3.5 years during which I usually woke up late and began to work around lunch time! - Glorious autonomy!\u00a0"},{"title":"Jupyter (iPython) notebooks +\u00a0Pandas","url":"Jupyter-ipython-notebooks-pandas.html","body":"When working with more data than can fit in an Excel file, or when you want to be sure the data won\u2019t be edited, you usually need to interact with the data by One of the biggest time sinks (for me) when working with these tools (ACL, SQL, Python) is debugging, and working out exactly where in the chain of individual commands something unexpected happened. Even with only a modest page of code, I can quickly find myself rerunning the entire script multiple times and commenting and uncommenti multiple lines in order to understand what\u2019s really going on. If you have a time consuming task at the start of your script, such as a summarise and sort command, the extra time required can be even greater. This leads to interrupte flow Pandas is a python package to manipulate large datasets, the Jupyter notebook is an applicatio which allows the user to run a python script in chunks, and output the results of each chunk before continuing You can re-run a previous chunk without returning to the beginning, and change the code as you go along. This is amazingly flexible and\u00a0intuit I recently worked through an exceptiona good Pandas tutorial recorded at PyCon 2015. \u201cPandas from the ground up\u201d is well structured clear, has good scope and the resources are available to download from github. Brandon Rhodes gives you a good working foundation for using Pandas and the Jupyter notebook to manipulate datasets using\u00a0Pyth"},{"title":"Coursera\u2019s \u201cData Science\u00a0Specialisation\u201d","url":"courseras-data-science-specialisation.html","body":"Last year I decided to learn the tools required to work as a data scientist. I was confident I already had the mathematic and analysis skills I needed, but I wasn\u2019t familiar with the tools of the\u00a0trade. Some googling brought me to Coursera, and the Data Science Specialisa run by Johns Hopkins University It consists of 9 courses, and so far I\u2019ve completed five. If you do the courses in order then prior knowledge isn\u2019t required, and I think the courses strike a good balance of brevity and\u00a0depth. The main downside to me is that the courses exclusivel use R (which is popular in academia) and I would rather be using Python (which is more popular in\u00a0Industr Each course lasts about 3 weeks and deals with a specific aspect of data science, such as statistica inference or machine learning. Key concepts and tools in each subject are explained and developed, and whilst it\u2019s not as thorough as a longer course would be, there is more than enough material packed into the lectures, quizzes, assignment and projects to apply to I\u2019ve read that the second half of the specialisa is a lot more technical than the first, so I\u2019m looking forward to setting aside some time, working through the assignment and acquiring some useful\u00a0ski"},{"title":"\u00dcbersicht widget: Time\u00a0Since","url":"ubersicht-widget-time-since.html","body":"\u00dcbersicht is a desktop widgets app for OS X. Its free, open source, and has a pretty good widgets library to download and play with. A widget is a small app or feature that embeds into the desktop and displays some simple informatio It can tell you what song is currently playing, a weather forecast, disk space remaining, etc. The widgets are written in CoffeeScri which is a variant of\u00a0JavaScr When I started using \u00dcbersicht I began playing with the widgets, changing their appearance and their position on the screen. Some of the widgets are too complicate to mess with without specific programmin knowledge, but others are surprising simple and\u00a0intuit By trial and error, I began to customize widgets to my preference One widget I wanted to have but couldn\u2019t simply download was a timer to tell me exactly how much time had elapsed since a specific past\u00a0event By combining the display attributes of one widget and the calculatio method of another, I was able to mash together a foundation for a new widget. I then added some extra Optional text before and after the elapsed\u00a0ti Choice of expanded or abbreviate display\u00a0st Flexible formatting to remove and zero\u00a0amoun The widget is called \u201cTime Since\u201d and is in the \u00dcbersicht widgets gallery."}]